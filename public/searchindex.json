[{"content":"注：本文用LLM辅助写作的地方主要在 我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M nnz: 图边数（稀疏矩阵非零元素数，Num NonZero） 规模为10n~1000n q, k, v: (n, d) A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)): if M[i, j] != 0: out[i, j] = dot(A[i,:], B[:,j]) else: out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h) ↓ Each Block → (rid, hid): one node, one head ↓ Load Q[rid, hid, :] → s_Q[f] (shm) ↓ For each tile of neighbors (size ≤ 32): - Load neighbor IDs from indices[] - Compute Q · K^T (dot product using s_Q and K[dst]) - Reduce in warp → store in neigh_nodes_weight[eid] - Find max(weight) in current tile → weightMax - Adjust partial_sum and acc with exp(old_max - new_max) - Compute exp(weight - weightMax) and accumulate acc += exp_w * V[] - Accumulate partial_sum += exp_w - Update weightMax_old ↓ Final normalization: out_feat = acc / partial_sum ↓ Write back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":0,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在 我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"2 October, 2025","id":1,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":2,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":3,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":4,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":6,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":7,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":9,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"},{"content":"注：本文用LLM辅助写作的地方主要在 我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M nnz: 图边数（稀疏矩阵非零元素数，Num NonZero） 规模为10n~1000n q, k, v: (n, d) A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)): if M[i, j] != 0: out[i, j] = dot(A[i,:], B[:,j]) else: out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h) ↓ Each Block → (rid, hid): one node, one head ↓ Load Q[rid, hid, :] → s_Q[f] (shm) ↓ For each tile of neighbors (size ≤ 32): - Load neighbor IDs from indices[] - Compute Q · K^T (dot product using s_Q and K[dst]) - Reduce in warp → store in neigh_nodes_weight[eid] - Find max(weight) in current tile → weightMax - Adjust partial_sum and acc with exp(old_max - new_max) - Compute exp(weight - weightMax) and accumulate acc += exp_w * V[] - Accumulate partial_sum += exp_w - Update weightMax_old ↓ Final normalization: out_feat = acc / partial_sum ↓ Write back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":0,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在 我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"2 October, 2025","id":1,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":2,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":3,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":4,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":6,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":7,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":9,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"},{"content":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M nnz: 图边数（稀疏矩阵非零元素数，Num NonZero） 规模为10n~1000n q, k, v: (n, d) A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)): if M[i, j] != 0: out[i, j] = dot(A[i,:], B[:,j]) else: out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h) ↓ Each Block → (rid, hid): one node, one head ↓ Load Q[rid, hid, :] → s_Q[f] (shm) ↓ For each tile of neighbors (size ≤ 32): - Load neighbor IDs from indices[] - Compute Q · K^T (dot product using s_Q and K[dst]) - Reduce in warp → store in neigh_nodes_weight[eid] - Find max(weight) in current tile → weightMax - Adjust partial_sum and acc with exp(old_max - new_max) - Compute exp(weight - weightMax) and accumulate acc += exp_w * V[] - Accumulate partial_sum += exp_w - Update weightMax_old ↓ Final normalization: out_feat = acc / partial_sum ↓ Write back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":0,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"2 October, 2025","id":1,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":2,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":3,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":4,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":6,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":7,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":9,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"}]