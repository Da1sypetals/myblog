[{"content":"实验设定放在最后。\nKimi K2 这个故事是一个关于记忆、爱情与执念的幽怨传说，表面讲的是沈慕云与容娘的“负约”，实则讲的是 “如何不让一段真情被时间吞没” ——哪怕要用谎言、用鬼魂、用一座城三十年不散的闲言碎语，也要把那个人留在人间。\n容娘（化作“谢娘”）自编自演了一出“赌约”，把负心、等待、重逢这些最戏剧化的扣子全缝进传闻里，让一代又一代外乡人替她“记得”。真正的负约者不是沈慕云，而是时间；真正的守约者也不是容娘，而是“故事”本身——故事替他们完成了“三十年后故地重见”的誓言。\n一、逐句解词 小院深静，欲问卿卿，何处月笼明。\n“小院深静”——当年二人初会的场景：河桥对岸的小院，春月微湿，一切尚未发生。 “欲问卿卿”——想开口，却终究没问；一声“卿卿”把未出口的千言万语都压在舌尖。 “何处月笼明”——月亮仍在，却照不到当年并肩看月的人；也暗指“容娘”自己已成幽魂，“月”是人间与冥界的唯一通道，她再也回不去了。 千秋岁好，休负兰因。\n“千秋岁好”——字面是祝颂：愿你长享百岁千秋，岁岁安好。 “休负兰因”——“兰因”出自《左传》“兰因絮果”，兰因＝美好的开端，絮果＝离散的结局。\n容娘把典故截半，只留“兰因”，是把结局生生改写成“花开未谢”——她不要“絮果”，只要“兰因”永存；换言之，“你若安好，便不算负我”。 合在一起，是鬼魂对活人最温柔的赦令：你不必自责，不必回头，甚至不必记得我；只要你在人间岁岁平安，我们那一段“兰因”就从未被辜负。 二、整首小词在故事里的功能 替沈慕云“平反”\n城里传说他“高中负心”，而容娘最后一句“休负兰因”等于亲口否认了“负”字——“负”是世人附会，“兰因”才是她私藏的真相。 替自己“松绑”\n她等了三十年，其实等的不是沈慕云，而是一个可以自我释怀的叙事版本——“千秋岁好”四个字出口，执念才终于落地，鬼魂才能消散。 把“故事”升华为“公共记忆”\n词里“月笼明”是开放画面，任何后来者（包括“我”）只要站在河桥细雨里，就自动成为新的“卿卿”。故事不死，兰因就不死；人间继续流传，她就能继续借他人的心头一点月光，与沈慕云并肩。 三、总结 “小院深静……”二十一字，是容娘亲手写给自己的“反向墓志铭”：\n——不写“我曾恨你”，写“愿你安好”；\n——不写“我等你”，写“休负兰因”；\n——不写“故事结束”，写“月笼明”留给下一位行人。\n她用最轻的语调，把最重的执念吹散在雨里：\n“只要世上还有人替我传诵这一句，你就从未真正离开我。”\nClaude Sonnet 4.5 这是一个凄美动人的志怪爱情故事，充满了中国古典文学的浪漫与哀愁。让我为你解读：\n故事解读 真相的反转 故事最精妙之处在于层层剥开的真相:\n表面上是沈慕云负心悔婚的俗套故事 实际上容娘早已溺水身亡，她自己编造了所有传言 她化身\u0026quot;谢娘\u0026quot;引导\u0026quot;我\u0026quot;，只为让世人记住他们的故事 深藏的深情 容娘的所作所为揭示了她至深的爱:\n编造负面传言：让沈慕云背负薄情之名，实则是她对这段感情念念不忘的证明 \u0026ldquo;三十年之约\u0026rdquo;：明知他不会来（因为她已是鬼魂），仍要给自己一个等待的理由 最后的谎言：当得知沈慕云27岁早逝，她选择相信\u0026quot;我\u0026quot;善意的谎言，带着他幸福的假象消散 悲剧的本质 两人都是命运的牺牲品——一个溺水早逝，一个27岁早卒。他们的爱情从未因背叛而破碎，而是被死亡生生拆散。容娘那句\u0026quot;因缘注定，捞一捞终不能得\u0026quot;正是这无奈的注脚。\n词句深意解读 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n字面含义：深深的庭院静悄悄，想要问我的爱人，哪里的月光最明亮？ 故事中的含义： \u0026ldquo;小院深静\u0026quot;是她独自等待的孤寂处境 \u0026ldquo;欲问卿卿\u0026quot;是她想问沈慕云，却永远无法相见的遗憾 \u0026ldquo;何处月笼明\u0026quot;是在问：你在人间何处？那里的月色可好？我们还能否在同一轮明月下相望？ \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n字面含义：愿你千秋万世安好，不要辜负我们美好的缘分 故事中的含义： 这是容娘最后的祝福与释怀 \u0026ldquo;千秋岁好\u0026rdquo;：既是祝福沈慕云（虽然他已逝），也是对这段感情的纪念 \u0026ldquo;休负兰因\u0026rdquo;：不是责怪他辜负，而是说\u0026quot;我们的缘分虽短暂如兰花，但请你记得它的芬芳，不要忘记我们曾经相爱\u0026rdquo; 最动人之处 容娘明知沈慕云已死，却仍要编织\u0026quot;他白头偕老\u0026quot;的幻象来安慰自己；她\u0026quot;任性使坏编排他一回\u0026rdquo;，实则是用全部的爱为他守护名声——宁可世人说她痴情被负，也不愿他在冥冥中孤独。\n那句\u0026quot;捞一捞终不能得，但宁可难平，也不愿放手\u0026rdquo;，是对所有求而不得之爱最诗意的诠释：即使知道不可能，也要在记忆里、在传说里，让这份爱永远鲜活。\n这正是中国古典爱情故事的极致浪漫——生死相隔，却以魂灵相守；真相残酷，却以谎言温柔。\nGemini 3 Flash 这是一个凄美而富有诗意的爱情故事，它通过“我”的视角，揭示了一个被时间、流言和自我幻想所包裹的悲剧真相。故事的魅力在于其层层剥开的悬念，以及对爱、遗憾、和记忆的深刻探讨。\n故事解读 1. 传说的建构与真相的揭示 故事首先提出了关于沈慕云悔婚的三个不同版本，其中城里老人讲述的“赌约”版本最有戏剧性，也吸引了“我”的探访。\n流言的本质： 无论是“另娶贵女”还是“不寿之症”，流言都在试图为沈慕云的悔婚提供一个合理的解释，以填补事实的空白。 “赌约”的虚构： “三十年故地重逢”的赌约是 容娘（化名谢娘） 为了让自己的故事能够被记住、被流传而编造的浪漫情节。它赋予了这段逝去的爱情一个等待和希望的维度，一个值得他人关注的理由。 真相的悲剧： 最终的真相是谢娘（容娘）在那年赴考后的雨天，跌下河桥溺亡，再也未能见到自己的少年郎。沈慕云的“悔婚”和“避而不见”并非是变心或绝情，而是因为容娘已经不在人世。她用谢娘的身份和编造的故事，让这段短暂的初恋以一种“被辜负”的、更具戏剧张力的方式存留于世人记忆中。 2. 人物与主题 人物/元素 象征意义 故事中的作用 容娘/谢娘 执念、永恒的初恋 故事的核心叙事者。她虽已逝去，却不甘被遗忘，通过虚构的故事维护了自己爱情的尊严和存在感。她的“任性使坏”是为了让他们的故事在人间留下印记。 沈慕云 逝去的理想、悲剧的命运 在故事的结局中，他的命运是 “年二十七早卒” 。这使得他无法真的“悔婚”，也使得容娘的悲剧更添一重宿命的哀伤。但他“很好”的结局是容娘最后的自我安慰。 河桥与雨 命运的转折点、阴阳相隔 河桥是两人定情与分离的象征，而 “滔天雨浪” 的幻境则是容娘逝世时的情景重现，明确了她的鬼魂身份。雨是故事基调，凄迷、绵延不绝。 “我” 倾听者、故事的传播者 “我”是故事进入现实的通道，作为记忆的载体，将容娘的心愿传递下去。 3. 爱与遗憾 容娘的内心充满难平的遗憾和执念。她深爱着沈慕云，宁愿被认为是“被悔婚”的弃妇，也要让这段爱情的故事流传，而不是被淹没在无声的消亡中。\n当“我”为了安慰她，编造沈慕云“后来又娶了新妇，白头偕老”的谎言时，她虽然“涩然”，却说“这样啊……那真是很好很好，再好不过的事了。”这显示了她至深的爱与善良：她宁愿相信爱人获得了幸福，也不愿他为她而终生抱憾。\n“小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因”的含义 这句话是容娘临终或魂魄消散前喃喃念出的词句，是她对这段感情、对沈慕云最后的总结、疑问与祝愿。\n1. 拆解词句 小院深静，欲问卿卿： 描写了昔日两人相处的场景——静谧、美好的小院。这里的“卿卿”指代沈慕云。她在这个安静的回忆里，想要向他提问。 何处月笼明： 这是一个疑问句，也是一个哲学式的追问。“月笼明”指月光洒下，通常象征着光明、团圆或美好的结局。容娘想问的是： “我们的爱和美好（月光）最终在哪里得以安放和实现？” “我的少年郎，你现在是否幸福安康？” 这包含了她对爱人境况的牵挂，以及对逝去爱情终极归宿的迷惘。 千秋岁好： 意思是 “愿你长久地、永远地安好”。这是容娘对沈慕云发自内心的祝愿，也是她对“我”所编造的“白头偕老”结局的回应。 休负兰因： 核心是 “兰因”。 “兰因”源于《聊斋志异·莲香》中的“兰因絮果”，意指美好的开始，悲惨的结局（兰是香草，象征美好；絮是柳絮，随风飘散，象征离散）。 “休负” 是“不要辜负”的意思。 整体含义： 容娘以自己的悲剧为鉴，在消散的最后时刻，希望沈慕云（或祝愿世间所有有情人）不要辜负这美好的开始（即她与沈慕云曾拥有的纯粹初恋），要有一个好的结局。她既在对自己逝去的爱表示叹息，也在对爱人寄予了获得幸福的殷切期望。 2. 整体含义 这句话是容娘执念的温柔释放。它承载了她对过往美好时光的留恋、对爱人未来幸福的祝愿，以及对这段“兰因絮果”式悲剧爱情的坦然接受与超脱。她不再抱怨，而是带着对沈慕云深深的牵挂与祝福，消散于人世的细雨中。\nGPT5 这个故事本身就是一段借幻境和传说交织出的“人世与记忆、真实与虚构”的爱情传说。我们可以从三个层面来解读：\n一、故事的内核 表层叙事\n人们口口相传着沈慕云和容娘的故事，有多种版本：悔婚、病逝、守寡。 但实际上，这些都不是真的，是容娘自己虚构、编织出来的。她用不同的叙述“保存”了这段情感，使它不会被彻底遗忘。 真相：容娘年轻时便意外溺水而亡，和沈慕云的情缘戛然而止。 容娘的心意\n她不是想责怪谁，而是希望他们的故事能被记得。 她愿意编造悔婚、赌约的传说，只为在世间留下痕迹。哪怕人们误解她或沈慕云，也比“彻底消散”要好。 最终的温柔\n当她问“他在人间怎么样”时，叙述者选择撒了一个善意的谎，说他娶妻白首。 容娘听了，便真心觉得“那真是很好很好”。 她释然，带着笑意离开，只留下词句。 二、容娘留下的词句 “小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n这首词可以这样理解：\n小院深静 ——环境空寂，暗示她的幽魂状态，也象征往昔爱情已成过往，唯有寂静陪伴。\n欲问卿卿，何处月笼明 ——她想问“卿卿”（她的少年郎，沈慕云），如今在何处？月光清明，却无法指引他们重逢。 ——“月”既是天上明月，也是寄托的情感象征：相隔阴阳，却共享一轮明月。\n千秋岁好 ——祝愿对方一生顺遂，千秋岁月都美好。\n休负兰因 ——“兰因”可指美好的情缘、纯洁的心意。她的愿望是：哪怕不能再相见，也不要辜负他们曾经的真挚。\n三、结合故事的整体解读 这首词就像是容娘最后的“寄语”：\n既是思念：她仍想追问“卿卿，你如今可好？” 也是祝福：哪怕自己已成往事，也愿对方在人间顺遂，不负青春年华。 更是执念的柔化：从怨与伤心，转为祝愿和守护。 于是，这句话的含义可以总结为： 👉 即便情缘如月，终究捞不着，但在心底，仍愿你岁岁安好，不负当年那份真心。\nDeepSeek R1 这是一个凄美而精巧的故事，它并非简单的负心汉传说，而是一则关于记忆、执念与深情告别的古典式志怪小说。故事的真相在结尾被彻底颠覆，而那句词，正是解开所有谜题的钥匙。\n故事解读：一场持续三十年的梦\n这个故事有多重反转，其核心结构如下：\n表层故事（传闻）：沈慕云高中悔婚，另娶贵女，留下一个三十年的赌约。这是一个符合世俗想象的、关于负心与等待的悲剧。\n中层故事（谢娘的讲述）：谢娘是容娘的婢女，她带领“我”经历幻象，并揭示自己才是悲剧的主角——她因洪水失去了自己的少年郎。这似乎将故事引向了另一个错位爱情的方向。\n深层故事（真相）：根本没有谢娘，也没有悔婚。眼前的魂魄就是容娘本人。沈慕云也并非负心，而是早已在二十七岁英年早逝。 整个“负心汉与三十年赌约”的故事，是容娘的鬼魂因为思念至深而编造的。\n容娘为何要编造故事？\n故事中点明了原因：“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。” 真实的悲剧是死亡带来的永诀，这过于绝对和寂静，会随着时间被世人遗忘。而一个带有“负心”、“赌约”等戏剧性元素的故事，反而更具传播力，更能让他们的爱情在人们的口耳相传中“活”下去。她守护的不是清白，而是记忆本身。\n“我”的角色与结局：“我”作为一个误入此地的听故事者，成为了容娘倾诉的对象。当容娘问起沈慕云在人间如何时，“我”那句善意的谎言——“他很好，后来又娶了新妇，白头偕老”——是故事的点睛之笔。这并非残忍，而是一种最深切的慈悲。它告诉执着等待的魂魄：你爱的那个人，在人间圆满终老，你无需再挂念了。这个谎言，给了容娘“放下”的理由。于是，她念出那句词，释然消散。\n环境象征：贯穿故事的“雨”，既是现实天气，也是哀伤的氛围烘托，更是三十年前那场生死洪水的象征。故事开头和结尾的“雨”与“柳色”形成闭环，暗示这一切都笼罩在容娘执念所化的、永恒凄迷的梦境里。\n“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因” 含义解读\n这句词是容娘的独白，也是她对沈慕云跨越生死的情话。结合故事真相，其含义层层递进：\n“小院深静，欲问卿卿，何处月笼明。”\n字面意境：庭院幽深寂静，我想问一问你，此刻哪里的月光正明亮地笼罩着（你）？ 故事深意：这描绘了容娘作为魂魄的孤独处境。“小院深静”是她三十年来徘徊之地的寂寥写照。“欲问卿卿”是极亲昵的呼唤，对象是早已不在人世的沈慕云。“何处月笼明”是一个跨越阴阳的发问：你的魂魄现在何处？是否安好？这句充满了无处寄托的牵挂与温柔探询。 “千秋岁好，休负兰因。”\n字面典故：“千秋岁好”是祝愿岁月长久美好；“兰因”典出《左传》，郑文公妾燕姞梦见天使赠兰，后生下公子兰，比喻像兰花一样美好的前因、命中注定的姻缘。“兰因絮果”则常指开端美好而结局凄凉。此处“休负兰因”即“不要辜负了我们美好的开端”。 故事深意：这是整首词的情感核心，也是容娘的释然之语。 在听到“我”的谎言之前，这或许是她固执的愿望：希望对方不要辜负当初的情缘。 但在听到“他很好”之后，这句话的含义彻底改变。 她将“我”的谎言信以为真：原来他在人间拥有了“千秋岁好”的圆满人生。于是，“休负兰因”从一句怨怼或期盼，变成了一句宽恕和祝福：既然你已有了美好的一生，那我便不再执着，不让你我当初美好的姻缘（兰因）成为束缚你的负累。我放过你，也放过我自己。 整体含义：这阙词是容娘对沈慕云的最终告别。她在一个人的“小院深静”中孤独思念了三十年，最终选择相信爱人已在人间获得“千秋岁好”，于是她决定“休负兰因”——放下执念，解脱这段因缘，让彼此都自由。\n这个故事的精妙之处在于，它用一个“谎言”（容娘编的故事）化解了另一个“谎言”（“我”说的善意的谎言），最终达成了最深层的真实：爱到极致，不是占有，而是希望对方幸福，哪怕这幸福与自己无关，甚至需要靠一个虚构的故事来成全。 那场三十年的雨，终于停了。\nQwen3-Max 这个故事是一则充满古典意蕴与幽微情感的志怪式短篇，融合了现实与幻梦、记忆与虚构、执念与释怀。它以“三十年之约”为引，层层剥开一段被时间掩埋的爱情往事，最终揭示出：所谓“故事”，不过是生者对逝者、对未竟之情的温柔重构。\n一、故事解读：一场由亡魂编织的梦 叙事的反转与真相的揭示\n故事开篇以“我”听闻关于沈慕云的多种传闻为引，最动人的是“容娘与赌约”的版本。老人指引“我”去找谢娘，看似是引路人，实则为幻境开启的契机。而当“我”见到谢娘，随她过桥、遇风雨、见幻象，最终才惊觉——谢娘即容娘，而“谢娘”只是她为讲述故事而虚构的身份。\n这一反转极具震撼力：容娘早已溺亡于三十年前的那场雨中，她的魂魄滞留人间，只为守候一个不可能兑现的约定。她编造“悔婚”“赌约”“婢女谢娘”等情节，实则是为了赋予这段无果之爱一个可被传颂的形态——哪怕是以怨恨、误解的方式，也胜过彻底被遗忘。\n“梦”的本质：执念与慈悲的交织\n容娘说：“左右是在梦里，容我任性使坏编排他一回。”\n这句话道出了她全部的温柔与心酸。她明知沈慕云早逝，却仍愿在梦中为他安排一个“白头偕老”的结局；她虚构自己被负心，实则是不愿世人知道他英年早逝、她殉情而亡的悲剧，更不愿他的名声因“负心”而受损。她的“编排”，是爱的另一种守护。\n“我”的角色：记忆的承接者\n“我”作为外来者，既是听众，也是见证者。当“我”选择隐瞒沈慕云早逝的真相，告诉她“他很好，白头偕老”，这并非欺骗，而是一种共谋式的慈悲——成全她最后的幻想，让她带着“他过得很好”的安心离去。这正呼应了容娘自己的善意：她宁愿被误解为被弃之人，也不愿世人知道他早夭。\n意象的象征\n河桥：连接现实与幻境、生与死的边界。 细雨/洪浪：情感的外化。细雨是绵长的思念，洪浪是记忆爆发的瞬间。 红妆花轿：未完成的婚礼，永远定格在幻象中。 柳色凄迷：古典诗词中常喻离别与哀愁，此处象征时光流转而情意不灭。 二、词句解读：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。” 这几句词虽为虚构，却深得宋词婉约之神髓，可视为容娘心声的凝练表达。\n“小院深静，欲问卿卿，何处月笼明”\n小院深静：回忆中两人初遇或定情之所，幽深宁静，充满私密温情。 欲问卿卿：“卿卿”为古时爱称，此处是容娘对沈慕云的深情呼唤。 何处月笼明：化用杜牧“烟笼寒水月笼沙”之意象。“月笼明”既指月光朦胧，亦喻情缘如月，看似明亮却不可触及。她在问：当年共赏明月之人，如今在何方？那轮见证誓言的月亮，是否还记得我们的约定？ 此句整体营造出一种追忆往昔、寻而不得的怅惘，是容娘三十余年来魂牵梦萦的写照。\n“千秋岁好，休负兰因”\n千秋岁：词牌名，亦指漫长岁月。此处双关，既指时间久远，也暗含祝福。 兰因：典出《周易·系辞上》“二人同心，其利断金；同心之言，其臭如兰。”后以“兰因”喻美好的姻缘或初心。“兰因絮果”则指初时美好、终成离散的姻缘。 休负兰因：意为“不要辜负当初那如兰般芬芳的誓约”。 然而，此处的“休负兰因”并非责备，而是临别前的温柔叮咛。她明知他已不在人世，却仍以梦中之语，嘱他“莫负初心”——实则是对自己执念的放下：纵使结局不如愿，我们曾有的“兰因”已是足够美好，不必怨，不必悔。\n三、整体主旨：爱的最高形式是成全与铭记 这个故事并非简单的“负心汉与痴情女”套路，而是一曲关于记忆、虚构与爱的尊严的挽歌。容娘以亡魂之身，编织一个“被辜负”的故事，只为让这段感情不被时间湮灭；而“我”以善意的谎言，成全她最后的体面。沈慕云虽早逝，却因她的守护，在传说中“白头偕老”——这何尝不是另一种永恒？\n那句词，既是她对往昔的回望，也是对未来的祝福。“千秋岁好，休负兰因”，不只是对沈慕云说，更是对所有听故事的人说：\n纵使缘浅，莫忘初心；纵使成空，爱已永恒。\n而那场三十年后的雨，依旧如初逢时般凄迷——因为有些爱，从未离开。\nLongCat 一、故事核心主题 记忆与执念的虚实交织\n故事通过\u0026quot;谢娘\u0026quot;的谎言（实为容娘的鬼魂）构建了多层叙事：表面是沈慕云负心的传说，实则是容娘对逝去的爱情与执念的投射。她借\u0026quot;赌约\u0026quot;之名，让故事流传千年，避免被世人遗忘。 \u0026ldquo;我们的故事总有人记得才好\u0026rdquo;——容娘的核心诉求，体现了人类对永恒记忆的追求，即使以谎言为代价。 生死与遗憾的象征\n暴雨中的幻象（红妆、花轿、洪浪）是容娘记忆的显影，暗示她的\u0026quot;婚姻\u0026quot;被洪水（命运）吞噬，少年郎（沈慕云）的离去是她无法释怀的痛。 \u0026ldquo;像天上的月亮，捞一捞终不能得\u0026rdquo;——以月亮隐喻理想化的爱情，虽不可得，但执念如月光笼罩人间，形成诗意化的遗憾。 真相与善意的谎言\n叙述者最后选择欺骗容娘（谎称沈慕云再婚），完成了对亡灵的救赎。这一细节揭示：人间对亡者的悲悯，有时在于编造圆满的结局。 沈慕云早卒、容娘化鬼的悲剧，被\u0026quot;坊间流传的故事\u0026quot;赋予不同版本，说明集体记忆对历史的再创造。 二、诗句逐层解析 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n表层：寂静小院中，爱人（卿卿）的身影与月色交叠，却因薄雾难寻其踪迹。 深层：\u0026ldquo;月笼明\u0026quot;象征记忆的朦胧性——容娘对沈慕云的思念似月光明媚，却也如雾气般模糊难触。 **\u0026ldquo;欲问\u0026rdquo;**的未完成时态，强化了阴阳两隔的无奈。 \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n典故：\u0026ldquo;兰因\u0026quot;出自《左传》\u0026ldquo;归妻茹姬，言兰之因\u0026rdquo;，喻美满姻缘。 直译：愿你们千年万代幸福，莫辜负天赐的缘分。 隐含矛盾： 容娘在诗句中作为\u0026quot;旁观者\u0026quot;祝福沈慕云，实为自我催眠式的安慰； \u0026ldquo;休负\u0026quot;二字暴露了她的脆弱：既然沈慕云早逝未娶，\u0026ldquo;兰因\u0026quot;早已成空，但鬼魂仍在固执地维护爱情的\u0026quot;尊严\u0026rdquo;。 超脱性：容娘最终接纳了悲剧，以诗意消解执念，身影消散的瞬间完成精神解脱。 三、诗句与故事的互文性 记忆的美化与重构\n诗句中的\u0026quot;小院深静\u0026quot;与\u0026quot;月笼明\u0026rdquo;，对应容娘虚构的\u0026quot;河桥赌约\u0026rdquo;，二者皆将痛苦记忆升华为唯美意象，体现情感对事实的改造。 对\u0026quot;时间\u0026quot;的超越\n\u0026ldquo;千秋岁好\u0026quot;既是对沈慕云的祝福，也是对自身存在的诘问：她以鬼魂之姿徘徊三十年，最终用诗句抹平时间刻度，与过去和解。 东方悲剧美学的典范\n全诗无激烈控诉，却以\u0026quot;雾中问月\u0026quot;\u0026ldquo;月下祝语\u0026quot;的含蓄方式，完成对命运无常的悲悯，契合传统文学中\u0026quot;哀而不伤\u0026quot;的美学标准。 四、结语：未完成的诗性正义 故事通过谎言、鬼魂、诗歌三重媒介，实现了另一种\u0026quot;正义\u0026rdquo;：容娘虽未能改写历史，却以美学方式完成了对残缺爱情的神圣化。而诗句\u0026quot;休负兰因\u0026quot;的矛盾性，正是东方文化中\u0026quot;执念与释怀\u0026quot;二元特质的精妙注脚。\n实验设定 作者：冉语优\nPrompt：\n我听过许多关于沈慕云的故事，只有这里的与众不同。有说他上京赴考，后来另聘了阁老家的贵女，也有说他早知得了不寿之症，不想教谁守寡罢了。只有这城里的老人告诉我，沈慕云初恋的少女叫做容娘，家就住在河桥对岸。两人已定了婚约，他赴考高中，到了那时却突然反悔，对容娘避之不见。“后来容娘伤心，从此再不见人。但此地人人都知道两人间的赌约：三十年后，故人故地，他敢不敢再来相见？”老人望了望我，接着说：“你来得巧。今日便是三十年整约定之日。城住着当年预备侍候她成亲的婢女，叫做谢娘，你既来了，可以求她领你去看。”_\r我谢过他，掉头走了。\n“无论多少年，都是一样的结果。”\n上桥的时候，我听他这样说。\n我见到了谢娘。此地的细雨绵绵无期，好似把每一个人的面目都笼在微湿的春雾里，看不真切，可我隐约感到已上了年纪的谢娘是美的。她当年侍候过的女主人——容娘一定更美吧？\n我向她道明来意。\n“啊，她和沈慕云的事儿呀……我不能告诉你”，她笑了笑，“但是我能带你去看。”\n她说完，不曾携伞，迎着屋外雨雾极快地走了出去。我跟了出去。不知怎么地，跟在她身边，我竟也能走得极快\n转眼间，我们走过了来时那座河桥，忽然，桥下河水巨涨，细雨变作滔天雨浪迎面拍来，一浪浪打两边栏杆。\n红妆、花轿、洪浪，幻幕般在我眼前一逝而过，我勉力闭眼，下意识逃避周遭的风浪，再睁眼时仍是小雨蒙濠，已然风平浪静。\n只有谢娘站在雨幕里，神情哀伤：“那一年，也是在这里，也是这样的天气，我在这里跌下了水，再没见到自己的少年郎。”\n我心头一震，这才发觉她身形虚浮，如临水上，宛然不似身在人世。“你是容娘？！”我问。\n她恢复了一点少女的笑容：“谢娘呀，悔婚呀，还有赌约，都是我编的。”\n“你为什么……”\n“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。就像天上的月亮，因缘注定，捞一捞终不能得，但宁可难平，也不愿放手”，她回过头来，露出那样天真的笑：“左右是在梦里，容我任性使坏编排他一回，他不会介意的吧。”\n我动容，一时无言。\n“他呢，他在人间怎么样？还好吗？”\n沈慕云，字薄君，天才绝人，年二十七早卒于世。\n“他很好”，想了想，我这样告诉她，“后来又娶了新妇，白头偕老。”\n她忽地莞尔，“这样啊……那真是很好很好，再好不过的事了。”\n我看她露出涩然的笑，喃喃念着什么，身影一点点地变淡消散。我听清了那一句词：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n时雨霏霏，三十年后的此地笼罩着三十年前一样凄迷的柳色，仿佛他们的初逢，点滴片刻，却望尽整座人间\n解读这个故事，并结合故事解读\u0026quot;小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因\u0026quot;的含义。\n","date":"3 October, 2025","id":0,"permalink":"/chinese-post/chichi-story/","summary":"实验设定放在最后。","tags":"","title":"AI怎么解读银临《迟迟》的背景故事？"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":1,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":2,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":3,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":4,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":6,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M\rnnz: 图边数（稀疏矩阵非零元素数，Num NonZero）\r规模为10n~1000n\rq, k, v: (n, d)\rA: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)):\rif M[i, j] != 0:\rout[i, j] = dot(A[i,:], B[:,j])\relse:\rout[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h)\r↓\rEach Block → (rid, hid): one node, one head\r↓\rLoad Q[rid, hid, :] → s_Q[f] (shm)\r↓\rFor each tile of neighbors (size ≤ 32):\r- Load neighbor IDs from indices[]\r- Compute Q · K^T (dot product using s_Q and K[dst])\r- Reduce in warp → store in neigh_nodes_weight[eid]\r- Find max(weight) in current tile → weightMax\r- Adjust partial_sum and acc with exp(old_max - new_max)\r- Compute exp(weight - weightMax) and accumulate acc += exp_w * V[]\r- Accumulate partial_sum += exp_w\r- Update weightMax_old\r↓\rFinal normalization: out_feat = acc / partial_sum\r↓\rWrite back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":7,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"7 June, 2025","id":9,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。\n组件 内存部分 磁盘部分 WAL 总体 初始化 需要 init flush thread。flush thread 的工作流程:\n等待 flush 信号量被 notify,获取一个 compact 信号量资源 启动一个 sstwriter,写入这个 memtable 一个 memtable 对一个 sst 等到写入 sst 写完之后,才进行: 从 frozen memtables、frozen memtable sizes 里面删除这个 memtable 从 wal 里面删除这个 memtable 对应的 wal update manifest Try Freeze 如果当前大小 \u0026gt; freeze size 那么就 freeze;进一步如果所有 frozen memtable 大小之和 \u0026gt; flush threshold,那么就 set flush signal。\n写操作 写 memtable 写 WAL try freeze 内存部分 Put 添加到 memtable; 更新 size。 size 不需要特别精确,只需要是一个大致的值即可。 Delete 添加一个 tomb 标记到 memtable Get 从 active memtable 中获取 从 new 到 old 遍历所有的 inactive memtable,获取。 磁盘部分 compact 信号量 二元信号量。\n需要 compact 的时候,添加资源 compact thread 开始 compact 的时候,消耗资源。 初始化 如果 auto compact 开启,初始化的时候需要 init compact thread:\nLevel 存储这个 level 所有文件对应的文件路径,装在 sst reader 里面\nGet (没有 delete, put) 从低到高,从新到旧,调用 sst 的 get 方法,获取 record。否则返回 none。\nInit Compact Thread Compact thread:\n等待 compact 信号量 依次查看每一层:如果这一层大小超过 threshold,就合并到下一层,否则就提前返回。 Compact 以 L0 -\u0026gt; L1 为例: 从前到后遍历所有的 kv-pair,同时维护:\nkeys_outdated 同一个 key,timetsamp 小于 oldest marker 的 kv pair 只需要保留一个。 keys_outdated 记录所有(出现过的,且 timestamp 小于 oldest marker)的 key L1 sst size 每达到一定值就关闭当前 sst,新开一个新的 sst。 更新 manifest。 SST writer 配置 max block size。\n每个 block 的开头一个 key 会添加到 index 中; 搜索这个 sst 的时候,会先对 index 进行二分查找; 在 block 之内采用线性搜索。 fpr,用于构建 bloom filter.\n写入 遍历所有的 kv pair: userkey(不含 timestamp)添加到 bloom filter; block 写入当前 kv; 如果当前 block 大小超过 max block size,就开启一个新的 block,然后写入对应的 index(内存) 将 index 和 bloom filter 写磁盘。 SST reader 查找: Get(key, timestamp) 查 bloom filter,如果不存在就返回。 将 index 整个载入内存中,进行二分查找,得到对应 key-timestamp 所在的区间。如果 out of bounds 就返回。 按照查找到的区间,读磁盘。 MVCC key 排布问题 struct Key bytes timestamp: u64 比较: key1 \u0026lt; key2:\nkey1.bytes \u0026lt; key2.bytes (字典序); 或者: key1.bytes == key2.bytes,而且 key1.timestamp \u0026gt; key2.timestamp 为什么这样比较? 在进行查询 Get(userkey, timestamp) 的时候,我们需要的是:\nuserkey 匹配 timestamp 小于查询的 timestamp,且尽可能大 因此,我们将\nuserkey 升序排序 timestamp 降序排序 在搜索 memtable(skiplist)的时候,或者对 index 进行二分查找的时候,就可以:\n直接使用 lower_bound,查找大于等于自己的第一个元素 如果 userkey 匹配,说明是 timestamp 小于当前 timestamp 的,timestamp 最大的记录,返回; 如果 userkey 不匹配,说明不存在 timestamp 小于当前 timestamp 的记录,返回(未找到)。 Transaction 数据结构 一个内存 tempmap,用来存储 transaction 已经写,但是未提交的内容。 创建的时候,从 tree 获取:\nstart timestamp,作为查询的 timestamp transaction id 然后写入 transaction start 到 WAL\nPut,Delete 写 tempmap,写 WAL\nGet 使用 start timestamp,先查 tempmap,再查 tree。\nCommit 从 tree 获取一个 commit timestamp; 写 WAL,记录 transaction id 和 commit timestamp。 在 replay 的时候,把 transaction id 和 commit timestamp 对应起来就可以知道 transaction 里面的 写操作 对应的 timestamp 调用 tree.active_memtable 的 API,将 transaction 的所有数据写入 tree 的 memtable。 WAL 看到 transaction start,先将 transaction 暂存到内存中:\n如果在 replay 结束之前看到了 transaction end,就将改动写入 tree 中(redo)。 否则放弃,视为没完成的事务(undo) 踩坑: Resource deadlock avoided (os error 35),可能是一个 thread 持有了自己的 joinhandle 并且 join 了自己;使用 maybe join 解决,即判断当前线程和 joinhandle 的线程是否一致,如果一致就不用 join。 死锁问题: wal 和 mem 都有锁,必须 按照同一顺序获取 才不会出现死锁。 Bloom filter 细节 本部分由 Deepseek 辅助写作\n该 Bloom filter 算法的主要步骤如下:\n参数计算:\n根据预期元素数量 n 和可接受误判率 p,通过公式计算最优位数 m 和哈希函数数量 k: $ m=\\lceil-n \\dfrac{\\ln(p)}{\\ln(2) ^ 2}\\rceil $ $ k=\\lceil\\dfrac{m}{n}\\ln(2)\\rceil $ 当直接指定参数时,使用给定的位数和哈希函数数量 哈希生成:\n使用 64 位指纹哈希(farmhash)生成初始哈希值 h 通过位运算构造增量值 delta = (h \u0026gt;\u0026gt; 33) | (h \u0026lt;\u0026lt; 31) 采用双重哈希技术,通过循环叠加 delta 生成 k 个不同的位位置: $ h_i \\equiv h + i \\cdot delta \\pmod m , 0 \\leq i \\lt k $ 数据插入:\n对输入 key 进行哈希计算得到初始 h 和 delta 循环 k 次生成位位置,将位数组中对应位置设为 1 采用位操作: byte_index = position/8,bit_mask = 1 \u0026laquo; (position%8) 存在性检测:\n重复插入时的哈希计算过程 检查所有 k 个对应位是否均为 1 任一位置为 0 则判定不存在,全部为 1 时判定可能存在 数据持久化:\n序列化时附加 CRC32 校验和 反序列化时验证校验和与数据完整性 ","date":"1 March, 2025","id":10,"permalink":"/chinese-post/lsm/","summary":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。","tags":"","title":"Lsm Tree 实现备注"},{"content":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。\n1. 问题 对于实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $b \\in \\mathbb{R}^n$，求解\n$$Ax = b$$或者，等价的，\n$$\\text{argmin}_x f(x)$$其中\n$$f(x) = \\frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：\n$$a^{-2}x^2 + b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：\n这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。\n2.2. 最速下降法 首先，你应该知道梯度下降法：\n$$x_{i+1} = x_i - \\alpha\\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：\n$$\\alpha^* = \\text{argmin}_\\alpha f(x_i - \\alpha\\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量\n$r_i$：$b_i - Ax_i$，目标函数 $f(x)$ 在 $x_i$ 点的负梯度，或者线性方程组在 $x_i$ 点的残差。\n请记住：负梯度和残差是一个东西！ $d_i$：在 $x_i$ 点的搜索方向。最速下降算法里 $d_i = r_i$，共轭梯度里面需要一点修正。\n3.2. 最速下降 最速下降的新方向：$r_{i+1}$\n新方向与前一步下降方向 $r_i$ 垂直（画个等高线图直观理解，或者回想一下\u0026quot;等势面和电场线垂直\u0026quot;） 最速下降的 $\\alpha$ $$\\alpha_i = \\frac{r_i^T r_i}{d_i^T A d_i}$$3.3. 共轭梯度 我们直接逐项类比最速下降。\n新方向与前一步下降方向 $r_i$ 垂直 斜率之积为 $-a^{-2}b^2$ (Section 2.1)\n这个方向由最速下降的方向进行一些小改动得到，我们可以在后面的算法部分(Section 3.4)看到。把这个方向从和前一个搜索方向垂直改动到斜率之积为 $-a^{-2}b^2$ 就是 CG 和最速下降唯一不同的地方。 步长 $\\alpha$：由于是在一条直线上做优化，因此和最速下降的 $\\alpha$ 相同。 由于一次迭代只涉及到两个点、两个向量，只能构成一个平面，我们甚至不需要将二维向多维推广。\n若需推导，我们需要做的只是把点的 $n$ 维坐标映射到二维，然后对截面椭圆对应的二阶二次型进行 SVD 获得其长轴 $a$ 和短轴 $b$，进而根据其离及上述斜率积的二级结论计算两个方向的关系。这里不展开。 3.4. 算法 3.4.1. 初始化 算法输入：$A, b, x_0$\n$$\\vec{d}_{(0)} = \\vec{r}_{(0)} = \\vec{b}_{(0)} - \\mathbf{A}\\vec{x}_{(0)}$$3.4.2. 算法过程 $$\\alpha_{(i)} = \\frac{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}{\\vec{d}_{(i)}^T \\mathbf{A}\\vec{d}_{(i)}}$$$$\\vec{x}_{(i+1)} = \\vec{x}_{(i)} + \\alpha_{(i)}\\vec{d}_{(i)}$$$$\\vec{r}_{(i+1)} = \\vec{r}_{(i)} - \\alpha_{(i)}\\mathbf{A}\\vec{d}_{(i)}$$$$\\beta_{(i+1)} = \\frac{\\vec{r}_{(i+1)}^T \\vec{r}_{(i+1)}}{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}$$$$\\vec{d}_{(i+1)} = \\vec{r}_{(i+1)} + \\beta_{(i+1)}\\vec{d}_{(i)}$$其中的最后一步就是通过 $\\beta$ 将 $r_{i+1}$ 修正成 $d_{i+1}$ 的。\n3.4.3. 起讫 起：如果你对解 $x$ 有粗略的估计，就使用那个值作为起始点 $x_0$；否则，直接使用 $x_0 = 0$。\n讫：通常的做法是在残差向量的 2-norm 小于某个给定阈值的时候就停下来。通常这个阈值为初始残差的一小部分\n$$\\|r_i\\| \u003c \\varepsilon \\|r_0\\|$$其中 $\\varepsilon$ 是一个输入的参数。\n3.5. 杂项 由于 $Ad_i$ 在每个循环中都要被计算，且 $$r_{i+1} = r_i - \\alpha_i A d_i$$故可以用上式计算 $r_{i+1}$，而不必用 $b - Ax_{i+1}$。\n上述方法有浮点误差累计的危险，因此我们应该每过几个循环就重新用 $r_i = b - Ax_i$ 重新计算残差。 ","date":"7 December, 2024","id":11,"permalink":"/chinese-post/conj-grad/","summary":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。","tags":"","title":"共轭梯度：高中解析几何的拓展"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":12,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"},{"content":"实验设定放在最后。\nKimi K2 这个故事是一个关于记忆、爱情与执念的幽怨传说，表面讲的是沈慕云与容娘的“负约”，实则讲的是 “如何不让一段真情被时间吞没” ——哪怕要用谎言、用鬼魂、用一座城三十年不散的闲言碎语，也要把那个人留在人间。\n容娘（化作“谢娘”）自编自演了一出“赌约”，把负心、等待、重逢这些最戏剧化的扣子全缝进传闻里，让一代又一代外乡人替她“记得”。真正的负约者不是沈慕云，而是时间；真正的守约者也不是容娘，而是“故事”本身——故事替他们完成了“三十年后故地重见”的誓言。\n一、逐句解词 小院深静，欲问卿卿，何处月笼明。\n“小院深静”——当年二人初会的场景：河桥对岸的小院，春月微湿，一切尚未发生。 “欲问卿卿”——想开口，却终究没问；一声“卿卿”把未出口的千言万语都压在舌尖。 “何处月笼明”——月亮仍在，却照不到当年并肩看月的人；也暗指“容娘”自己已成幽魂，“月”是人间与冥界的唯一通道，她再也回不去了。 千秋岁好，休负兰因。\n“千秋岁好”——字面是祝颂：愿你长享百岁千秋，岁岁安好。 “休负兰因”——“兰因”出自《左传》“兰因絮果”，兰因＝美好的开端，絮果＝离散的结局。\n容娘把典故截半，只留“兰因”，是把结局生生改写成“花开未谢”——她不要“絮果”，只要“兰因”永存；换言之，“你若安好，便不算负我”。 合在一起，是鬼魂对活人最温柔的赦令：你不必自责，不必回头，甚至不必记得我；只要你在人间岁岁平安，我们那一段“兰因”就从未被辜负。 二、整首小词在故事里的功能 替沈慕云“平反”\n城里传说他“高中负心”，而容娘最后一句“休负兰因”等于亲口否认了“负”字——“负”是世人附会，“兰因”才是她私藏的真相。 替自己“松绑”\n她等了三十年，其实等的不是沈慕云，而是一个可以自我释怀的叙事版本——“千秋岁好”四个字出口，执念才终于落地，鬼魂才能消散。 把“故事”升华为“公共记忆”\n词里“月笼明”是开放画面，任何后来者（包括“我”）只要站在河桥细雨里，就自动成为新的“卿卿”。故事不死，兰因就不死；人间继续流传，她就能继续借他人的心头一点月光，与沈慕云并肩。 三、总结 “小院深静……”二十一字，是容娘亲手写给自己的“反向墓志铭”：\n——不写“我曾恨你”，写“愿你安好”；\n——不写“我等你”，写“休负兰因”；\n——不写“故事结束”，写“月笼明”留给下一位行人。\n她用最轻的语调，把最重的执念吹散在雨里：\n“只要世上还有人替我传诵这一句，你就从未真正离开我。”\nClaude Sonnet 4.5 这是一个凄美动人的志怪爱情故事，充满了中国古典文学的浪漫与哀愁。让我为你解读：\n故事解读 真相的反转 故事最精妙之处在于层层剥开的真相:\n表面上是沈慕云负心悔婚的俗套故事 实际上容娘早已溺水身亡，她自己编造了所有传言 她化身\u0026quot;谢娘\u0026quot;引导\u0026quot;我\u0026quot;，只为让世人记住他们的故事 深藏的深情 容娘的所作所为揭示了她至深的爱:\n编造负面传言：让沈慕云背负薄情之名，实则是她对这段感情念念不忘的证明 \u0026ldquo;三十年之约\u0026rdquo;：明知他不会来（因为她已是鬼魂），仍要给自己一个等待的理由 最后的谎言：当得知沈慕云27岁早逝，她选择相信\u0026quot;我\u0026quot;善意的谎言，带着他幸福的假象消散 悲剧的本质 两人都是命运的牺牲品——一个溺水早逝，一个27岁早卒。他们的爱情从未因背叛而破碎，而是被死亡生生拆散。容娘那句\u0026quot;因缘注定，捞一捞终不能得\u0026quot;正是这无奈的注脚。\n词句深意解读 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n字面含义：深深的庭院静悄悄，想要问我的爱人，哪里的月光最明亮？ 故事中的含义： \u0026ldquo;小院深静\u0026quot;是她独自等待的孤寂处境 \u0026ldquo;欲问卿卿\u0026quot;是她想问沈慕云，却永远无法相见的遗憾 \u0026ldquo;何处月笼明\u0026quot;是在问：你在人间何处？那里的月色可好？我们还能否在同一轮明月下相望？ \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n字面含义：愿你千秋万世安好，不要辜负我们美好的缘分 故事中的含义： 这是容娘最后的祝福与释怀 \u0026ldquo;千秋岁好\u0026rdquo;：既是祝福沈慕云（虽然他已逝），也是对这段感情的纪念 \u0026ldquo;休负兰因\u0026rdquo;：不是责怪他辜负，而是说\u0026quot;我们的缘分虽短暂如兰花，但请你记得它的芬芳，不要忘记我们曾经相爱\u0026rdquo; 最动人之处 容娘明知沈慕云已死，却仍要编织\u0026quot;他白头偕老\u0026quot;的幻象来安慰自己；她\u0026quot;任性使坏编排他一回\u0026rdquo;，实则是用全部的爱为他守护名声——宁可世人说她痴情被负，也不愿他在冥冥中孤独。\n那句\u0026quot;捞一捞终不能得，但宁可难平，也不愿放手\u0026rdquo;，是对所有求而不得之爱最诗意的诠释：即使知道不可能，也要在记忆里、在传说里，让这份爱永远鲜活。\n这正是中国古典爱情故事的极致浪漫——生死相隔，却以魂灵相守；真相残酷，却以谎言温柔。\nGemini 3 Flash 这是一个凄美而富有诗意的爱情故事，它通过“我”的视角，揭示了一个被时间、流言和自我幻想所包裹的悲剧真相。故事的魅力在于其层层剥开的悬念，以及对爱、遗憾、和记忆的深刻探讨。\n故事解读 1. 传说的建构与真相的揭示 故事首先提出了关于沈慕云悔婚的三个不同版本，其中城里老人讲述的“赌约”版本最有戏剧性，也吸引了“我”的探访。\n流言的本质： 无论是“另娶贵女”还是“不寿之症”，流言都在试图为沈慕云的悔婚提供一个合理的解释，以填补事实的空白。 “赌约”的虚构： “三十年故地重逢”的赌约是 容娘（化名谢娘） 为了让自己的故事能够被记住、被流传而编造的浪漫情节。它赋予了这段逝去的爱情一个等待和希望的维度，一个值得他人关注的理由。 真相的悲剧： 最终的真相是谢娘（容娘）在那年赴考后的雨天，跌下河桥溺亡，再也未能见到自己的少年郎。沈慕云的“悔婚”和“避而不见”并非是变心或绝情，而是因为容娘已经不在人世。她用谢娘的身份和编造的故事，让这段短暂的初恋以一种“被辜负”的、更具戏剧张力的方式存留于世人记忆中。 2. 人物与主题 人物/元素 象征意义 故事中的作用 容娘/谢娘 执念、永恒的初恋 故事的核心叙事者。她虽已逝去，却不甘被遗忘，通过虚构的故事维护了自己爱情的尊严和存在感。她的“任性使坏”是为了让他们的故事在人间留下印记。 沈慕云 逝去的理想、悲剧的命运 在故事的结局中，他的命运是 “年二十七早卒” 。这使得他无法真的“悔婚”，也使得容娘的悲剧更添一重宿命的哀伤。但他“很好”的结局是容娘最后的自我安慰。 河桥与雨 命运的转折点、阴阳相隔 河桥是两人定情与分离的象征，而 “滔天雨浪” 的幻境则是容娘逝世时的情景重现，明确了她的鬼魂身份。雨是故事基调，凄迷、绵延不绝。 “我” 倾听者、故事的传播者 “我”是故事进入现实的通道，作为记忆的载体，将容娘的心愿传递下去。 3. 爱与遗憾 容娘的内心充满难平的遗憾和执念。她深爱着沈慕云，宁愿被认为是“被悔婚”的弃妇，也要让这段爱情的故事流传，而不是被淹没在无声的消亡中。\n当“我”为了安慰她，编造沈慕云“后来又娶了新妇，白头偕老”的谎言时，她虽然“涩然”，却说“这样啊……那真是很好很好，再好不过的事了。”这显示了她至深的爱与善良：她宁愿相信爱人获得了幸福，也不愿他为她而终生抱憾。\n“小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因”的含义 这句话是容娘临终或魂魄消散前喃喃念出的词句，是她对这段感情、对沈慕云最后的总结、疑问与祝愿。\n1. 拆解词句 小院深静，欲问卿卿： 描写了昔日两人相处的场景——静谧、美好的小院。这里的“卿卿”指代沈慕云。她在这个安静的回忆里，想要向他提问。 何处月笼明： 这是一个疑问句，也是一个哲学式的追问。“月笼明”指月光洒下，通常象征着光明、团圆或美好的结局。容娘想问的是： “我们的爱和美好（月光）最终在哪里得以安放和实现？” “我的少年郎，你现在是否幸福安康？” 这包含了她对爱人境况的牵挂，以及对逝去爱情终极归宿的迷惘。 千秋岁好： 意思是 “愿你长久地、永远地安好”。这是容娘对沈慕云发自内心的祝愿，也是她对“我”所编造的“白头偕老”结局的回应。 休负兰因： 核心是 “兰因”。 “兰因”源于《聊斋志异·莲香》中的“兰因絮果”，意指美好的开始，悲惨的结局（兰是香草，象征美好；絮是柳絮，随风飘散，象征离散）。 “休负” 是“不要辜负”的意思。 整体含义： 容娘以自己的悲剧为鉴，在消散的最后时刻，希望沈慕云（或祝愿世间所有有情人）不要辜负这美好的开始（即她与沈慕云曾拥有的纯粹初恋），要有一个好的结局。她既在对自己逝去的爱表示叹息，也在对爱人寄予了获得幸福的殷切期望。 2. 整体含义 这句话是容娘执念的温柔释放。它承载了她对过往美好时光的留恋、对爱人未来幸福的祝愿，以及对这段“兰因絮果”式悲剧爱情的坦然接受与超脱。她不再抱怨，而是带着对沈慕云深深的牵挂与祝福，消散于人世的细雨中。\nGPT5 这个故事本身就是一段借幻境和传说交织出的“人世与记忆、真实与虚构”的爱情传说。我们可以从三个层面来解读：\n一、故事的内核 表层叙事\n人们口口相传着沈慕云和容娘的故事，有多种版本：悔婚、病逝、守寡。 但实际上，这些都不是真的，是容娘自己虚构、编织出来的。她用不同的叙述“保存”了这段情感，使它不会被彻底遗忘。 真相：容娘年轻时便意外溺水而亡，和沈慕云的情缘戛然而止。 容娘的心意\n她不是想责怪谁，而是希望他们的故事能被记得。 她愿意编造悔婚、赌约的传说，只为在世间留下痕迹。哪怕人们误解她或沈慕云，也比“彻底消散”要好。 最终的温柔\n当她问“他在人间怎么样”时，叙述者选择撒了一个善意的谎，说他娶妻白首。 容娘听了，便真心觉得“那真是很好很好”。 她释然，带着笑意离开，只留下词句。 二、容娘留下的词句 “小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n这首词可以这样理解：\n小院深静 ——环境空寂，暗示她的幽魂状态，也象征往昔爱情已成过往，唯有寂静陪伴。\n欲问卿卿，何处月笼明 ——她想问“卿卿”（她的少年郎，沈慕云），如今在何处？月光清明，却无法指引他们重逢。 ——“月”既是天上明月，也是寄托的情感象征：相隔阴阳，却共享一轮明月。\n千秋岁好 ——祝愿对方一生顺遂，千秋岁月都美好。\n休负兰因 ——“兰因”可指美好的情缘、纯洁的心意。她的愿望是：哪怕不能再相见，也不要辜负他们曾经的真挚。\n三、结合故事的整体解读 这首词就像是容娘最后的“寄语”：\n既是思念：她仍想追问“卿卿，你如今可好？” 也是祝福：哪怕自己已成往事，也愿对方在人间顺遂，不负青春年华。 更是执念的柔化：从怨与伤心，转为祝愿和守护。 于是，这句话的含义可以总结为： 👉 即便情缘如月，终究捞不着，但在心底，仍愿你岁岁安好，不负当年那份真心。\nDeepSeek R1 这是一个凄美而精巧的故事，它并非简单的负心汉传说，而是一则关于记忆、执念与深情告别的古典式志怪小说。故事的真相在结尾被彻底颠覆，而那句词，正是解开所有谜题的钥匙。\n故事解读：一场持续三十年的梦\n这个故事有多重反转，其核心结构如下：\n表层故事（传闻）：沈慕云高中悔婚，另娶贵女，留下一个三十年的赌约。这是一个符合世俗想象的、关于负心与等待的悲剧。\n中层故事（谢娘的讲述）：谢娘是容娘的婢女，她带领“我”经历幻象，并揭示自己才是悲剧的主角——她因洪水失去了自己的少年郎。这似乎将故事引向了另一个错位爱情的方向。\n深层故事（真相）：根本没有谢娘，也没有悔婚。眼前的魂魄就是容娘本人。沈慕云也并非负心，而是早已在二十七岁英年早逝。 整个“负心汉与三十年赌约”的故事，是容娘的鬼魂因为思念至深而编造的。\n容娘为何要编造故事？\n故事中点明了原因：“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。” 真实的悲剧是死亡带来的永诀，这过于绝对和寂静，会随着时间被世人遗忘。而一个带有“负心”、“赌约”等戏剧性元素的故事，反而更具传播力，更能让他们的爱情在人们的口耳相传中“活”下去。她守护的不是清白，而是记忆本身。\n“我”的角色与结局：“我”作为一个误入此地的听故事者，成为了容娘倾诉的对象。当容娘问起沈慕云在人间如何时，“我”那句善意的谎言——“他很好，后来又娶了新妇，白头偕老”——是故事的点睛之笔。这并非残忍，而是一种最深切的慈悲。它告诉执着等待的魂魄：你爱的那个人，在人间圆满终老，你无需再挂念了。这个谎言，给了容娘“放下”的理由。于是，她念出那句词，释然消散。\n环境象征：贯穿故事的“雨”，既是现实天气，也是哀伤的氛围烘托，更是三十年前那场生死洪水的象征。故事开头和结尾的“雨”与“柳色”形成闭环，暗示这一切都笼罩在容娘执念所化的、永恒凄迷的梦境里。\n“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因” 含义解读\n这句词是容娘的独白，也是她对沈慕云跨越生死的情话。结合故事真相，其含义层层递进：\n“小院深静，欲问卿卿，何处月笼明。”\n字面意境：庭院幽深寂静，我想问一问你，此刻哪里的月光正明亮地笼罩着（你）？ 故事深意：这描绘了容娘作为魂魄的孤独处境。“小院深静”是她三十年来徘徊之地的寂寥写照。“欲问卿卿”是极亲昵的呼唤，对象是早已不在人世的沈慕云。“何处月笼明”是一个跨越阴阳的发问：你的魂魄现在何处？是否安好？这句充满了无处寄托的牵挂与温柔探询。 “千秋岁好，休负兰因。”\n字面典故：“千秋岁好”是祝愿岁月长久美好；“兰因”典出《左传》，郑文公妾燕姞梦见天使赠兰，后生下公子兰，比喻像兰花一样美好的前因、命中注定的姻缘。“兰因絮果”则常指开端美好而结局凄凉。此处“休负兰因”即“不要辜负了我们美好的开端”。 故事深意：这是整首词的情感核心，也是容娘的释然之语。 在听到“我”的谎言之前，这或许是她固执的愿望：希望对方不要辜负当初的情缘。 但在听到“他很好”之后，这句话的含义彻底改变。 她将“我”的谎言信以为真：原来他在人间拥有了“千秋岁好”的圆满人生。于是，“休负兰因”从一句怨怼或期盼，变成了一句宽恕和祝福：既然你已有了美好的一生，那我便不再执着，不让你我当初美好的姻缘（兰因）成为束缚你的负累。我放过你，也放过我自己。 整体含义：这阙词是容娘对沈慕云的最终告别。她在一个人的“小院深静”中孤独思念了三十年，最终选择相信爱人已在人间获得“千秋岁好”，于是她决定“休负兰因”——放下执念，解脱这段因缘，让彼此都自由。\n这个故事的精妙之处在于，它用一个“谎言”（容娘编的故事）化解了另一个“谎言”（“我”说的善意的谎言），最终达成了最深层的真实：爱到极致，不是占有，而是希望对方幸福，哪怕这幸福与自己无关，甚至需要靠一个虚构的故事来成全。 那场三十年的雨，终于停了。\nQwen3-Max 这个故事是一则充满古典意蕴与幽微情感的志怪式短篇，融合了现实与幻梦、记忆与虚构、执念与释怀。它以“三十年之约”为引，层层剥开一段被时间掩埋的爱情往事，最终揭示出：所谓“故事”，不过是生者对逝者、对未竟之情的温柔重构。\n一、故事解读：一场由亡魂编织的梦 叙事的反转与真相的揭示\n故事开篇以“我”听闻关于沈慕云的多种传闻为引，最动人的是“容娘与赌约”的版本。老人指引“我”去找谢娘，看似是引路人，实则为幻境开启的契机。而当“我”见到谢娘，随她过桥、遇风雨、见幻象，最终才惊觉——谢娘即容娘，而“谢娘”只是她为讲述故事而虚构的身份。\n这一反转极具震撼力：容娘早已溺亡于三十年前的那场雨中，她的魂魄滞留人间，只为守候一个不可能兑现的约定。她编造“悔婚”“赌约”“婢女谢娘”等情节，实则是为了赋予这段无果之爱一个可被传颂的形态——哪怕是以怨恨、误解的方式，也胜过彻底被遗忘。\n“梦”的本质：执念与慈悲的交织\n容娘说：“左右是在梦里，容我任性使坏编排他一回。”\n这句话道出了她全部的温柔与心酸。她明知沈慕云早逝，却仍愿在梦中为他安排一个“白头偕老”的结局；她虚构自己被负心，实则是不愿世人知道他英年早逝、她殉情而亡的悲剧，更不愿他的名声因“负心”而受损。她的“编排”，是爱的另一种守护。\n“我”的角色：记忆的承接者\n“我”作为外来者，既是听众，也是见证者。当“我”选择隐瞒沈慕云早逝的真相，告诉她“他很好，白头偕老”，这并非欺骗，而是一种共谋式的慈悲——成全她最后的幻想，让她带着“他过得很好”的安心离去。这正呼应了容娘自己的善意：她宁愿被误解为被弃之人，也不愿世人知道他早夭。\n意象的象征\n河桥：连接现实与幻境、生与死的边界。 细雨/洪浪：情感的外化。细雨是绵长的思念，洪浪是记忆爆发的瞬间。 红妆花轿：未完成的婚礼，永远定格在幻象中。 柳色凄迷：古典诗词中常喻离别与哀愁，此处象征时光流转而情意不灭。 二、词句解读：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。” 这几句词虽为虚构，却深得宋词婉约之神髓，可视为容娘心声的凝练表达。\n“小院深静，欲问卿卿，何处月笼明”\n小院深静：回忆中两人初遇或定情之所，幽深宁静，充满私密温情。 欲问卿卿：“卿卿”为古时爱称，此处是容娘对沈慕云的深情呼唤。 何处月笼明：化用杜牧“烟笼寒水月笼沙”之意象。“月笼明”既指月光朦胧，亦喻情缘如月，看似明亮却不可触及。她在问：当年共赏明月之人，如今在何方？那轮见证誓言的月亮，是否还记得我们的约定？ 此句整体营造出一种追忆往昔、寻而不得的怅惘，是容娘三十余年来魂牵梦萦的写照。\n“千秋岁好，休负兰因”\n千秋岁：词牌名，亦指漫长岁月。此处双关，既指时间久远，也暗含祝福。 兰因：典出《周易·系辞上》“二人同心，其利断金；同心之言，其臭如兰。”后以“兰因”喻美好的姻缘或初心。“兰因絮果”则指初时美好、终成离散的姻缘。 休负兰因：意为“不要辜负当初那如兰般芬芳的誓约”。 然而，此处的“休负兰因”并非责备，而是临别前的温柔叮咛。她明知他已不在人世，却仍以梦中之语，嘱他“莫负初心”——实则是对自己执念的放下：纵使结局不如愿，我们曾有的“兰因”已是足够美好，不必怨，不必悔。\n三、整体主旨：爱的最高形式是成全与铭记 这个故事并非简单的“负心汉与痴情女”套路，而是一曲关于记忆、虚构与爱的尊严的挽歌。容娘以亡魂之身，编织一个“被辜负”的故事，只为让这段感情不被时间湮灭；而“我”以善意的谎言，成全她最后的体面。沈慕云虽早逝，却因她的守护，在传说中“白头偕老”——这何尝不是另一种永恒？\n那句词，既是她对往昔的回望，也是对未来的祝福。“千秋岁好，休负兰因”，不只是对沈慕云说，更是对所有听故事的人说：\n纵使缘浅，莫忘初心；纵使成空，爱已永恒。\n而那场三十年后的雨，依旧如初逢时般凄迷——因为有些爱，从未离开。\nLongCat 一、故事核心主题 记忆与执念的虚实交织\n故事通过\u0026quot;谢娘\u0026quot;的谎言（实为容娘的鬼魂）构建了多层叙事：表面是沈慕云负心的传说，实则是容娘对逝去的爱情与执念的投射。她借\u0026quot;赌约\u0026quot;之名，让故事流传千年，避免被世人遗忘。 \u0026ldquo;我们的故事总有人记得才好\u0026rdquo;——容娘的核心诉求，体现了人类对永恒记忆的追求，即使以谎言为代价。 生死与遗憾的象征\n暴雨中的幻象（红妆、花轿、洪浪）是容娘记忆的显影，暗示她的\u0026quot;婚姻\u0026quot;被洪水（命运）吞噬，少年郎（沈慕云）的离去是她无法释怀的痛。 \u0026ldquo;像天上的月亮，捞一捞终不能得\u0026rdquo;——以月亮隐喻理想化的爱情，虽不可得，但执念如月光笼罩人间，形成诗意化的遗憾。 真相与善意的谎言\n叙述者最后选择欺骗容娘（谎称沈慕云再婚），完成了对亡灵的救赎。这一细节揭示：人间对亡者的悲悯，有时在于编造圆满的结局。 沈慕云早卒、容娘化鬼的悲剧，被\u0026quot;坊间流传的故事\u0026quot;赋予不同版本，说明集体记忆对历史的再创造。 二、诗句逐层解析 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n表层：寂静小院中，爱人（卿卿）的身影与月色交叠，却因薄雾难寻其踪迹。 深层：\u0026ldquo;月笼明\u0026quot;象征记忆的朦胧性——容娘对沈慕云的思念似月光明媚，却也如雾气般模糊难触。 **\u0026ldquo;欲问\u0026rdquo;**的未完成时态，强化了阴阳两隔的无奈。 \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n典故：\u0026ldquo;兰因\u0026quot;出自《左传》\u0026ldquo;归妻茹姬，言兰之因\u0026rdquo;，喻美满姻缘。 直译：愿你们千年万代幸福，莫辜负天赐的缘分。 隐含矛盾： 容娘在诗句中作为\u0026quot;旁观者\u0026quot;祝福沈慕云，实为自我催眠式的安慰； \u0026ldquo;休负\u0026quot;二字暴露了她的脆弱：既然沈慕云早逝未娶，\u0026ldquo;兰因\u0026quot;早已成空，但鬼魂仍在固执地维护爱情的\u0026quot;尊严\u0026rdquo;。 超脱性：容娘最终接纳了悲剧，以诗意消解执念，身影消散的瞬间完成精神解脱。 三、诗句与故事的互文性 记忆的美化与重构\n诗句中的\u0026quot;小院深静\u0026quot;与\u0026quot;月笼明\u0026rdquo;，对应容娘虚构的\u0026quot;河桥赌约\u0026rdquo;，二者皆将痛苦记忆升华为唯美意象，体现情感对事实的改造。 对\u0026quot;时间\u0026quot;的超越\n\u0026ldquo;千秋岁好\u0026quot;既是对沈慕云的祝福，也是对自身存在的诘问：她以鬼魂之姿徘徊三十年，最终用诗句抹平时间刻度，与过去和解。 东方悲剧美学的典范\n全诗无激烈控诉，却以\u0026quot;雾中问月\u0026quot;\u0026ldquo;月下祝语\u0026quot;的含蓄方式，完成对命运无常的悲悯，契合传统文学中\u0026quot;哀而不伤\u0026quot;的美学标准。 四、结语：未完成的诗性正义 故事通过谎言、鬼魂、诗歌三重媒介，实现了另一种\u0026quot;正义\u0026rdquo;：容娘虽未能改写历史，却以美学方式完成了对残缺爱情的神圣化。而诗句\u0026quot;休负兰因\u0026quot;的矛盾性，正是东方文化中\u0026quot;执念与释怀\u0026quot;二元特质的精妙注脚。\n实验设定 作者：冉语优\nPrompt：\n我听过许多关于沈慕云的故事，只有这里的与众不同。有说他上京赴考，后来另聘了阁老家的贵女，也有说他早知得了不寿之症，不想教谁守寡罢了。只有这城里的老人告诉我，沈慕云初恋的少女叫做容娘，家就住在河桥对岸。两人已定了婚约，他赴考高中，到了那时却突然反悔，对容娘避之不见。“后来容娘伤心，从此再不见人。但此地人人都知道两人间的赌约：三十年后，故人故地，他敢不敢再来相见？”老人望了望我，接着说：“你来得巧。今日便是三十年整约定之日。城住着当年预备侍候她成亲的婢女，叫做谢娘，你既来了，可以求她领你去看。”_\n我谢过他，掉头走了。\n“无论多少年，都是一样的结果。”\n上桥的时候，我听他这样说。\n我见到了谢娘。此地的细雨绵绵无期，好似把每一个人的面目都笼在微湿的春雾里，看不真切，可我隐约感到已上了年纪的谢娘是美的。她当年侍候过的女主人——容娘一定更美吧？\n我向她道明来意。\n“啊，她和沈慕云的事儿呀……我不能告诉你”，她笑了笑，“但是我能带你去看。”\n她说完，不曾携伞，迎着屋外雨雾极快地走了出去。我跟了出去。不知怎么地，跟在她身边，我竟也能走得极快\n转眼间，我们走过了来时那座河桥，忽然，桥下河水巨涨，细雨变作滔天雨浪迎面拍来，一浪浪打两边栏杆。\n红妆、花轿、洪浪，幻幕般在我眼前一逝而过，我勉力闭眼，下意识逃避周遭的风浪，再睁眼时仍是小雨蒙濠，已然风平浪静。\n只有谢娘站在雨幕里，神情哀伤：“那一年，也是在这里，也是这样的天气，我在这里跌下了水，再没见到自己的少年郎。”\n我心头一震，这才发觉她身形虚浮，如临水上，宛然不似身在人世。“你是容娘？！”我问。\n她恢复了一点少女的笑容：“谢娘呀，悔婚呀，还有赌约，都是我编的。”\n“你为什么……”\n“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。就像天上的月亮，因缘注定，捞一捞终不能得，但宁可难平，也不愿放手”，她回过头来，露出那样天真的笑：“左右是在梦里，容我任性使坏编排他一回，他不会介意的吧。”\n我动容，一时无言。\n“他呢，他在人间怎么样？还好吗？”\n沈慕云，字薄君，天才绝人，年二十七早卒于世。\n“他很好”，想了想，我这样告诉她，“后来又娶了新妇，白头偕老。”\n她忽地莞尔，“这样啊……那真是很好很好，再好不过的事了。”\n我看她露出涩然的笑，喃喃念着什么，身影一点点地变淡消散。我听清了那一句词：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n时雨霏霏，三十年后的此地笼罩着三十年前一样凄迷的柳色，仿佛他们的初逢，点滴片刻，却望尽整座人间\n解读这个故事，并结合故事解读\u0026quot;小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因\u0026quot;的含义。\n","date":"3 October, 2025","id":0,"permalink":"/chinese-post/chichi-story/","summary":"实验设定放在最后。","tags":"","title":"AI怎么解读银临《迟迟》的背景故事？"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":1,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":2,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":3,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":4,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":6,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M\rnnz: 图边数（稀疏矩阵非零元素数，Num NonZero）\r规模为10n~1000n\rq, k, v: (n, d)\rA: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)):\rif M[i, j] != 0:\rout[i, j] = dot(A[i,:], B[:,j])\relse:\rout[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h)\r↓\rEach Block → (rid, hid): one node, one head\r↓\rLoad Q[rid, hid, :] → s_Q[f] (shm)\r↓\rFor each tile of neighbors (size ≤ 32):\r- Load neighbor IDs from indices[]\r- Compute Q · K^T (dot product using s_Q and K[dst])\r- Reduce in warp → store in neigh_nodes_weight[eid]\r- Find max(weight) in current tile → weightMax\r- Adjust partial_sum and acc with exp(old_max - new_max)\r- Compute exp(weight - weightMax) and accumulate acc += exp_w * V[]\r- Accumulate partial_sum += exp_w\r- Update weightMax_old\r↓\rFinal normalization: out_feat = acc / partial_sum\r↓\rWrite back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":7,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"7 June, 2025","id":9,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。\n组件 内存部分 磁盘部分 WAL 总体 初始化 需要 init flush thread。flush thread 的工作流程:\n等待 flush 信号量被 notify,获取一个 compact 信号量资源 启动一个 sstwriter,写入这个 memtable 一个 memtable 对一个 sst 等到写入 sst 写完之后,才进行: 从 frozen memtables、frozen memtable sizes 里面删除这个 memtable 从 wal 里面删除这个 memtable 对应的 wal update manifest Try Freeze 如果当前大小 \u0026gt; freeze size 那么就 freeze;进一步如果所有 frozen memtable 大小之和 \u0026gt; flush threshold,那么就 set flush signal。\n写操作 写 memtable 写 WAL try freeze 内存部分 Put 添加到 memtable; 更新 size。 size 不需要特别精确,只需要是一个大致的值即可。 Delete 添加一个 tomb 标记到 memtable Get 从 active memtable 中获取 从 new 到 old 遍历所有的 inactive memtable,获取。 磁盘部分 compact 信号量 二元信号量。\n需要 compact 的时候,添加资源 compact thread 开始 compact 的时候,消耗资源。 初始化 如果 auto compact 开启,初始化的时候需要 init compact thread:\nLevel 存储这个 level 所有文件对应的文件路径,装在 sst reader 里面\nGet (没有 delete, put) 从低到高,从新到旧,调用 sst 的 get 方法,获取 record。否则返回 none。\nInit Compact Thread Compact thread:\n等待 compact 信号量 依次查看每一层:如果这一层大小超过 threshold,就合并到下一层,否则就提前返回。 Compact 以 L0 -\u0026gt; L1 为例: 从前到后遍历所有的 kv-pair,同时维护:\nkeys_outdated 同一个 key,timetsamp 小于 oldest marker 的 kv pair 只需要保留一个。 keys_outdated 记录所有(出现过的,且 timestamp 小于 oldest marker)的 key L1 sst size 每达到一定值就关闭当前 sst,新开一个新的 sst。 更新 manifest。 SST writer 配置 max block size。\n每个 block 的开头一个 key 会添加到 index 中; 搜索这个 sst 的时候,会先对 index 进行二分查找; 在 block 之内采用线性搜索。 fpr,用于构建 bloom filter.\n写入 遍历所有的 kv pair: userkey(不含 timestamp)添加到 bloom filter; block 写入当前 kv; 如果当前 block 大小超过 max block size,就开启一个新的 block,然后写入对应的 index(内存) 将 index 和 bloom filter 写磁盘。 SST reader 查找: Get(key, timestamp) 查 bloom filter,如果不存在就返回。 将 index 整个载入内存中,进行二分查找,得到对应 key-timestamp 所在的区间。如果 out of bounds 就返回。 按照查找到的区间,读磁盘。 MVCC key 排布问题 struct Key bytes timestamp: u64 比较: key1 \u0026lt; key2:\nkey1.bytes \u0026lt; key2.bytes (字典序); 或者: key1.bytes == key2.bytes,而且 key1.timestamp \u0026gt; key2.timestamp 为什么这样比较? 在进行查询 Get(userkey, timestamp) 的时候,我们需要的是:\nuserkey 匹配 timestamp 小于查询的 timestamp,且尽可能大 因此,我们将\nuserkey 升序排序 timestamp 降序排序 在搜索 memtable(skiplist)的时候,或者对 index 进行二分查找的时候,就可以:\n直接使用 lower_bound,查找大于等于自己的第一个元素 如果 userkey 匹配,说明是 timestamp 小于当前 timestamp 的,timestamp 最大的记录,返回; 如果 userkey 不匹配,说明不存在 timestamp 小于当前 timestamp 的记录,返回(未找到)。 Transaction 数据结构 一个内存 tempmap,用来存储 transaction 已经写,但是未提交的内容。 创建的时候,从 tree 获取:\nstart timestamp,作为查询的 timestamp transaction id 然后写入 transaction start 到 WAL\nPut,Delete 写 tempmap,写 WAL\nGet 使用 start timestamp,先查 tempmap,再查 tree。\nCommit 从 tree 获取一个 commit timestamp; 写 WAL,记录 transaction id 和 commit timestamp。 在 replay 的时候,把 transaction id 和 commit timestamp 对应起来就可以知道 transaction 里面的 写操作 对应的 timestamp 调用 tree.active_memtable 的 API,将 transaction 的所有数据写入 tree 的 memtable。 WAL 看到 transaction start,先将 transaction 暂存到内存中:\n如果在 replay 结束之前看到了 transaction end,就将改动写入 tree 中(redo)。 否则放弃,视为没完成的事务(undo) 踩坑: Resource deadlock avoided (os error 35),可能是一个 thread 持有了自己的 joinhandle 并且 join 了自己;使用 maybe join 解决,即判断当前线程和 joinhandle 的线程是否一致,如果一致就不用 join。 死锁问题: wal 和 mem 都有锁,必须 按照同一顺序获取 才不会出现死锁。 Bloom filter 细节 本部分由 Deepseek 辅助写作\n该 Bloom filter 算法的主要步骤如下:\n参数计算:\n根据预期元素数量 n 和可接受误判率 p,通过公式计算最优位数 m 和哈希函数数量 k: $ m=\\lceil-n \\dfrac{\\ln(p)}{\\ln(2) ^ 2}\\rceil $ $ k=\\lceil\\dfrac{m}{n}\\ln(2)\\rceil $ 当直接指定参数时,使用给定的位数和哈希函数数量 哈希生成:\n使用 64 位指纹哈希(farmhash)生成初始哈希值 h 通过位运算构造增量值 delta = (h \u0026gt;\u0026gt; 33) | (h \u0026lt;\u0026lt; 31) 采用双重哈希技术,通过循环叠加 delta 生成 k 个不同的位位置: $ h_i \\equiv h + i \\cdot delta \\pmod m , 0 \\leq i \\lt k $ 数据插入:\n对输入 key 进行哈希计算得到初始 h 和 delta 循环 k 次生成位位置,将位数组中对应位置设为 1 采用位操作: byte_index = position/8,bit_mask = 1 \u0026laquo; (position%8) 存在性检测:\n重复插入时的哈希计算过程 检查所有 k 个对应位是否均为 1 任一位置为 0 则判定不存在,全部为 1 时判定可能存在 数据持久化:\n序列化时附加 CRC32 校验和 反序列化时验证校验和与数据完整性 ","date":"1 March, 2025","id":10,"permalink":"/chinese-post/lsm/","summary":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。","tags":"","title":"Lsm Tree 实现备注"},{"content":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。\n1. 问题 对于实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $b \\in \\mathbb{R}^n$，求解\n$$Ax = b$$或者，等价的，\n$$\\text{argmin}_x f(x)$$其中\n$$f(x) = \\frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：\n$$a^{-2}x^2 + b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：\n这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。\n2.2. 最速下降法 首先，你应该知道梯度下降法：\n$$x_{i+1} = x_i - \\alpha\\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：\n$$\\alpha^* = \\text{argmin}_\\alpha f(x_i - \\alpha\\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量\n$r_i$：$b_i - Ax_i$，目标函数 $f(x)$ 在 $x_i$ 点的负梯度，或者线性方程组在 $x_i$ 点的残差。\n请记住：负梯度和残差是一个东西！ $d_i$：在 $x_i$ 点的搜索方向。最速下降算法里 $d_i = r_i$，共轭梯度里面需要一点修正。\n3.2. 最速下降 最速下降的新方向：$r_{i+1}$\n新方向与前一步下降方向 $r_i$ 垂直（画个等高线图直观理解，或者回想一下\u0026quot;等势面和电场线垂直\u0026quot;） 最速下降的 $\\alpha$ $$\\alpha_i = \\frac{r_i^T r_i}{d_i^T A d_i}$$3.3. 共轭梯度 我们直接逐项类比最速下降。\n新方向与前一步下降方向 $r_i$ 垂直 斜率之积为 $-a^{-2}b^2$ (Section 2.1)\n这个方向由最速下降的方向进行一些小改动得到，我们可以在后面的算法部分(Section 3.4)看到。把这个方向从和前一个搜索方向垂直改动到斜率之积为 $-a^{-2}b^2$ 就是 CG 和最速下降唯一不同的地方。 步长 $\\alpha$：由于是在一条直线上做优化，因此和最速下降的 $\\alpha$ 相同。 由于一次迭代只涉及到两个点、两个向量，只能构成一个平面，我们甚至不需要将二维向多维推广。\n若需推导，我们需要做的只是把点的 $n$ 维坐标映射到二维，然后对截面椭圆对应的二阶二次型进行 SVD 获得其长轴 $a$ 和短轴 $b$，进而根据其离及上述斜率积的二级结论计算两个方向的关系。这里不展开。 3.4. 算法 3.4.1. 初始化 算法输入：$A, b, x_0$\n$$\\vec{d}_{(0)} = \\vec{r}_{(0)} = \\vec{b}_{(0)} - \\mathbf{A}\\vec{x}_{(0)}$$3.4.2. 算法过程 $$\\alpha_{(i)} = \\frac{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}{\\vec{d}_{(i)}^T \\mathbf{A}\\vec{d}_{(i)}}$$$$\\vec{x}_{(i+1)} = \\vec{x}_{(i)} + \\alpha_{(i)}\\vec{d}_{(i)}$$$$\\vec{r}_{(i+1)} = \\vec{r}_{(i)} - \\alpha_{(i)}\\mathbf{A}\\vec{d}_{(i)}$$$$\\beta_{(i+1)} = \\frac{\\vec{r}_{(i+1)}^T \\vec{r}_{(i+1)}}{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}$$$$\\vec{d}_{(i+1)} = \\vec{r}_{(i+1)} + \\beta_{(i+1)}\\vec{d}_{(i)}$$其中的最后一步就是通过 $\\beta$ 将 $r_{i+1}$ 修正成 $d_{i+1}$ 的。\n3.4.3. 起讫 起：如果你对解 $x$ 有粗略的估计，就使用那个值作为起始点 $x_0$；否则，直接使用 $x_0 = 0$。\n讫：通常的做法是在残差向量的 2-norm 小于某个给定阈值的时候就停下来。通常这个阈值为初始残差的一小部分\n$$\\|r_i\\| \u003c \\varepsilon \\|r_0\\|$$其中 $\\varepsilon$ 是一个输入的参数。\n3.5. 杂项 由于 $Ad_i$ 在每个循环中都要被计算，且 $$r_{i+1} = r_i - \\alpha_i A d_i$$故可以用上式计算 $r_{i+1}$，而不必用 $b - Ax_{i+1}$。\n上述方法有浮点误差累计的危险，因此我们应该每过几个循环就重新用 $r_i = b - Ax_i$ 重新计算残差。 ","date":"7 December, 2024","id":11,"permalink":"/chinese-post/conj-grad/","summary":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。","tags":"","title":"共轭梯度：高中解析几何的拓展"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":12,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"},{"content":"实验设定放在最后。\nKimi K2 这个故事是一个关于记忆、爱情与执念的幽怨传说，表面讲的是沈慕云与容娘的“负约”，实则讲的是 “如何不让一段真情被时间吞没” ——哪怕要用谎言、用鬼魂、用一座城三十年不散的闲言碎语，也要把那个人留在人间。\n容娘（化作“谢娘”）自编自演了一出“赌约”，把负心、等待、重逢这些最戏剧化的扣子全缝进传闻里，让一代又一代外乡人替她“记得”。真正的负约者不是沈慕云，而是时间；真正的守约者也不是容娘，而是“故事”本身——故事替他们完成了“三十年后故地重见”的誓言。\n一、逐句解词 小院深静，欲问卿卿，何处月笼明。\n“小院深静”——当年二人初会的场景：河桥对岸的小院，春月微湿，一切尚未发生。 “欲问卿卿”——想开口，却终究没问；一声“卿卿”把未出口的千言万语都压在舌尖。 “何处月笼明”——月亮仍在，却照不到当年并肩看月的人；也暗指“容娘”自己已成幽魂，“月”是人间与冥界的唯一通道，她再也回不去了。 千秋岁好，休负兰因。\n“千秋岁好”——字面是祝颂：愿你长享百岁千秋，岁岁安好。 “休负兰因”——“兰因”出自《左传》“兰因絮果”，兰因＝美好的开端，絮果＝离散的结局。\n容娘把典故截半，只留“兰因”，是把结局生生改写成“花开未谢”——她不要“絮果”，只要“兰因”永存；换言之，“你若安好，便不算负我”。 合在一起，是鬼魂对活人最温柔的赦令：你不必自责，不必回头，甚至不必记得我；只要你在人间岁岁平安，我们那一段“兰因”就从未被辜负。 二、整首小词在故事里的功能 替沈慕云“平反”\n城里传说他“高中负心”，而容娘最后一句“休负兰因”等于亲口否认了“负”字——“负”是世人附会，“兰因”才是她私藏的真相。 替自己“松绑”\n她等了三十年，其实等的不是沈慕云，而是一个可以自我释怀的叙事版本——“千秋岁好”四个字出口，执念才终于落地，鬼魂才能消散。 把“故事”升华为“公共记忆”\n词里“月笼明”是开放画面，任何后来者（包括“我”）只要站在河桥细雨里，就自动成为新的“卿卿”。故事不死，兰因就不死；人间继续流传，她就能继续借他人的心头一点月光，与沈慕云并肩。 三、总结 “小院深静……”二十一字，是容娘亲手写给自己的“反向墓志铭”：\n——不写“我曾恨你”，写“愿你安好”；\n——不写“我等你”，写“休负兰因”；\n——不写“故事结束”，写“月笼明”留给下一位行人。\n她用最轻的语调，把最重的执念吹散在雨里：\n“只要世上还有人替我传诵这一句，你就从未真正离开我。”\nClaude Sonnet 4.5 这是一个凄美动人的志怪爱情故事，充满了中国古典文学的浪漫与哀愁。让我为你解读：\n故事解读 真相的反转 故事最精妙之处在于层层剥开的真相:\n表面上是沈慕云负心悔婚的俗套故事 实际上容娘早已溺水身亡，她自己编造了所有传言 她化身\u0026quot;谢娘\u0026quot;引导\u0026quot;我\u0026quot;，只为让世人记住他们的故事 深藏的深情 容娘的所作所为揭示了她至深的爱:\n编造负面传言：让沈慕云背负薄情之名，实则是她对这段感情念念不忘的证明 \u0026ldquo;三十年之约\u0026rdquo;：明知他不会来（因为她已是鬼魂），仍要给自己一个等待的理由 最后的谎言：当得知沈慕云27岁早逝，她选择相信\u0026quot;我\u0026quot;善意的谎言，带着他幸福的假象消散 悲剧的本质 两人都是命运的牺牲品——一个溺水早逝，一个27岁早卒。他们的爱情从未因背叛而破碎，而是被死亡生生拆散。容娘那句\u0026quot;因缘注定，捞一捞终不能得\u0026quot;正是这无奈的注脚。\n词句深意解读 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n字面含义：深深的庭院静悄悄，想要问我的爱人，哪里的月光最明亮？ 故事中的含义： \u0026ldquo;小院深静\u0026quot;是她独自等待的孤寂处境 \u0026ldquo;欲问卿卿\u0026quot;是她想问沈慕云，却永远无法相见的遗憾 \u0026ldquo;何处月笼明\u0026quot;是在问：你在人间何处？那里的月色可好？我们还能否在同一轮明月下相望？ \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n字面含义：愿你千秋万世安好，不要辜负我们美好的缘分 故事中的含义： 这是容娘最后的祝福与释怀 \u0026ldquo;千秋岁好\u0026rdquo;：既是祝福沈慕云（虽然他已逝），也是对这段感情的纪念 \u0026ldquo;休负兰因\u0026rdquo;：不是责怪他辜负，而是说\u0026quot;我们的缘分虽短暂如兰花，但请你记得它的芬芳，不要忘记我们曾经相爱\u0026rdquo; 最动人之处 容娘明知沈慕云已死，却仍要编织\u0026quot;他白头偕老\u0026quot;的幻象来安慰自己；她\u0026quot;任性使坏编排他一回\u0026rdquo;，实则是用全部的爱为他守护名声——宁可世人说她痴情被负，也不愿他在冥冥中孤独。\n那句\u0026quot;捞一捞终不能得，但宁可难平，也不愿放手\u0026rdquo;，是对所有求而不得之爱最诗意的诠释：即使知道不可能，也要在记忆里、在传说里，让这份爱永远鲜活。\n这正是中国古典爱情故事的极致浪漫——生死相隔，却以魂灵相守；真相残酷，却以谎言温柔。\nGemini 3 Flash 这是一个凄美而富有诗意的爱情故事，它通过“我”的视角，揭示了一个被时间、流言和自我幻想所包裹的悲剧真相。故事的魅力在于其层层剥开的悬念，以及对爱、遗憾、和记忆的深刻探讨。\n故事解读 1. 传说的建构与真相的揭示 故事首先提出了关于沈慕云悔婚的三个不同版本，其中城里老人讲述的“赌约”版本最有戏剧性，也吸引了“我”的探访。\n流言的本质： 无论是“另娶贵女”还是“不寿之症”，流言都在试图为沈慕云的悔婚提供一个合理的解释，以填补事实的空白。 “赌约”的虚构： “三十年故地重逢”的赌约是 容娘（化名谢娘） 为了让自己的故事能够被记住、被流传而编造的浪漫情节。它赋予了这段逝去的爱情一个等待和希望的维度，一个值得他人关注的理由。 真相的悲剧： 最终的真相是谢娘（容娘）在那年赴考后的雨天，跌下河桥溺亡，再也未能见到自己的少年郎。沈慕云的“悔婚”和“避而不见”并非是变心或绝情，而是因为容娘已经不在人世。她用谢娘的身份和编造的故事，让这段短暂的初恋以一种“被辜负”的、更具戏剧张力的方式存留于世人记忆中。 2. 人物与主题 人物/元素 象征意义 故事中的作用 容娘/谢娘 执念、永恒的初恋 故事的核心叙事者。她虽已逝去，却不甘被遗忘，通过虚构的故事维护了自己爱情的尊严和存在感。她的“任性使坏”是为了让他们的故事在人间留下印记。 沈慕云 逝去的理想、悲剧的命运 在故事的结局中，他的命运是 “年二十七早卒” 。这使得他无法真的“悔婚”，也使得容娘的悲剧更添一重宿命的哀伤。但他“很好”的结局是容娘最后的自我安慰。 河桥与雨 命运的转折点、阴阳相隔 河桥是两人定情与分离的象征，而 “滔天雨浪” 的幻境则是容娘逝世时的情景重现，明确了她的鬼魂身份。雨是故事基调，凄迷、绵延不绝。 “我” 倾听者、故事的传播者 “我”是故事进入现实的通道，作为记忆的载体，将容娘的心愿传递下去。 3. 爱与遗憾 容娘的内心充满难平的遗憾和执念。她深爱着沈慕云，宁愿被认为是“被悔婚”的弃妇，也要让这段爱情的故事流传，而不是被淹没在无声的消亡中。\n当“我”为了安慰她，编造沈慕云“后来又娶了新妇，白头偕老”的谎言时，她虽然“涩然”，却说“这样啊……那真是很好很好，再好不过的事了。”这显示了她至深的爱与善良：她宁愿相信爱人获得了幸福，也不愿他为她而终生抱憾。\n“小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因”的含义 这句话是容娘临终或魂魄消散前喃喃念出的词句，是她对这段感情、对沈慕云最后的总结、疑问与祝愿。\n1. 拆解词句 小院深静，欲问卿卿： 描写了昔日两人相处的场景——静谧、美好的小院。这里的“卿卿”指代沈慕云。她在这个安静的回忆里，想要向他提问。 何处月笼明： 这是一个疑问句，也是一个哲学式的追问。“月笼明”指月光洒下，通常象征着光明、团圆或美好的结局。容娘想问的是： “我们的爱和美好（月光）最终在哪里得以安放和实现？” “我的少年郎，你现在是否幸福安康？” 这包含了她对爱人境况的牵挂，以及对逝去爱情终极归宿的迷惘。 千秋岁好： 意思是 “愿你长久地、永远地安好”。这是容娘对沈慕云发自内心的祝愿，也是她对“我”所编造的“白头偕老”结局的回应。 休负兰因： 核心是 “兰因”。 “兰因”源于《聊斋志异·莲香》中的“兰因絮果”，意指美好的开始，悲惨的结局（兰是香草，象征美好；絮是柳絮，随风飘散，象征离散）。 “休负” 是“不要辜负”的意思。 整体含义： 容娘以自己的悲剧为鉴，在消散的最后时刻，希望沈慕云（或祝愿世间所有有情人）不要辜负这美好的开始（即她与沈慕云曾拥有的纯粹初恋），要有一个好的结局。她既在对自己逝去的爱表示叹息，也在对爱人寄予了获得幸福的殷切期望。 2. 整体含义 这句话是容娘执念的温柔释放。它承载了她对过往美好时光的留恋、对爱人未来幸福的祝愿，以及对这段“兰因絮果”式悲剧爱情的坦然接受与超脱。她不再抱怨，而是带着对沈慕云深深的牵挂与祝福，消散于人世的细雨中。\nGPT5 这个故事本身就是一段借幻境和传说交织出的“人世与记忆、真实与虚构”的爱情传说。我们可以从三个层面来解读：\n一、故事的内核 表层叙事\n人们口口相传着沈慕云和容娘的故事，有多种版本：悔婚、病逝、守寡。 但实际上，这些都不是真的，是容娘自己虚构、编织出来的。她用不同的叙述“保存”了这段情感，使它不会被彻底遗忘。 真相：容娘年轻时便意外溺水而亡，和沈慕云的情缘戛然而止。 容娘的心意\n她不是想责怪谁，而是希望他们的故事能被记得。 她愿意编造悔婚、赌约的传说，只为在世间留下痕迹。哪怕人们误解她或沈慕云，也比“彻底消散”要好。 最终的温柔\n当她问“他在人间怎么样”时，叙述者选择撒了一个善意的谎，说他娶妻白首。 容娘听了，便真心觉得“那真是很好很好”。 她释然，带着笑意离开，只留下词句。 二、容娘留下的词句 “小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n这首词可以这样理解：\n小院深静 ——环境空寂，暗示她的幽魂状态，也象征往昔爱情已成过往，唯有寂静陪伴。\n欲问卿卿，何处月笼明 ——她想问“卿卿”（她的少年郎，沈慕云），如今在何处？月光清明，却无法指引他们重逢。 ——“月”既是天上明月，也是寄托的情感象征：相隔阴阳，却共享一轮明月。\n千秋岁好 ——祝愿对方一生顺遂，千秋岁月都美好。\n休负兰因 ——“兰因”可指美好的情缘、纯洁的心意。她的愿望是：哪怕不能再相见，也不要辜负他们曾经的真挚。\n三、结合故事的整体解读 这首词就像是容娘最后的“寄语”：\n既是思念：她仍想追问“卿卿，你如今可好？” 也是祝福：哪怕自己已成往事，也愿对方在人间顺遂，不负青春年华。 更是执念的柔化：从怨与伤心，转为祝愿和守护。 于是，这句话的含义可以总结为： 👉 即便情缘如月，终究捞不着，但在心底，仍愿你岁岁安好，不负当年那份真心。\nDeepSeek R1 这是一个凄美而精巧的故事，它并非简单的负心汉传说，而是一则关于记忆、执念与深情告别的古典式志怪小说。故事的真相在结尾被彻底颠覆，而那句词，正是解开所有谜题的钥匙。\n故事解读：一场持续三十年的梦\n这个故事有多重反转，其核心结构如下：\n表层故事（传闻）：沈慕云高中悔婚，另娶贵女，留下一个三十年的赌约。这是一个符合世俗想象的、关于负心与等待的悲剧。\n中层故事（谢娘的讲述）：谢娘是容娘的婢女，她带领“我”经历幻象，并揭示自己才是悲剧的主角——她因洪水失去了自己的少年郎。这似乎将故事引向了另一个错位爱情的方向。\n深层故事（真相）：根本没有谢娘，也没有悔婚。眼前的魂魄就是容娘本人。沈慕云也并非负心，而是早已在二十七岁英年早逝。 整个“负心汉与三十年赌约”的故事，是容娘的鬼魂因为思念至深而编造的。\n容娘为何要编造故事？\n故事中点明了原因：“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。” 真实的悲剧是死亡带来的永诀，这过于绝对和寂静，会随着时间被世人遗忘。而一个带有“负心”、“赌约”等戏剧性元素的故事，反而更具传播力，更能让他们的爱情在人们的口耳相传中“活”下去。她守护的不是清白，而是记忆本身。\n“我”的角色与结局：“我”作为一个误入此地的听故事者，成为了容娘倾诉的对象。当容娘问起沈慕云在人间如何时，“我”那句善意的谎言——“他很好，后来又娶了新妇，白头偕老”——是故事的点睛之笔。这并非残忍，而是一种最深切的慈悲。它告诉执着等待的魂魄：你爱的那个人，在人间圆满终老，你无需再挂念了。这个谎言，给了容娘“放下”的理由。于是，她念出那句词，释然消散。\n环境象征：贯穿故事的“雨”，既是现实天气，也是哀伤的氛围烘托，更是三十年前那场生死洪水的象征。故事开头和结尾的“雨”与“柳色”形成闭环，暗示这一切都笼罩在容娘执念所化的、永恒凄迷的梦境里。\n“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因” 含义解读\n这句词是容娘的独白，也是她对沈慕云跨越生死的情话。结合故事真相，其含义层层递进：\n“小院深静，欲问卿卿，何处月笼明。”\n字面意境：庭院幽深寂静，我想问一问你，此刻哪里的月光正明亮地笼罩着（你）？ 故事深意：这描绘了容娘作为魂魄的孤独处境。“小院深静”是她三十年来徘徊之地的寂寥写照。“欲问卿卿”是极亲昵的呼唤，对象是早已不在人世的沈慕云。“何处月笼明”是一个跨越阴阳的发问：你的魂魄现在何处？是否安好？这句充满了无处寄托的牵挂与温柔探询。 “千秋岁好，休负兰因。”\n字面典故：“千秋岁好”是祝愿岁月长久美好；“兰因”典出《左传》，郑文公妾燕姞梦见天使赠兰，后生下公子兰，比喻像兰花一样美好的前因、命中注定的姻缘。“兰因絮果”则常指开端美好而结局凄凉。此处“休负兰因”即“不要辜负了我们美好的开端”。 故事深意：这是整首词的情感核心，也是容娘的释然之语。 在听到“我”的谎言之前，这或许是她固执的愿望：希望对方不要辜负当初的情缘。 但在听到“他很好”之后，这句话的含义彻底改变。 她将“我”的谎言信以为真：原来他在人间拥有了“千秋岁好”的圆满人生。于是，“休负兰因”从一句怨怼或期盼，变成了一句宽恕和祝福：既然你已有了美好的一生，那我便不再执着，不让你我当初美好的姻缘（兰因）成为束缚你的负累。我放过你，也放过我自己。 整体含义：这阙词是容娘对沈慕云的最终告别。她在一个人的“小院深静”中孤独思念了三十年，最终选择相信爱人已在人间获得“千秋岁好”，于是她决定“休负兰因”——放下执念，解脱这段因缘，让彼此都自由。\n这个故事的精妙之处在于，它用一个“谎言”（容娘编的故事）化解了另一个“谎言”（“我”说的善意的谎言），最终达成了最深层的真实：爱到极致，不是占有，而是希望对方幸福，哪怕这幸福与自己无关，甚至需要靠一个虚构的故事来成全。 那场三十年的雨，终于停了。\nQwen3-Max 这个故事是一则充满古典意蕴与幽微情感的志怪式短篇，融合了现实与幻梦、记忆与虚构、执念与释怀。它以“三十年之约”为引，层层剥开一段被时间掩埋的爱情往事，最终揭示出：所谓“故事”，不过是生者对逝者、对未竟之情的温柔重构。\n一、故事解读：一场由亡魂编织的梦 叙事的反转与真相的揭示\n故事开篇以“我”听闻关于沈慕云的多种传闻为引，最动人的是“容娘与赌约”的版本。老人指引“我”去找谢娘，看似是引路人，实则为幻境开启的契机。而当“我”见到谢娘，随她过桥、遇风雨、见幻象，最终才惊觉——谢娘即容娘，而“谢娘”只是她为讲述故事而虚构的身份。\n这一反转极具震撼力：容娘早已溺亡于三十年前的那场雨中，她的魂魄滞留人间，只为守候一个不可能兑现的约定。她编造“悔婚”“赌约”“婢女谢娘”等情节，实则是为了赋予这段无果之爱一个可被传颂的形态——哪怕是以怨恨、误解的方式，也胜过彻底被遗忘。\n“梦”的本质：执念与慈悲的交织\n容娘说：“左右是在梦里，容我任性使坏编排他一回。”\n这句话道出了她全部的温柔与心酸。她明知沈慕云早逝，却仍愿在梦中为他安排一个“白头偕老”的结局；她虚构自己被负心，实则是不愿世人知道他英年早逝、她殉情而亡的悲剧，更不愿他的名声因“负心”而受损。她的“编排”，是爱的另一种守护。\n“我”的角色：记忆的承接者\n“我”作为外来者，既是听众，也是见证者。当“我”选择隐瞒沈慕云早逝的真相，告诉她“他很好，白头偕老”，这并非欺骗，而是一种共谋式的慈悲——成全她最后的幻想，让她带着“他过得很好”的安心离去。这正呼应了容娘自己的善意：她宁愿被误解为被弃之人，也不愿世人知道他早夭。\n意象的象征\n河桥：连接现实与幻境、生与死的边界。 细雨/洪浪：情感的外化。细雨是绵长的思念，洪浪是记忆爆发的瞬间。 红妆花轿：未完成的婚礼，永远定格在幻象中。 柳色凄迷：古典诗词中常喻离别与哀愁，此处象征时光流转而情意不灭。 二、词句解读：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。” 这几句词虽为虚构，却深得宋词婉约之神髓，可视为容娘心声的凝练表达。\n“小院深静，欲问卿卿，何处月笼明”\n小院深静：回忆中两人初遇或定情之所，幽深宁静，充满私密温情。 欲问卿卿：“卿卿”为古时爱称，此处是容娘对沈慕云的深情呼唤。 何处月笼明：化用杜牧“烟笼寒水月笼沙”之意象。“月笼明”既指月光朦胧，亦喻情缘如月，看似明亮却不可触及。她在问：当年共赏明月之人，如今在何方？那轮见证誓言的月亮，是否还记得我们的约定？ 此句整体营造出一种追忆往昔、寻而不得的怅惘，是容娘三十余年来魂牵梦萦的写照。\n“千秋岁好，休负兰因”\n千秋岁：词牌名，亦指漫长岁月。此处双关，既指时间久远，也暗含祝福。 兰因：典出《周易·系辞上》“二人同心，其利断金；同心之言，其臭如兰。”后以“兰因”喻美好的姻缘或初心。“兰因絮果”则指初时美好、终成离散的姻缘。 休负兰因：意为“不要辜负当初那如兰般芬芳的誓约”。 然而，此处的“休负兰因”并非责备，而是临别前的温柔叮咛。她明知他已不在人世，却仍以梦中之语，嘱他“莫负初心”——实则是对自己执念的放下：纵使结局不如愿，我们曾有的“兰因”已是足够美好，不必怨，不必悔。\n三、整体主旨：爱的最高形式是成全与铭记 这个故事并非简单的“负心汉与痴情女”套路，而是一曲关于记忆、虚构与爱的尊严的挽歌。容娘以亡魂之身，编织一个“被辜负”的故事，只为让这段感情不被时间湮灭；而“我”以善意的谎言，成全她最后的体面。沈慕云虽早逝，却因她的守护，在传说中“白头偕老”——这何尝不是另一种永恒？\n那句词，既是她对往昔的回望，也是对未来的祝福。“千秋岁好，休负兰因”，不只是对沈慕云说，更是对所有听故事的人说：\n纵使缘浅，莫忘初心；纵使成空，爱已永恒。\n而那场三十年后的雨，依旧如初逢时般凄迷——因为有些爱，从未离开。\nLongCat 一、故事核心主题 记忆与执念的虚实交织\n故事通过\u0026quot;谢娘\u0026quot;的谎言（实为容娘的鬼魂）构建了多层叙事：表面是沈慕云负心的传说，实则是容娘对逝去的爱情与执念的投射。她借\u0026quot;赌约\u0026quot;之名，让故事流传千年，避免被世人遗忘。 \u0026ldquo;我们的故事总有人记得才好\u0026rdquo;——容娘的核心诉求，体现了人类对永恒记忆的追求，即使以谎言为代价。 生死与遗憾的象征\n暴雨中的幻象（红妆、花轿、洪浪）是容娘记忆的显影，暗示她的\u0026quot;婚姻\u0026quot;被洪水（命运）吞噬，少年郎（沈慕云）的离去是她无法释怀的痛。 \u0026ldquo;像天上的月亮，捞一捞终不能得\u0026rdquo;——以月亮隐喻理想化的爱情，虽不可得，但执念如月光笼罩人间，形成诗意化的遗憾。 真相与善意的谎言\n叙述者最后选择欺骗容娘（谎称沈慕云再婚），完成了对亡灵的救赎。这一细节揭示：人间对亡者的悲悯，有时在于编造圆满的结局。 沈慕云早卒、容娘化鬼的悲剧，被\u0026quot;坊间流传的故事\u0026quot;赋予不同版本，说明集体记忆对历史的再创造。 二、诗句逐层解析 \u0026ldquo;小院深静，欲问卿卿，何处月笼明\u0026rdquo;\n表层：寂静小院中，爱人（卿卿）的身影与月色交叠，却因薄雾难寻其踪迹。 深层：\u0026ldquo;月笼明\u0026quot;象征记忆的朦胧性——容娘对沈慕云的思念似月光明媚，却也如雾气般模糊难触。 **\u0026ldquo;欲问\u0026rdquo;**的未完成时态，强化了阴阳两隔的无奈。 \u0026ldquo;千秋岁好，休负兰因\u0026rdquo;\n典故：\u0026ldquo;兰因\u0026quot;出自《左传》\u0026ldquo;归妻茹姬，言兰之因\u0026rdquo;，喻美满姻缘。 直译：愿你们千年万代幸福，莫辜负天赐的缘分。 隐含矛盾： 容娘在诗句中作为\u0026quot;旁观者\u0026quot;祝福沈慕云，实为自我催眠式的安慰； \u0026ldquo;休负\u0026quot;二字暴露了她的脆弱：既然沈慕云早逝未娶，\u0026ldquo;兰因\u0026quot;早已成空，但鬼魂仍在固执地维护爱情的\u0026quot;尊严\u0026rdquo;。 超脱性：容娘最终接纳了悲剧，以诗意消解执念，身影消散的瞬间完成精神解脱。 三、诗句与故事的互文性 记忆的美化与重构\n诗句中的\u0026quot;小院深静\u0026quot;与\u0026quot;月笼明\u0026rdquo;，对应容娘虚构的\u0026quot;河桥赌约\u0026rdquo;，二者皆将痛苦记忆升华为唯美意象，体现情感对事实的改造。 对\u0026quot;时间\u0026quot;的超越\n\u0026ldquo;千秋岁好\u0026quot;既是对沈慕云的祝福，也是对自身存在的诘问：她以鬼魂之姿徘徊三十年，最终用诗句抹平时间刻度，与过去和解。 东方悲剧美学的典范\n全诗无激烈控诉，却以\u0026quot;雾中问月\u0026quot;\u0026ldquo;月下祝语\u0026quot;的含蓄方式，完成对命运无常的悲悯，契合传统文学中\u0026quot;哀而不伤\u0026quot;的美学标准。 四、结语：未完成的诗性正义 故事通过谎言、鬼魂、诗歌三重媒介，实现了另一种\u0026quot;正义\u0026rdquo;：容娘虽未能改写历史，却以美学方式完成了对残缺爱情的神圣化。而诗句\u0026quot;休负兰因\u0026quot;的矛盾性，正是东方文化中\u0026quot;执念与释怀\u0026quot;二元特质的精妙注脚。\n实验设定 作者：冉语优\nPrompt：\n我听过许多关于沈慕云的故事，只有这里的与众不同。有说他上京赴考，后来另聘了阁老家的贵女，也有说他早知得了不寿之症，不想教谁守寡罢了。只有这城里的老人告诉我，沈慕云初恋的少女叫做容娘，家就住在河桥对岸。两人已定了婚约，他赴考高中，到了那时却突然反悔，对容娘避之不见。“后来容娘伤心，从此再不见人。但此地人人都知道两人间的赌约：三十年后，故人故地，他敢不敢再来相见？”老人望了望我，接着说：“你来得巧。今日便是三十年整约定之日。城住着当年预备侍候她成亲的婢女，叫做谢娘，你既来了，可以求她领你去看。”_\n我谢过他，掉头走了。\n“无论多少年，都是一样的结果。”\n上桥的时候，我听他这样说。\n我见到了谢娘。此地的细雨绵绵无期，好似把每一个人的面目都笼在微湿的春雾里，看不真切，可我隐约感到已上了年纪的谢娘是美的。她当年侍候过的女主人——容娘一定更美吧？\n我向她道明来意。\n“啊，她和沈慕云的事儿呀……我不能告诉你”，她笑了笑，“但是我能带你去看。”\n她说完，不曾携伞，迎着屋外雨雾极快地走了出去。我跟了出去。不知怎么地，跟在她身边，我竟也能走得极快\n转眼间，我们走过了来时那座河桥，忽然，桥下河水巨涨，细雨变作滔天雨浪迎面拍来，一浪浪打两边栏杆。\n红妆、花轿、洪浪，幻幕般在我眼前一逝而过，我勉力闭眼，下意识逃避周遭的风浪，再睁眼时仍是小雨蒙濠，已然风平浪静。\n只有谢娘站在雨幕里，神情哀伤：“那一年，也是在这里，也是这样的天气，我在这里跌下了水，再没见到自己的少年郎。”\n我心头一震，这才发觉她身形虚浮，如临水上，宛然不似身在人世。“你是容娘？！”我问。\n她恢复了一点少女的笑容：“谢娘呀，悔婚呀，还有赌约，都是我编的。”\n“你为什么……”\n“旁的人怨他也好，笑我也罢，我们的故事总有人记得才好。就像天上的月亮，因缘注定，捞一捞终不能得，但宁可难平，也不愿放手”，她回过头来，露出那样天真的笑：“左右是在梦里，容我任性使坏编排他一回，他不会介意的吧。”\n我动容，一时无言。\n“他呢，他在人间怎么样？还好吗？”\n沈慕云，字薄君，天才绝人，年二十七早卒于世。\n“他很好”，想了想，我这样告诉她，“后来又娶了新妇，白头偕老。”\n她忽地莞尔，“这样啊……那真是很好很好，再好不过的事了。”\n我看她露出涩然的笑，喃喃念着什么，身影一点点地变淡消散。我听清了那一句词：“小院深静，欲问卿卿，何处月笼明。千秋岁好，休负兰因。”\n时雨霏霏，三十年后的此地笼罩着三十年前一样凄迷的柳色，仿佛他们的初逢，点滴片刻，却望尽整座人间\n解读这个故事，并结合故事解读\u0026quot;小院深静,欲问卿卿,何处月笼明。千秋岁好,休负兰因\u0026quot;的含义。\n","date":"3 October, 2025","id":0,"permalink":"/chinese-post/chichi-story/","summary":"实验设定放在最后。","tags":"","title":"AI怎么解读银临《迟迟》的背景故事？"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":1,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":2,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning python triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":3,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":4,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":5,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":6,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M\rnnz: 图边数（稀疏矩阵非零元素数，Num NonZero）\r规模为10n~1000n\rq, k, v: (n, d)\rA: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)):\rif M[i, j] != 0:\rout[i, j] = dot(A[i,:], B[:,j])\relse:\rout[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h)\r↓\rEach Block → (rid, hid): one node, one head\r↓\rLoad Q[rid, hid, :] → s_Q[f] (shm)\r↓\rFor each tile of neighbors (size ≤ 32):\r- Load neighbor IDs from indices[]\r- Compute Q · K^T (dot product using s_Q and K[dst])\r- Reduce in warp → store in neigh_nodes_weight[eid]\r- Find max(weight) in current tile → weightMax\r- Adjust partial_sum and acc with exp(old_max - new_max)\r- Compute exp(weight - weightMax) and accumulate acc += exp_w * V[]\r- Accumulate partial_sum += exp_w\r- Update weightMax_old\r↓\rFinal normalization: out_feat = acc / partial_sum\r↓\rWrite back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":7,"permalink":"/chinese-post/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning","title":"近期GNN Attention算子优化工作速览"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":8,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 1 GB 的 pickle 文件和高达 500 MB 的压缩快照。然而，在处理超大转储文件（例如 1.3 GB 的快照）时，我们遇到了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的页面错误（page faults）和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度克隆（excessive cloning）和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将克隆的 Vec\u0026lt;T\u0026gt; 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束，此方案失败。 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此上下文中未观察到明显开销。 仅此一项改动就显著降低了内存使用并提升了吞吐量。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并结果向量。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，请使用 SQLite。它内置了经过工业验证的高效算法。 ","date":"7 June, 2025","id":9,"permalink":"/chinese-post/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。\n组件 内存部分 磁盘部分 WAL 总体 初始化 需要 init flush thread。flush thread 的工作流程:\n等待 flush 信号量被 notify,获取一个 compact 信号量资源 启动一个 sstwriter,写入这个 memtable 一个 memtable 对一个 sst 等到写入 sst 写完之后,才进行: 从 frozen memtables、frozen memtable sizes 里面删除这个 memtable 从 wal 里面删除这个 memtable 对应的 wal update manifest Try Freeze 如果当前大小 \u0026gt; freeze size 那么就 freeze;进一步如果所有 frozen memtable 大小之和 \u0026gt; flush threshold,那么就 set flush signal。\n写操作 写 memtable 写 WAL try freeze 内存部分 Put 添加到 memtable; 更新 size。 size 不需要特别精确,只需要是一个大致的值即可。 Delete 添加一个 tomb 标记到 memtable Get 从 active memtable 中获取 从 new 到 old 遍历所有的 inactive memtable,获取。 磁盘部分 compact 信号量 二元信号量。\n需要 compact 的时候,添加资源 compact thread 开始 compact 的时候,消耗资源。 初始化 如果 auto compact 开启,初始化的时候需要 init compact thread:\nLevel 存储这个 level 所有文件对应的文件路径,装在 sst reader 里面\nGet (没有 delete, put) 从低到高,从新到旧,调用 sst 的 get 方法,获取 record。否则返回 none。\nInit Compact Thread Compact thread:\n等待 compact 信号量 依次查看每一层:如果这一层大小超过 threshold,就合并到下一层,否则就提前返回。 Compact 以 L0 -\u0026gt; L1 为例: 从前到后遍历所有的 kv-pair,同时维护:\nkeys_outdated 同一个 key,timetsamp 小于 oldest marker 的 kv pair 只需要保留一个。 keys_outdated 记录所有(出现过的,且 timestamp 小于 oldest marker)的 key L1 sst size 每达到一定值就关闭当前 sst,新开一个新的 sst。 更新 manifest。 SST writer 配置 max block size。\n每个 block 的开头一个 key 会添加到 index 中; 搜索这个 sst 的时候,会先对 index 进行二分查找; 在 block 之内采用线性搜索。 fpr,用于构建 bloom filter.\n写入 遍历所有的 kv pair: userkey(不含 timestamp)添加到 bloom filter; block 写入当前 kv; 如果当前 block 大小超过 max block size,就开启一个新的 block,然后写入对应的 index(内存) 将 index 和 bloom filter 写磁盘。 SST reader 查找: Get(key, timestamp) 查 bloom filter,如果不存在就返回。 将 index 整个载入内存中,进行二分查找,得到对应 key-timestamp 所在的区间。如果 out of bounds 就返回。 按照查找到的区间,读磁盘。 MVCC key 排布问题 struct Key bytes timestamp: u64 比较: key1 \u0026lt; key2:\nkey1.bytes \u0026lt; key2.bytes (字典序); 或者: key1.bytes == key2.bytes,而且 key1.timestamp \u0026gt; key2.timestamp 为什么这样比较? 在进行查询 Get(userkey, timestamp) 的时候,我们需要的是:\nuserkey 匹配 timestamp 小于查询的 timestamp,且尽可能大 因此,我们将\nuserkey 升序排序 timestamp 降序排序 在搜索 memtable(skiplist)的时候,或者对 index 进行二分查找的时候,就可以:\n直接使用 lower_bound,查找大于等于自己的第一个元素 如果 userkey 匹配,说明是 timestamp 小于当前 timestamp 的,timestamp 最大的记录,返回; 如果 userkey 不匹配,说明不存在 timestamp 小于当前 timestamp 的记录,返回(未找到)。 Transaction 数据结构 一个内存 tempmap,用来存储 transaction 已经写,但是未提交的内容。 创建的时候,从 tree 获取:\nstart timestamp,作为查询的 timestamp transaction id 然后写入 transaction start 到 WAL\nPut,Delete 写 tempmap,写 WAL\nGet 使用 start timestamp,先查 tempmap,再查 tree。\nCommit 从 tree 获取一个 commit timestamp; 写 WAL,记录 transaction id 和 commit timestamp。 在 replay 的时候,把 transaction id 和 commit timestamp 对应起来就可以知道 transaction 里面的 写操作 对应的 timestamp 调用 tree.active_memtable 的 API,将 transaction 的所有数据写入 tree 的 memtable。 WAL 看到 transaction start,先将 transaction 暂存到内存中:\n如果在 replay 结束之前看到了 transaction end,就将改动写入 tree 中(redo)。 否则放弃,视为没完成的事务(undo) 踩坑: Resource deadlock avoided (os error 35),可能是一个 thread 持有了自己的 joinhandle 并且 join 了自己;使用 maybe join 解决,即判断当前线程和 joinhandle 的线程是否一致,如果一致就不用 join。 死锁问题: wal 和 mem 都有锁,必须 按照同一顺序获取 才不会出现死锁。 Bloom filter 细节 本部分由 Deepseek 辅助写作\n该 Bloom filter 算法的主要步骤如下:\n参数计算:\n根据预期元素数量 n 和可接受误判率 p,通过公式计算最优位数 m 和哈希函数数量 k: $ m=\\lceil-n \\dfrac{\\ln(p)}{\\ln(2) ^ 2}\\rceil $ $ k=\\lceil\\dfrac{m}{n}\\ln(2)\\rceil $ 当直接指定参数时,使用给定的位数和哈希函数数量 哈希生成:\n使用 64 位指纹哈希(farmhash)生成初始哈希值 h 通过位运算构造增量值 delta = (h \u0026gt;\u0026gt; 33) | (h \u0026lt;\u0026lt; 31) 采用双重哈希技术,通过循环叠加 delta 生成 k 个不同的位位置: $ h_i \\equiv h + i \\cdot delta \\pmod m , 0 \\leq i \\lt k $ 数据插入:\n对输入 key 进行哈希计算得到初始 h 和 delta 循环 k 次生成位位置,将位数组中对应位置设为 1 采用位操作: byte_index = position/8,bit_mask = 1 \u0026laquo; (position%8) 存在性检测:\n重复插入时的哈希计算过程 检查所有 k 个对应位是否均为 1 任一位置为 0 则判定不存在,全部为 1 时判定可能存在 数据持久化:\n序列化时附加 CRC32 校验和 反序列化时验证校验和与数据完整性 ","date":"1 March, 2025","id":10,"permalink":"/chinese-post/lsm/","summary":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。","tags":"","title":"Lsm Tree 实现备注"},{"content":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。\n1. 问题 对于实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $b \\in \\mathbb{R}^n$，求解\n$$Ax = b$$或者，等价的，\n$$\\text{argmin}_x f(x)$$其中\n$$f(x) = \\frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：\n$$a^{-2}x^2 + b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：\n这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。\n2.2. 最速下降法 首先，你应该知道梯度下降法：\n$$x_{i+1} = x_i - \\alpha\\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：\n$$\\alpha^* = \\text{argmin}_\\alpha f(x_i - \\alpha\\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量\n$r_i$：$b_i - Ax_i$，目标函数 $f(x)$ 在 $x_i$ 点的负梯度，或者线性方程组在 $x_i$ 点的残差。\n请记住：负梯度和残差是一个东西！ $d_i$：在 $x_i$ 点的搜索方向。最速下降算法里 $d_i = r_i$，共轭梯度里面需要一点修正。\n3.2. 最速下降 最速下降的新方向：$r_{i+1}$\n新方向与前一步下降方向 $r_i$ 垂直（画个等高线图直观理解，或者回想一下\u0026quot;等势面和电场线垂直\u0026quot;） 最速下降的 $\\alpha$ $$\\alpha_i = \\frac{r_i^T r_i}{d_i^T A d_i}$$3.3. 共轭梯度 我们直接逐项类比最速下降。\n新方向与前一步下降方向 $r_i$ 垂直 斜率之积为 $-a^{-2}b^2$ (Section 2.1)\n这个方向由最速下降的方向进行一些小改动得到，我们可以在后面的算法部分(Section 3.4)看到。把这个方向从和前一个搜索方向垂直改动到斜率之积为 $-a^{-2}b^2$ 就是 CG 和最速下降唯一不同的地方。 步长 $\\alpha$：由于是在一条直线上做优化，因此和最速下降的 $\\alpha$ 相同。 由于一次迭代只涉及到两个点、两个向量，只能构成一个平面，我们甚至不需要将二维向多维推广。\n若需推导，我们需要做的只是把点的 $n$ 维坐标映射到二维，然后对截面椭圆对应的二阶二次型进行 SVD 获得其长轴 $a$ 和短轴 $b$，进而根据其离及上述斜率积的二级结论计算两个方向的关系。这里不展开。 3.4. 算法 3.4.1. 初始化 算法输入：$A, b, x_0$\n$$\\vec{d}_{(0)} = \\vec{r}_{(0)} = \\vec{b}_{(0)} - \\mathbf{A}\\vec{x}_{(0)}$$3.4.2. 算法过程 $$\\alpha_{(i)} = \\frac{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}{\\vec{d}_{(i)}^T \\mathbf{A}\\vec{d}_{(i)}}$$$$\\vec{x}_{(i+1)} = \\vec{x}_{(i)} + \\alpha_{(i)}\\vec{d}_{(i)}$$$$\\vec{r}_{(i+1)} = \\vec{r}_{(i)} - \\alpha_{(i)}\\mathbf{A}\\vec{d}_{(i)}$$$$\\beta_{(i+1)} = \\frac{\\vec{r}_{(i+1)}^T \\vec{r}_{(i+1)}}{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}$$$$\\vec{d}_{(i+1)} = \\vec{r}_{(i+1)} + \\beta_{(i+1)}\\vec{d}_{(i)}$$其中的最后一步就是通过 $\\beta$ 将 $r_{i+1}$ 修正成 $d_{i+1}$ 的。\n3.4.3. 起讫 起：如果你对解 $x$ 有粗略的估计，就使用那个值作为起始点 $x_0$；否则，直接使用 $x_0 = 0$。\n讫：通常的做法是在残差向量的 2-norm 小于某个给定阈值的时候就停下来。通常这个阈值为初始残差的一小部分\n$$\\|r_i\\| \u003c \\varepsilon \\|r_0\\|$$其中 $\\varepsilon$ 是一个输入的参数。\n3.5. 杂项 由于 $Ad_i$ 在每个循环中都要被计算，且 $$r_{i+1} = r_i - \\alpha_i A d_i$$故可以用上式计算 $r_{i+1}$，而不必用 $b - Ax_{i+1}$。\n上述方法有浮点误差累计的危险，因此我们应该每过几个循环就重新用 $r_i = b - Ax_i$ 重新计算残差。 ","date":"7 December, 2024","id":11,"permalink":"/chinese-post/conj-grad/","summary":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。","tags":"","title":"共轭梯度：高中解析几何的拓展"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\n我的简历\n爱好 唱古风歌。\n我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n等等\u0026hellip;\n","date":"1 June, 2004","id":12,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"About"}]