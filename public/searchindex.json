[{"content":"eDSL，开发难度，以及DX DSL（Domain-Specific Language，领域特定语言）是一种专为特定问题领域设计的编程语言。\nGPT老师:\nDSL 可分为两类：\n外部 DSL（External DSL） 是一种独立的语言，通常需要专门的编译器或解释器。 优点：语法可以完全为领域定制，表达力强。 缺点：需要开发parser。 示例：SQL（用于数据库查询）、正则表达式、Makefile、LaTeX。 内嵌 DSL（embedded DSL，eDSL） 并非独立语言，而是利用宿主通用语言的语法和特性，在其内部“模拟”出一种贴近领域的表达方式。 依赖宿主语言的lexer/parser，无需额外实现。 优点：开发便捷。 缺点：受限于宿主语言的语法和表达能力；IDE往往无法提供针对DSL的insight（内部类型信息等）。 示例：被@torch.compile作用的torch代码，triton，以及我们的主角cuTile。 听某写了很多个triton kernel的大佬同事说，主要的debug triton代码的方式是：\n跑一遍看报错 triton提供的print 读IR 并没有能够提供良好的IDE功能的软件可以用，导致许多可以静态知道（并让IDE提供诊断）的类型信息需要靠运行时报错来修复，造成了（个人认为的）DX在这方面的欠缺。\n找回静态的信息 编译器的一半的一半的一半 先把cuTile的整个编译流程切一切。\n上半：开源部分\n上半：python -\u0026gt; cutile-python-ir (python实现)\n上半（*）：参数检查，语法检查（不合法的python对象等），类型检查（tile shape/dtype mismatch） 下半：基本优化，eliminate_assign_ops, hoist_loop_invariants, dead_code_elimination_pass 等 下半：cutile-python-ir -\u0026gt; TileIR (C++实现)\n下半：tileiras\n需求 尝试写一个软件，找回这些静态的信息，并显示到编辑器上。 我对这个软件的需求是：把（*）的检查所确定的尽可能多的信息显示在源代码上。\n实现 还好目前cuTile的编译器python侧代码还比较清晰, 趁还没有太多更新先研究个大概\n也有一种可能, 机器相关的特性会完全放到tileiras里面做, python侧永远都不会变得太复杂了\n大致结构 查看infer type pass生成的IR（下面称为IR）可以发现：\n大致结构是这样的递归定义： Program = list[Stmt] Stmt = Block | Assign Block = for + list[Stmt] | if + list[Stmt] + else + list[Stmt] 标识符：\n$开头的标识符是编译器生成的变量($number 用于中间变量, $ret 用于函数展开时传递返回值等) 没有$开头的标识符是代码原有的变量； 循环里面carry的变量要求（和triton一样）不能改变类型。\n在python层面的每个assign操作，要么只出现在IR中一次，且携带被推断出来的类型信息；或者在for中被carry的是多次，但是type信息是相同的\n函数会被完全展开，但是变量名只会添加一个后缀，不会做mangle。\n类型检查 我尝试通过如下方式确定每一次assign等号左边的类型:\n通过每次assign的IR，检查等号左边标识符如果不是$开头的，那么就把这一次assign对应的type信息添加到所需的type信息里面。 如果同一次assign的type信息只出现了一次，或者出现了多次且没有冲突，那么就可以得到这一次assign对应的type信息。然后显示在源代码的等号左边即可。 一些代码细节: 要把带有control flow的block递归展开 进入control flow的时候会把需要在里面的block使用的变量通过Block.make_temp_var通过Assign给绑定到新的名称上, 这部分不会改变变量的类型, 在类型检查的时候可以忽略. src/cutile_typeviz/cutile_utils/cuda/tile/_ast2ir.py 输入参数 需要输入的类型才能推断出中间变量的类型,这包括\ntensor: dtype, ndim scalar: dtype, 是否在编译期确认(constant) 并且可能需要考虑:\n目前原生支持的只有torch, 但是我不希望只为了创建一个tensor的metadata就import torch, 否则一次import就要两秒, 其他步骤加起来都没有0.5秒. 不希望用户去引入新的库, 希望能不添加任何代码就能使用. 我认为一个比较简单的解决方法是:\n添加一个MockTensor的实现, 只创建metadata不创建数据, 并且在编译器处添加一个MockTensor的接口, 能够接收MockTensor携带的metadata. 参考码农高天用来标识CI测试类型 的做法, 让用户在docstring通过某种特定的语法标识输入参数, 并在LSP server端对docstring进行解析, 生成输入参数的构造代码. 大致形态是 (注意\u0026lt;typecheck\u0026gt;标识): @ct.kernel def layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, TILE_M: ConstInt, TILE_N: ConstInt): \u0026#34;\u0026#34;\u0026#34; \u0026lt;typecheck\u0026gt; MockTensor((64, 2048), dtype=\u0026#34;float32\u0026#34;) MockTensor((64, 2048), dtype=\u0026#34;float32\u0026#34;) MockTensor((2048,), dtype=\u0026#34;float16\u0026#34;) MockTensor((2048,), dtype=\u0026#34;float16\u0026#34;) 32 1024 \u0026lt;/typecheck\u0026gt; Backward pass part 2: Final reduction for dW and dB. Args: DW: Partial gradient with respect to W (TILE_M, N). DB: Partial gradient with respect to B (TILE_M, N). FINAL_DW: Final gradient with respect to W (N,). FINAL_DB: Final gradient with respect to B (N,). TILE_M: Number of partial gradients to reduce. TILE_N: Tile size along N dimension. \u0026#34;\u0026#34;\u0026#34; bid_n = ct.bid(0) num_tiles = ct.num_tiles(DW, axis=0, shape=(TILE_M, TILE_N)) dw = ct.zeros((TILE_M, TILE_N), dtype=ct.float32) db = ct.zeros((TILE_M, TILE_N), dtype=ct.float32) for i in range(num_tiles): # Sum partial gradients dw += ct.load(DW, index=(i, bid_n), shape=(TILE_M, TILE_N), padding_mode=PAD_ZERO) db += ct.load(DB, index=(i, bid_n), shape=(TILE_M, TILE_N), padding_mode=PAD_ZERO) sum_dw = ct.sum(dw, axis=0) sum_db = ct.sum(db, axis=0) ct.store(FINAL_DW, index=(bid_n,), tile=sum_dw.astype(FINAL_DW.dtype)) ct.store(FINAL_DB, index=(bid_n,), tile=sum_db.astype(FINAL_DB.dtype)) LSP server vibe coding, 启动!\n我完全没有实现LSP server的经验, 但是LLM已经十足强大了! 我的vibe code路径是这样的:\n1. minimal demo 使用pygls库实现LSP server\n我发现目前LLM只记得pygls v1的API. 解决方法是把Migrate to v2 文档整个塞进LLM的context里面即可. 在文档(打开, 关闭, 保存, 变更) 的时候都触发更新.\n先实现一个统计每行有多少字符的LSP server, 然后就有一个server的骨架可以在上面自己修改了.\n2. Tile type hints 将原代码和输入参数组装成运行脚本, 然后通过PYTHONPATH捕获import路径, 使用我的MockTensor, 将python侧编译的流程进行到 infer_types_pass, 得到这一层的输出IR 按照上述流程检查每次assign的源代码位置loc和结果type, 提供inlay hints. 3. Diagnostics 有一点我一开始没有想到, 但是后来试用了发现需要加上: 要尝试parse源代码, 如果源代码有语法错误, 就不要继续了, 否则后面大片爆红会让使用者很难受 如果可以运行, 那么运行组装的脚本并且捕获所有TileError (所有cuTile报错的基类), 然后在对应位置显示红色diagnostics （半）成品 ","date":"11 December, 2025","id":0,"permalink":"/posts/cutile-2/","summary":"DSL（Domain-Specific Language，领域特定语言）是一种专为特定问题领域设计的编程语言。","tags":"","title":"cuTile 历险记，第2集：DX \u0026 LSP"},{"content":"原本第一集应该是语法和随便找个bmm，flash-attn2的kernel来实现一下并且进行benchmark的，因为所以gpu编程博客都是这样的。\nDisclaimer: 我不了解编译器，以下所有内容基于自己的理解，和编译器术语出现偏差乃至出错之处敬请指出\nnv官网提示我们，需要cuda driver一个较高的版本，cuda toolkit 13.1（tileiras汇编器），以及blackwell以上的GPU（目前）才能使用cutile。但是我没有b200或者50系（cc12）的游戏卡，我手上能碰到的机器刚好截至hopper，所以我并没有办法编译执行cutile程序，失去了尝鲜的机会。\n但是有了cutile-python这个python端，下降到mlir之前的中间代码还是可以了解一下的。\n探索过程 屏蔽C库 我们发现报错发生在src/cuda/tile/_cext.pyi，提示驱动版本过低。vibe coding启动，我们让LLM把_cext这个cpp库用一个mock进行替代，骗过编译器；然后包装了一个CutileIrDump类，通过cuda.tile._compile._get_final_ir函数可以获取到cuTile IR （不是TileIR）。有两种格式，人类可读的和binary的。\n了解编译流程，我们主要从cuda.tile._compile._get_final_ir入手。\ndef _get_final_ir(pyfunc, args, tile_context) -\u0026gt; ir.Function: ir_ctx = ir.IRContext() func_ir: ir.Function = get_function_ir(pyfunc, ir_ctx, call_site=None) ir_args = func_ir.bind_arguments(args, get_constant_annotations(pyfunc)) func_ir = infer_types_pass(func_ir, ir_args, pyfunc, tile_context) # -------- 上方：语法、类型检查 ---------- # -------- 下方：（部分）机器无关优化 ---------- eliminate_assign_ops(func_ir) # \u0026lt;-- breakpoint here dead_code_elimination_pass(func_ir) if not CUDA_TILE_TESTING_DISABLE_TOKEN_ORDER: alias_result = alias_analysis_pass(func_ir) token_order_pass(func_ir, alias_result) rewrite_patterns(func_ir) # Loop invariant code motion needs to run after the token order pass. # Otherwise, it may incorrectly hoist load operations out of the loop. hoist_loop_invariants(func_ir) split_loops(func_ir.root_block) dead_code_elimination_pass(func_ir) return func_ir 大概看一下代码：\nget_function_ir 将Python函数转换为第一层中间表示\n创建符号表：收集函数的全局变量、闭包变量等环境信息 AST到IR转换：通过 _ast2ir 函数将抽象语法树转换为IR表示, 处理变量作用域和版本管理 通过 _eliminate_load_store_pass 消除Load/Store操作 bind_arguments 将实际参数绑定到函数的形式参数，验证参数类型是否支持，并确定参数类型和常量属性\ninfer_types_pass 函数的作用\n创建类型映射表和常量映射表 将输入参数的类型信息绑定到函数的参数变量 处理常量参数，将其替换为常量值 操作类型推断：遍历IR中的每个操作，推断其结果类型 控制流优化： 如果条件表达式是常量，直接展开if-else分支 简化没有break/continue的简单循环 函数调用内联 类型传播：在控制流合并点（如if-else结束）处理类型合并 类型 （tile metadata） 可以从函数名称猜到在分割线之前的部分，都是语法、类型检查，在分割线之后的部分，是机器无关的优化。于是猜测，如果需要获取tile的metadata，最接近源代码的位置可能就是infer_types_pass的返回值了。\n在此处打上断点，进行分析： 可以看到func_ir.root_block._operations 里面就是我们代码经过最基本的翻译，并经过shape检查之后形成的中间表示。这里infer_types_pass这个pass已经执行完了，所以我们应当可以看到所有的tile的shape都是什么。此处（断点处）调用 func_ir.to_string() 即可将中间表示输出为文本形式。\n以下列这个kernel为例，\nimport cuda.tile as ct ConstInt = ct.Constant[int] PAD_ZERO = ct.PaddingMode.ZERO def zfunc(a, b): sum = a + b res = ct.cos(sum) return res def apply_mod(mod, c_tile, i_m, i_n, tm, tn): mod_tile = ct.load(mod, index=(i_m, i_n), shape=(tm, tn), padding_mode=PAD_ZERO) zval = zfunc(mod_tile, c_tile) return ct.sin(zval) @ct.kernel def my_kernel(a, b, c, mod, tm: ConstInt, tn: ConstInt, tk: ConstInt): i_m = ct.bid(0) i_n = ct.bid(1) acc = ct.zeros((tm, tn), dtype=ct.float32) for i_k in range(tk): t_a = ct.load(a, index=(i_m, i_k), shape=(tm, tk), padding_mode=PAD_ZERO) t_b = ct.load(b, index=(i_k, i_n), shape=(tk, tn), padding_mode=PAD_ZERO) acc = ct.mma(t_a, t_b, acc) # c_tile = apply_mod(mod, acc, i_m, i_n, tm, tn).astype(ct.float16) tile1 = ct.full((32, 16), 0.0, ct.float32) tile2 = ct.full((32, 16), 0.0, ct.float32) tile3 = zfunc(tile1, tile2) tile4 = ct.full((16, 64), 2.0, ct.bfloat16) tile5 = ct.full((16, 64), 2.0, ct.bfloat16) tile6 = zfunc(tile4, tile5) c_tile = apply_mod(mod, acc, i_m, i_n, tm, tn).astype(ct.float16) ct.store(c, index=(i_m, i_n), tile=c_tile) 截取一小段中间表示的文本形式，可以看到比如tile4和tile5的dtype和shape都已经确定了。\ntorch.Tensor的metadata是dtype, shape, strides.\n在cuTile的tile里面，为了硬件效率最大化，tile的存储不一定是某（行or列）主序，甚至可能任何一个维度的相邻两个元素在内存中都不连续，因此stride（其实是layout）是tileiras需要去figure out的东西，cuTile程序员无法操作。\ncuTile Python Bytecode 和后续 如果直接获取_get_final_ir函数的输出并且调用to_string()，就会得到cuTile Python IR （不是TileIR）。因为已经执行过了一些简单的优化，所以原始代码中的变量名信息已经丢失掉了(这也是我们之前要在优化之前就获取中间产物的原因)。\n在src/cuda/tile/_compile.py的compile_tile函数中，可以看到这种代码会先转换为cuTile Python Bytecode，然后被C++扩展库编译为TileIR，最后调用黑箱tileiras编译成cubin.\n因为我没有能力往下分析，且我没有老黄最新的卡，所以后续略。\n","date":"6 December, 2025","id":1,"permalink":"/posts/cutile-1/","summary":"原本第一集应该是语法和随便找个bmm，flash-attn2的kernel来实现一下并且进行benchmark的，因为所以gpu编程博客都是这样的。","tags":"","title":"cuTile 历险记，第1集：编译"},{"content":"我决定删掉 bilibili.\n起因 9 月的时候，我发现我每天在 bilibili 上花的时间太多了，几乎占了我每天使用手机的时间的 80%。因此我决定删掉 bilibili.\n其实从 3 月开始，我已经删过几次 bilibili，但是都以失败告终，并对自己立下了几个承诺：\n不看情感类话题； 不看政治类话题； 不看性别、男女对立、婚姻、家庭话题； 不看鸡汤； 尽可能只看直播（比如阿梓、七海和泠鸢yousa，这些我并没有认为应该戒掉）和知识； 以降低我一不小心看着bilibili发现时间过去了一个小时的概率。\n但是上述承诺最终都没能成功实现；每一次我都会在主页给我推上述话题的时候忍不住点进去看；然后看评论，幻想和评论对线。\n于是最后一次我删掉了bilibili之后，我给VPN续费了一年，然后决定用Youtube替代bilibili。\n初入Youtube 在换到Youtube之后，我立刻就感觉到和Bilibili（以及其他国内视频平台）的差别：\nBilibili的推荐算法会一直给你推荐可能你会看下去的东西（包括一些你平时可能不看，但是一时兴起会看很久的社会议题等，比如“回村996007天，二爷治好了我的精神分裂”），由此把你留在他的软件里面很久； Youtube则并不会过度做推测，你可以很简单的通过持续搜索某个关键词把推荐视频流洗成那个主题，并且很久很久不会偏题 而且还有以下优势：\nYoutube的人的评论虽然同样带有观点性，但是其年龄分布不像bilibili那么低龄，大家也没有时间在一条评论下长篇累牍地辩论；而且我读英文也不能算native，阅读的累倦感可以直接降低我读评论的冲动； Youtube有广告。这通常可能得算是一个劣势；但是看很多广告之后，我会意识到已经看了很久了，该停一下，所以某种程度上也算是一种优势。 Youtube的视频更加纯粹。科学视频就是讲科学、体育视频讲体育、商业视频讲商业，不会非得在结尾带一点政治/人文/哲思/价值观输出；相比之下Bilibili的哲学家们比如影视飓风、何同学、包括最近漫士沉思录也开始有类似的迹象，都会在自己制作的视频品类的末尾甚至中间夹带私货，对于我这种对人文和价值观输出完全无感的人来说，观感非常之差。 调教Youtube 可能是因为我的中国大陆IP，刚进Youtube的时候给我推荐了非常多的键政视频。于是我打算把Youtube的推荐算法调教成我想要的样子。\n首先，我将我能看见的键政博主的视频都按了不感兴趣。Youtube非常听话。\n刚好我最近想了解一下美国的体育，于是我去主动搜了“NFL highlights”，“MLB Highlights”等等主题，并且持续主动搜索观看了半个星期的球赛高光。\n从此之后，我的主页竟然会把每周新的NFL常规赛推荐给我，我甚至都不需要主动去搜索了。往下滑几个一般是NFL或者MLB的比赛数据统计、传奇故事、选手好球精选之类，往下无论怎么滑也再也不会看到键政视频了。\n感受 失去Bilibili最大的损失就是我没法听三无、yousa、阿梓等一众音乐博主最新的歌了。我的解决方法是，想听的时候晚上下班用电脑看。电脑快捷键更多，切换页面更快，不会因为软件的刻意设计而被主页视频流绑架。\n其他方面，确实会在偶尔别人给你分享视频的时候有一些不方便的地方，以及没法及时跟上最近的网络热点了（同时，我本来就不玩微博，小红书，抖音，快手。我的主要娱乐就剩下了Youtube、斗鱼，虎牙）。但是第一，我并没有多少朋友，所以也谈不上有多不方便；第二，跟上网络热点也并没有什么用。\n我将努力持续下去，让Bilibili再也不要出现在我的手机里。\n","date":"17 November, 2025","id":2,"permalink":"/art/quit-bili/","summary":"我决定删掉 bilibili.","tags":"","title":"我（初步）戒掉了 Bilibili"},{"content":"首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他劫持捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。\n因此在使用cuTile的时候，要一直告诉自己 \u0026ldquo;This is not Python\u0026rdquo;.\nAbstraction Level 作为用户需要知道的：这门语言工作在哪个抽象层级。这里指的是这门语言提供的接口，并不直接对应硬件。cuTile的compiler magic会把我们写的代码map到硬件上，但这并不是写cuTile的程序员需要关心的。\n内存 从逻辑上来说，cuTile暴露给用户的内存分为两种：\nGlobal Memory (Gmem): 读写速度：慢 Cache: 读写速度：较快 编程模型 Global Array 存放在Gmem上\n操作：只能进行Load(从Gmem读取到Cache)，以及Store(从Cache存入Gmem)。\n来源：PyTorch tensor 可以直接传入。\nTile Array 存放在Cache上\n操作：可以在上面进行数学操作如sin, mma等。\n来源：tile kernel内创建(例如cuda.tile.zeros)，或者Global Array load得到\nImmutable：在逻辑上，任何对Tile Array的计算操作都会返回新的Tile Array (Returns copies, not views)；你也不能直接对Tile Array里面的内容进行修改。\ncuTile的compiler magic肯定会在内部防止冗余内存的创建，毕竟速度\u0026gt;=SRAM的存储是如此昂贵；但是程序员是以immutable的形式编程的。 metadata: dtype, shape\nlayout对用户是不可见的，交由编译器处理。 示意图： 编程问题 问题输入 Tensor 级别的计算过程，比如：\nMatmul+activation Attention Mechanism 其他可以用NumPy/PyTorch这一级别的抽象所描述的算法。 cuTile是用来解决什么问题的 对于同一个算法，如何明智地加载数据、进行计算，可以减少Load/Store的数据量、增大计算密度，并且减少需要在Gmem上materialize的中间数据。\n奇怪的想法 据nv的编译器工程师在各种talk里面所说，cuTile将会屏蔽所有硬件特异性的功能，交由编译器处理。如果是这样的话，其他硬件厂商是不是更方便在这一层级往下做？比如，直接开发一套rocm.tile，在接口上对标cuda.tile，然后实现自己的编译器，Lower到自己的具有硬件特异性的IR代码上进行优化。这样似乎就避免了triton目前推出越来越多nvidia定制的功能，导致其他硬件厂商无法跟进的问题。\n参考 cuTile talk @ scipy cuTile talk @ torch ","date":"13 November, 2025","id":3,"permalink":"/posts/cutile-0/","summary":"首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他劫持捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。","tags":"","title":"cuTile 历险记，第0集：心智模型"},{"content":"注意：历史同人和历史毫无关系，请勿随便代入。\n设定 谢玄，字幼度，陈郡阳夏人。谢安之侄，封康乐县公，淝水之战前锋都督，一战击溃苻坚百万兵，后世称“江左第一将”。\n谢道韫，谢奕之女、谢安侄女，王凝之之妻，中国历史上第一位被正史立传的女诗人，有“咏絮才”之誉，晚年寡居会稽，以文墨自遣。\n【Verse 1】 行迹迟 踏细雪上东山\n“东山”直指会稽东山，谢安早年隐居之地，也是谢氏子弟日常游憩的“自家后花园”。谢玄少年多病，史书说他“晚达”，故“行迹迟”；“细雪”先布下“雪”意象，为后文“咏絮”埋伏笔。\n畏冬寒 看天边星愈淡\n谢玄幼年体弱，家人常恐其不寿，故“畏冬寒”是实写身体，也是隐喻：东晋末年的政治空气同样“寒冷”。“星愈淡”暗示晋室衰微，天命将改。\n回忆里小院温暖 捧灯相坐看\n“小院”是谢府乌衣巷旧宅，兄妹两小无猜，雪夜围灯读书。史书载谢安“内集儿女，讲论文义”，画面感极强。\n竟已隔几重山\n一灯如豆的童年，转瞬被战火、婚姻、官爵隔开。“几重山”既是空间，也是身份：一个走向沙场，一个困于闺阁。\n【Pre-Chorus 1】 稚语童音 说儿时旧梦梦不完\n谢安雪日“何所似”之问，谢朗答“撒盐空中”，道韫补“未若柳絮因风起”，全场“大笑乐”。歌词把这段千古对话，化成兄妹私下“说梦”。\n光华流转 心上写风月与民安\n“光华”承“灯”而来：灯影在壁，也照出二人各自抱负——\n谢玄：愿天下“民安”；\n道韫：愿文字“风月”长存。一句写尽“一文一武”两种人生取向。\n你披上荣光 走进青史路漫漫\n“荣光”指淝水战功；“青史路漫漫”点明谢玄已踏上“治国经略”的不归路。\n我捡点红妆 立笔冢玲珑辞畔\n“笔冢”典出王羲之“退笔成冢”，此处移用于道韫：她无法横刀立马，只能把“红妆”换“文冢”，以玲珑诗句自垒一冢青春。\n月已升 梦未返\n重复句，像昆曲“水磨腔”，一唱三叹：时间往前走，童年永远留在那盏灯里。\n【Chorus 1】 你走过治国经略 书写历史的轮廓\n谢玄后半生几乎“马背上的办公桌”：北伐、筑堰、移民、练兵，把东晋的国境线硬往北推了三百里。\n王侯将相的叙事吞噬少年旧肝胆\n“王侯将相”是青史正典，而正典只会记录“功业”，不会记录“撒盐空中”的童心。“吞噬”二字，说历史对个体的残酷淘洗。\n我立于浮世茫茫 逐流惹一身尘满\n道韫出嫁后，随王凝之守会稽，孙恩乱起，丈夫死于乱刀，她“手刃数贼”后仍被俘虏。歌词把这段经历，浓缩成“逐流惹尘”。\n朱门士族的圭臬规训才情与烂漫\n“朱门圭臬”指门第礼法：再高的“咏絮才”，也要先当好“王谢家媳妇”。才情与烂漫，被族规一层层缠成“礼”的茧。\n【Verse 2】 深深廊 世情比条框难\n乌衣巷深廊回合，一眼望不到头；“条框”是族规、三纲五常，也是东晋门阀政治那张看不见的网。\n惊鸿影 隐入岁月的川\n“惊鸿”出自《洛神赋》，比喻道韫才貌；然而才貌终被“岁月的川”带走，只剩史书里两行小字。\n我将细雪点成烛 寻三分旧忆\n回到“雪”意象：道韫把当年“咏絮”的那片雪，当成一支“蜡烛”点亮，只为在记忆里找回三分童年温度。\n又畏惧那浓灿\n“浓灿”是外部世界的刀光剑影，也是内心对“功名”二字的恐惧——她见过战争，也见过丈夫因迷信而丧命。\n【Pre-Chorus 2】 灯火可亲 说尘世与街巷俗谈\n晚年寡居会稽，她“披素几、设讲坛”，为邻家孩子讲书。史书说“神情爽迈，有林下风气”。\n重逢对坐 皱眉弯问冬衣可暖\n想象兄妹若能在战后重逢，不会聊战功，只会像小时候一样先问一句“衣可暖？”——最平凡的问候，也是最深的情感。\n你卸下战甲 退守在英雄梦散\n谢玄48岁即病逝，临终上表“乞骸骨”，把兵权交还朝廷。“英雄梦散”四字，写尽功高震主的悲凉。\n我困于庭院 渐隐入文人书案\n道韫晚年“以文自晦”，把庭院变成私塾，把自己“隐”进书案，也“隐”进历史角落。\n月已升 梦未返\n与第一次出现形成回环：同样的月亮，再也照不回那盏童年的灯。\n【Chorus 2】 堂前归燕纷飞去 难寻当年旧黄昏（月斑斑 雪寒寒）\n“堂前燕”出自刘禹锡《乌衣巷》“旧时王谢堂前燕”；“月斑斑、雪寒寒”把视觉与触觉叠在一起，像远处传来的叹息。\n大运所飘摇 入梦逐彼时絮语存（空院落 灯已暗）\n“大运”一词，道韫本人诗里用过：“时哉不我与，大运所飘揺。”歌词直接把她原句嵌进来，意为：时代洪流里，只剩梦还能追逐当年“咏絮”的片语。\n梦里有时哉我与 此身亦以天下济（月斑斑 雪寒寒）\n“亦以天下济”是替谢玄作答：虽然妹妹不能横刀，但也在用自己的方式“济天下”——教书、传文、保存家族文脉。\n千秋路漫漫 亦不过一载冬复春（我捡点旧日闲谈）\n把“千秋青史”缩成“一载冬复春”，与开篇“行迹迟、踏细雪”首尾呼应：再大的功业、再高的才情，也抵不过时间的一场雪，雪化后只剩“旧日闲谈”——也就是我们今天还在聊的这段兄妹故事。\n小结 整首词用“雪—灯—月”三重意象，把谢氏兄妹的一生缠成一条线：\n雪是童年（咏絮），灯是青春（围灯夜读），月是暮年（千里共婵娟却再不得见）。\n“你”建功立业却被史家框架吞没，“我”才情横溢却被礼教规训消磨；两条路看似分岔，实则同归于“青史一梦”。\n歌词最动人的地方，是把“宏大叙事”拆成一句“冬衣可暖”——无论谢玄还是谢道韫，最终想回到的，不过是那年东山小院里，一盏灯、一场雪、两个孩子并肩看天的瞬间。\n","date":"2 November, 2025","id":4,"permalink":"/art/ai%E5%B8%A6%E8%AF%BB%E6%AD%8C%E8%AF%8D%E7%9C%89%E9%97%B4%E5%86%AC%E8%B5%B4%E6%98%A5/","summary":"注意：历史同人和历史毫无关系，请勿随便代入。","tags":"","title":"AI带读歌词：《眉间冬赴春》"},{"content":"今天我们来介绍茴字的第3种写法\n今天我们来介绍 Triton 中的第三种进行 tensor 指针运算的 API：Tensor Descriptor。内容来自triton 文档。\n关于 triton 的基本概念 triton 只是和 python 共用语言前端（我们写的代码），triton 会接管 python 的 AST，然后后续步骤就交由 triton 编译器一步步 lower 到 GPU 代码了。 在第一次执行一个 kernel 之前发生的事情称为编译期，之后的执行称为运行时。 triton的 kernel launch 的grid 参数是一个 ndrange，在 kernel 里面获取到的 program_id(i) 就是第 i 维度的 index。 Tensor Descriptor的用法 创建 desc = tl.make_tensor_descriptor( pointer, shape=[M, N], strides=[N, 1], block_shape=[M_BLOCK, N_BLOCK], ) 其中：\npointer 就是传入triton kernel的tensor shape 是一个整数列表，可以编译期确定，也可以运行时动态传入，可以不是tilesize的倍数 传入 [tensor.shape(i) for i in range(tensor.dim())] strides 是一个整数列表，可以编译期确定，也可以运行时动态传入，可以不是tilesize的倍数 传入 [tensor.stride(i) for i in range(tensor.dim())] block_shape 是一个整数列表，必须是编译期常量 对应概念是CUDA的blockDim 上述三者的长度必须相同，等于输入tensor的 .dim() 读写 读 value = desc.load([moffset, noffset]) 其中唯一的参数offsets是一个整数列表：\n可以编译期确定，也可以运行时动态传入 列表里面每个值是对应维度的元素级别 offset 写 desc.store([moffset, noffset], tl.abs(value)) 其中：\n第一个参数offsets是一个整数列表，和读一样： 可以编译期确定，也可以运行时动态传入 列表里面每个值是对应维度的元素级别 offset 第二个参数是一个 buffer，shape 必须和 make_tensor_descriptor 的时候指定的 block_shape 相同 例子 例1 请一行代码一行代码读过去，你一定能看懂的。\n@triton.jit def inplace_abs(in_out_ptr, M, N, M_BLOCK: tl.constexpr, N_BLOCK: tl.constexpr): desc = tl.make_tensor_descriptor( in_out_ptr, shape=[M, N], strides=[N, 1], block_shape=[M_BLOCK, N_BLOCK], ) moffset = tl.program_id(0) * M_BLOCK noffset = tl.program_id(1) * N_BLOCK value = desc.load([moffset, noffset]) desc.store([moffset, noffset], tl.abs(value)) M, N = 256, 256 x = torch.randn(M, N, device=\u0026#34;cuda\u0026#34;) M_BLOCK, N_BLOCK = 32, 32 grid = (M / M_BLOCK, N / N_BLOCK) inplace_abs[grid](x, M, N, M_BLOCK, N_BLOCK) 例2：Flash Attention ","date":"31 October, 2025","id":5,"permalink":"/posts/triton-td/","summary":"今天我们来介绍茴字的第3种写法","tags":"","title":"Triton Tensor Descriptor: 茴字的第三种写法"},{"content":"花点时间配置一下我的Mac Mini.\n配件 Mac Mini M4 最丐版，3349 Samsung 990 evo plus, 996 海备思硬盘盒+扩展坞，471 Redmi 显示器 4K 60Hz，1449 鼠标，65 Setup 把系统装到外接硬盘里：教程 鼠标滚轮反向 软件 浏览器 我的需求是：Vertical tab, 以及熟悉。\n因为我之前在windows用的是edge，在mac上一搜居然也有，于是就直接用edge懒得换了。\nTerminal Emulator: iTerm2: 主题：FairyFloss，一个很girly的主题，背景颜色改深了一点 字体：Jetbrains Mono Shell: fish 注意去github装 fish 4，软件源上很有可能还是fish 3.7，fish 3 已经不维护了 设置默认shell：有两个步骤，两个都要做： chsh 命令，具体格式请问AI 按照下面指引 笔记 ima.copilot，主要为了省心，我并没有折腾私有部署的兴趣，对信息安全也不甚关心。\nBrew换源 详见：这篇帖子\n实用软件 typst fd (replacement of find) ripgrep ","date":"24 October, 2025","id":6,"permalink":"/posts/macmini-setup/","summary":"花点时间配置一下我的Mac Mini.","tags":"","title":"我的 Mac Mini Setup"},{"content":"原文：\nhttps://mirrors.ustc.edu.cn/help/brew.git.html https://mirrors.ustc.edu.cn/help/homebrew-bottles.html https://mirrors.ustc.edu.cn/help/homebrew-core.git.html https://mirrors.ustc.edu.cn/help/homebrew-cask.git.html ✅ 一、设置环境变量（永久生效，针对 fish） 在 fish 中，永久设置环境变量应使用 set -Ux （全局、导出、持久化）： （set文档： https://fishshell.com/docs/current/cmds/set.html ）\n# Homebrew 主程序仓库 set -Ux HOMEBREW_BREW_GIT_REMOTE https://mirrors.ustc.edu.cn/brew.git # Homebrew 核心公式仓库 set -Ux HOMEBREW_CORE_GIT_REMOTE https://mirrors.ustc.edu.cn/homebrew-core.git # 预编译二进制包（bottles）域名 set -Ux HOMEBREW_BOTTLE_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles # 元数据 API 域名（Brew 4.0+ 必需） set -Ux HOMEBREW_API_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles/api ✅ 二、配置 Homebrew Cask 使用镜像（Git 仓库方式） 由于Brew 4.0+ 默认使用 JSON API，大多数情况下不需要手动设置 cask 的 Git 镜像。但如果你仍希望显式使用 USTC 的 cask Git 仓库（例如离线环境或调试），请运行：\nbrew tap --custom-remote homebrew/cask https://mirrors.ustc.edu.cn/homebrew-cask.git ⚠️ 注意：如果你以后想恢复官方源，可运行： brew tap --custom-remote homebrew/cask https://github.com/Homebrew/homebrew-cask\n✅ 三、验证设置是否生效 检查环境变量：\necho $HOMEBREW_BREW_GIT_REMOTE echo $HOMEBREW_CORE_GIT_REMOTE echo $HOMEBREW_BOTTLE_DOMAIN echo $HOMEBREW_API_DOMAIN 更新 Homebrew：\nbrew update 如果下载速度很快（且无 GitHub 超时），说明 bottles 已走镜像。\n（如果首次安装Homebrew） 如果你尚未安装 Homebrew，可先设置上述环境变量，再运行安装脚本：\nset -Ux HOMEBREW_BREW_GIT_REMOTE https://mirrors.ustc.edu.cn/brew.git set -Ux HOMEBREW_CORE_GIT_REMOTE https://mirrors.ustc.edu.cn/homebrew-core.git set -Ux HOMEBREW_BOTTLE_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles set -Ux HOMEBREW_API_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles/api 使用官方安装脚本（若无法访问 GitHub，改用 USTC 备份脚本） /bin/bash -c \u0026#34;$(curl -fsSL https://mirrors.ustc.edu.cn/misc/brew-install.sh)\u0026#34; ","date":"23 October, 2025","id":7,"permalink":"/posts/brew-sources/","summary":"原文：","tags":"","title":"Brew 换源"},{"content":"内容来自这里.\n高级 Mini-Batching（Mini-Batching）\n创建 mini-batching 对于让深度学习模型的训练扩展到海量数据至关重要。mini-batch 不会一个接一个地处理样本，而是将一组样本分组到一个统一的表示中，从而可以高效地并行处理。在图像或语言领域，这个过程通常是通过将每个样本重新缩放或 padding 到一组等大小的形状来实现的，然后将样本分组到一个额外的维度中。这个维度的长度等于分组在一个 mini-batch 中的样本数量，通常称为 batch_size。\n由于图是一种最通用的数据结构，可以包含任意数量的节点（nodes）或边（edges），因此上述两种方法要么不可行，要么可能导致大量不必要的内存消耗。在 PyG 中，我们采用另一种方法来实现对大量样本的并行化。在这里，adjacency matrices 以对角线方式堆叠（创建一个包含多个孤立子图的巨大图），并且节点和目标特征（features）简单地沿节点维度进行拼接，即：\n$$ A = \\begin{bmatrix} A_1 \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 A_n \\end{bmatrix}, \\quad X = \\begin{bmatrix} X_1 \\\\ \\vdots \\\\ X_n \\end{bmatrix}, \\quad Y = \\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}. $$与其他 batching 过程相比，此过程具有一些关键优势：\n依赖于 message passing scheme 的 GNN operators 不需要修改，因为属于不同图的两个节点之间仍然不能交换消息。 没有计算或内存开销。例如，此 batching 过程完全不需要对节点或边的特征进行任何 padding。请注意，adjacency matrices 没有额外的内存开销，因为它们以稀疏（sparse）方式保存，只包含非零项，即边。 PyG 借助 torch_geometric.loader.DataLoader 类自动将多个图 batch 成一个巨大的图。在内部，DataLoader 只是一个常规的 PyTorch torch.utils.data.DataLoader，它重写了其 collate() 功能，即定义如何将样本列表分组在一起。因此，所有可以传递给 PyTorch DataLoader 的参数也可以传递给 PyG DataLoader，例如 worker 数量 num_workers。\n在其最一般的形式中，PyG DataLoader 会自动将 edge_index tensor 增加到当前处理图之前已聚合（collated）的所有图的累积节点数，并将 edge_index tensors（形状为 [2, num_edges]）在第二个维度上进行拼接。face tensors，即 mesh 中的 face 索引，也是如此。所有其他 tensors 将仅在第一个维度上进行拼接，而不会进一步增加其值。\n然而，存在一些特殊用例（如下所述），用户需要根据自己的需求主动修改此行为。PyG 允许通过重写 torch_geometric.data.Data.__inc__() 和 torch_geometric.data.Data.__cat_dim__() 功能来修改底层的 batching 过程。在没有任何修改的情况下，它们在 Data 类中定义如下：\ndef __inc__(self, key, value, *args, kwargs): if \u0026#39;index\u0026#39; in key: return self.num_nodes else: return 0 def __cat_dim__(self, key, value, *args, kwargs): if \u0026#39;index\u0026#39; in key: return 1 else: return 0 我们可以看到 __inc__() 定义了两个连续图属性之间的增量计数。默认情况下，只要属性名称包含子字符串 index（出于历史原因），PyG 就会将属性增加节点数量 num_nodes，这对于 edge_index 或 node_index 等属性非常方便。但是请注意，这可能会导致属性名称包含子字符串 index 但不应增加的属性出现意外行为。为确保正确，最佳实践是始终仔细检查 batching 的输出。此外，__cat_dim__() 定义了相同属性的图 tensors 应在哪个维度上进行拼接。这两个函数都会为存储在 Data 类中的每个属性调用，并将它们的特定 key 和 value 项作为参数传递。\n接下来，我们将介绍一些可能绝对需要修改 __inc__() 和 __cat_dim__() 的用例。\n图对（Pairs of Graphs） 如果您想在单个 Data object 中存储多个图，例如用于图匹配（graph matching）等应用，则需要确保所有这些图的 batching 行为正确。例如，考虑在 Data 中存储两个图：一个源图 $G_s$ 和一个目标图 $G_t$，例如：\nfrom torch_geometric.data import Data class PairData(Data): pass data = PairData(x_s=x_s, edge_index_s=edge_index_s, # Source graph. x_t=x_t, edge_index_t=edge_index_t) # Target graph. 在这种情况下，edge_index_s 应该增加源图 $G_s$ 中的节点数，例如 x_s.size(0)，而 edge_index_t 应该增加目标图 $G_t$ 中的节点数，例如 x_t.size(0)：\nclass PairData(Data): def __inc__(self, key, value, *args, kwargs): if key == \u0026#39;edge_index_s\u0026#39;: return self.x_s.size(0) if key == \u0026#39;edge_index_t\u0026#39;: return self.x_t.size(0) return super().__inc__(key, value, *args, kwargs) 我们可以通过设置一个简单的测试脚本来测试我们的 PairData batching 行为：\nfrom torch_geometric.loader import DataLoader import torch x_s = torch.randn(5, 16) # 5 nodes. edge_index_s = torch.tensor([ [0, 0, 0, 0], [1, 2, 3, 4], ]) x_t = torch.randn(4, 16) # 4 nodes. edge_index_t = torch.tensor([ [0, 0, 0], [1, 2, 3], ]) data = PairData(x_s=x_s, edge_index_s=edge_index_s, x_t=x_t, edge_index_t=edge_index_t) data_list = [data, data] loader = DataLoader(data_list, batch_size=2) batch = next(iter(loader)) print(batch) # \u0026gt;\u0026gt;\u0026gt; PairDataBatch(x_s=[10, 16], edge_index_s=[2, 8], x_t=[8, 16], edge_index_t=[2, 6]) print(batch.edge_index_s) # \u0026gt;\u0026gt;\u0026gt; tensor([[0, 0, 0, 0, 5, 5, 5, 5], # [1, 2, 3, 4, 6, 7, 8, 9]]) print(batch.edge_index_t) # \u0026gt;\u0026gt;\u0026gt; tensor([[0, 0, 0, 4, 4, 4], # [1, 2, 3, 5, 6, 7]]) 到目前为止一切顺利！即使 $G_s$ 和 $G_t$ 使用不同数量的节点，edge_index_s 和 edge_index_t 也能正确地 batch 到一起。然而，batch 属性（将每个节点映射到其各自的图）丢失了，因为 PyG 无法识别 PairData object 中的实际图。这就是 DataLoader 的 follow_batch 参数发挥作用的地方。在这里，我们可以指定要为哪些属性维护 batch 信息：\nloader = DataLoader(data_list, batch_size=2, follow_batch=[\u0026#39;x_s\u0026#39;, \u0026#39;x_t\u0026#39;]) batch = next(iter(loader)) print(batch) # \u0026gt;\u0026gt;\u0026gt; PairDataBatch(x_s=[10, 16], edge_index_s=[2, 8], x_s_batch=[10], x_t=[8, 16], edge_index_t=[2, 6], x_t_batch=[8]) print(batch.x_s_batch) # \u0026gt;\u0026gt;\u0026gt; tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) print(batch.x_t_batch) # \u0026gt;\u0026gt;\u0026gt; tensor([0, 0, 0, 0, 1, 1, 1, 1]) 正如所见，follow_batch=['x_s', 'x_t'] 现在成功地为节点特征 x_s 和 x_t 分别创建了分配向量 x_s_batch 和 x_t_batch。现在可以使用这些信息在单个 Batch object 中对多个图执行 reduce 操作，例如 global pooling。\n二分图（Bipartite Graphs） bipartite graph 的 adjacency matrix 定义了两种不同节点类型之间的关系。通常，每种节点类型的节点数量不必匹配，从而导致一个形状为 $A \\in \\{0, 1\\}^{N \\times M}$ 且可能 $N \\ne M$ 的非方阵 adjacency matrix。在 bipartite graphs 的 mini-batching 过程中，edge_index 中边的源节点应以不同于 edge_index 中边的目标节点的方式增加。为了实现这一点，考虑一个介于两种节点类型之间的 bipartite graph，分别具有相应的节点特征 x_s 和 x_t：\nfrom torch_geometric.data import Data class BipartiteData(Data): pass data = BipartiteData(x_s=x_s, x_t=x_t, edge_index=edge_index) 对于 bipartite graphs 中正确的 mini-batching 过程，我们需要告诉 PyG 它应该独立地增加 edge_index 中边的源节点和目标节点：\nclass BipartiteData(Data): def __inc__(self, key, value, *args, kwargs): if key == \u0026#39;edge_index\u0026#39;: return torch.tensor([[self.x_s.size(0)], [self.x_t.size(0)]]) return super().__inc__(key, value, *args, kwargs) 在这里，edge_index[0]（边的源节点）增加了 x_s.size(0)，而 edge_index[1]（边的目标节点）增加了 x_t.size(0)。我们可以再次通过运行一个简单的测试脚本来测试我们的实现：\nfrom torch_geometric.loader import DataLoader import torch x_s = torch.randn(2, 16) # 2 nodes. x_t = torch.randn(3, 16) # 3 nodes. edge_index = torch.tensor([ [0, 0, 1, 1], [0, 1, 1, 2], ]) data = BipartiteData(x_s=x_s, x_t=x_t, edge_index=edge_index) data_list = [data, data] loader = DataLoader(data_list, batch_size=2) batch = next(iter(loader)) print(batch) # \u0026gt;\u0026gt;\u0026gt; BipartiteDataBatch(x_s=[4, 16], x_t=[6, 16], edge_index=[2, 8]) print(batch.edge_index) # \u0026gt;\u0026gt;\u0026gt; tensor([[0, 0, 1, 1, 2, 2, 3, 3], # [0, 1, 1, 2, 3, 4, 4, 5]]) 再次，这正是我们想要的行为！\n沿新维度进行 Batching（Batching Along New Dimensions） 有时，Data object 的属性应该通过获得一个新的 batch dimension 来进行 batching（如经典 mini-batching 中那样），例如对于图级别的属性或目标。具体来说，形状为 [num_features] 的属性列表应作为 [num_examples, num_features] 返回，而不是 [num_examples * num_features]。PyG 通过在 __cat_dim__() 中返回一个 None 的拼接维度来实现这一点：\nfrom torch_geometric.data import Data from torch_geometric.loader import DataLoader class MyData(Data): def __cat_dim__(self, key, value, *args, kwargs): if key == \u0026#39;foo\u0026#39;: return None return super().__cat_dim__(key, value, *args, kwargs) edge_index = torch.tensor([ [0, 1, 1, 2], [1, 0, 2, 1], ]) foo = torch.randn(16) data = MyData(num_nodes=3, edge_index=edge_index, foo=foo) data_list = [data, data] loader = DataLoader(data_list, batch_size=2) batch = next(iter(loader)) print(batch) # \u0026gt;\u0026gt;\u0026gt; MyDataBatch(num_nodes=6, edge_index=[2, 8], foo=[2, 16]) 如预期，batch.foo 现在由两个维度描述：batch dimension 和 feature dimension。\n","date":"16 October, 2025","id":8,"permalink":"/documents/pyg-batching/","summary":"内容来自这里.","tags":"","title":"PyG Batching"},{"content":"矢量图 位图，hires 位图，lowres ","date":"12 October, 2025","id":9,"permalink":"/documents/carrot/","summary":"","tags":"","title":"我的萝卜头像"},{"content":"1. 用默认浏览器打开 2. 按F12，然后把Focus放在html 先按F12，然后按下面的步骤操作\n3. 搜索音频扩展名 先随便点一下里面的代码，然后 Ctrl-F\n搜索.m4a\n找到一个含有.m4a 的链接，然后双击，复制到新的标签页打开。很长一串 4. 下载 ","date":"5 October, 2025","id":10,"permalink":"/documents/quanmin-download/","summary":"","tags":"","title":"全民K歌如何下载歌曲"},{"content":"完整的问题 可以看我和Gemini的对话\n抽象出的问题 假设在mxn的矩阵中全为0，现在进行两个操作：\n以avgdeg/n的概率将矩阵中的值变为1； 如果矩阵中有一列有一个1，那就把这一列都变成1。 现在计算avgdeg\u0026rsquo;为矩阵中1的个数/(mn)，记avgdeg=d, avgdeg\u0026rsquo;=d'\nKimi的回答 我们来逐步分析这个问题，并给出数学推导。\n问题设定 我们有一个 $ m \\times n $ 的矩阵，初始时所有元素为 0。\n进行两个操作：\n随机置 1：以概率 $ \\frac{\\text{avgdeg}}{n} $ 将每个元素独立地变为 1（否则保持 0）。\n记 $ d = \\text{avgdeg} $，所以每个元素变为 1 的概率是 $ p = \\frac{d}{n} $。\n列传播：如果某一列中至少有一个 1，则将该列所有元素都变为 1。\n最终，我们计算新的平均度：\n\\[ d' = \\frac{\\text{矩阵中 1 的总数}}{mn} \\]我们的目标是求 $ d' $ 关于 $ d $ 的表达式，即 $ d' = f(d) $。\n步骤 1：计算某一列在操作 1 后至少有一个 1的概率 在操作 1 后，每个元素独立地以概率 $ p = \\frac{d}{n} $ 变为 1。\n某一列有 $ m $ 个元素，全为 0 的概率是：\n\\[ (1 - p)^m = \\left(1 - \\frac{d}{n}\\right)^m \\]所以，该列至少有一个 1 的概率是：\n\\[ P_{\\text{active}} = 1 - \\left(1 - \\frac{d}{n}\\right)^m \\]步骤 2：操作 2 后，每一列要么全为 1，要么全为 0 因为在操作 2 中，只要某一列有一个 1，整列就变成 1；否则保持全 0。\n所以，最终矩阵中，每一列要么是全 1，要么是全 0。\n步骤 3：计算期望的 1 的总数 每一列以概率 $ P_{\\text{active}} $ 变成全 1（即贡献 $ m $ 个 1），以概率 $ 1 - P_{\\text{active}} $ 保持全 0（即贡献 0 个 1）。\n所以，期望的 1 的总数为：\n\\[ \\mathbb{E}[\\text{total 1s}] = n \\cdot m \\cdot P_{\\text{active}} = mn \\left(1 - \\left(1 - \\frac{d}{n}\\right)^m\\right) \\]步骤 4：计算 $ d' $ \\[ d' = \\frac{\\mathbb{E}[\\text{total 1s}]}{mn} = 1 - \\left(1 - \\frac{d}{n}\\right)^m \\]最终答案 $\\boxed{d' = 1 - \\left(1 - \\frac{d}{n}\\right)^m}$\n这就是 $ d' $ 关于 $ d $ 的显式表达式。\n计算我们所需的结果 $m=8 / m=16$ $d$ 的分布统计量 $\\mu=5.856, \\sigma =0.133$, 这里取 $6$ $n$ 的典型值为 5000~50000 得到结果:\nm=8,n=5000,d\u0026#39;=0.009559776622987126,d\u0026#39;*n=47.798883114935634 m=8,n=50000,d\u0026#39;=0.0009595968967535118,d\u0026#39;*n=47.979844837675586 m=16,n=5000,d\u0026#39;=0.01902816391689277,d\u0026#39;*n=95.14081958446386 m=16,n=50000,d\u0026#39;=0.0019182729673028476,d\u0026#39;*n=95.91364836514238 极限 问题 如果n趋于无穷，nd\u0026rsquo;的极限是多少？m和d保留\nKimi的回答 我们已有\n\\[ d' = 1 - \\left(1 - \\frac{d}{n}\\right)^m. \\]令 $n\\to\\infty$ 而 $m,d$ 固定，考察\n\\[ n d' = n\\left[1 - \\left(1 - \\frac{d}{n}\\right)^m\\right]. \\]利用极限\n\\[ \\lim_{n\\to\\infty} n\\left[1 - \\left(1 - \\frac{d}{n}\\right)^m\\right] = \\lim_{n\\to\\infty} n\\left[1 - \\exp\\left(m\\ln\\!\\Bigl(1 - \\frac{d}{n}\\Bigr)\\right)\\right]. \\]对 $\\ln(1 - x)$ 做泰勒展开：$\\ln(1 - x) = -x - \\frac{x^2}{2} - \\dots$，取首项即可：\n\\[ m\\ln\\!\\Bigl(1 - \\frac{d}{n}\\Bigr) = -\\frac{md}{n} + O\\!\\left(\\frac{1}{n^2}\\right). \\]于是\n\\[ \\exp\\left(m\\ln\\!\\Bigl(1 - \\frac{d}{n}\\Bigr)\\right) = \\exp\\!\\left(-\\frac{md}{n} + O\\!\\left(\\frac{1}{n^2}\\right)\\right) = 1 - \\frac{md}{n} + O\\!\\left(\\frac{1}{n^2}\\right). \\]代回：\n\\[ n d' = n\\left[1 - \\left(1 - \\frac{md}{n} + O\\!\\left(\\frac{1}{n^2}\\right)\\right)\\right] = n\\left[\\frac{md}{n} + O\\!\\left(\\frac{1}{n^2}\\right)\\right] = md + O\\!\\left(\\frac{1}{n}\\right). \\]因此\n\\[ \\boxed{\\lim_{n\\to\\infty} n d' = md}. \\]感悟 这么简单的数学题我居然都要用AI了, 看来上班后数学能力大幅下滑.\n","date":"4 October, 2025","id":11,"permalink":"/posts/interesting-math-gnn-tensor-core/","summary":"可以看我和Gemini的对话","tags":"","title":"一个关于GNN在Tensor Core上的实现的数学问题"},{"content":"Github: https://github.com/spotify/pedalboard\nDocs: https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.LowShelfFilter\nPedalboard API Documentation The pedalboard module provides classes and functions for adding effects to audio. Most classes in this module are subclasses of Plugin, each of which allows applying effects to an audio buffer or stream.\nNote: For audio I/O functionality (i.e.: reading and writing audio files), see the pedalboard.io module.\nThe pedalboard module is named after the concept of a guitar pedalboard, in which musicians will chain various effects pedals together to give them complete control over their sound. The pedalboard module implements this concept with its main Pedalboard class:\nfrom pedalboard import Pedalboard, Chorus, Distortion, Reverb # Create an empty Pedalboard object: my_pedalboard = Pedalboard() # Treat this object like a Python list: my_pedalboard.append(Chorus()) my_pedalboard.append(Distortion()) my_pedalboard.append(Reverb()) # Pass audio through this pedalboard: output_audio = my_pedalboard(input_audio, input_audio_samplerate) Pedalboard objects are lists of zero or more Plugin objects, and Pedalboard objects themselves are subclasses of Plugin - which allows for nesting and composition.\nClasses pedalboard.AudioProcessorParameter A wrapper around various different parameters exposed by VST3Plugin or AudioUnitPlugin instances.\nAudioProcessorParameter objects are rarely used directly, and usually used via their implicit interface:\nmy_plugin = load_plugin(\u0026#34;My Cool Audio Effect.vst3\u0026#34;) # Print all of the parameter names: print(my_plugin.parameters.keys()) # [\u0026#34;mix\u0026#34;, \u0026#34;delay_time_ms\u0026#34;, \u0026#34;foobar\u0026#34;] # Access each parameter as if it were just a Python attribute: my_plugin.mix = 0.5 my_plugin.delay_time_ms = 400 Note: AudioProcessorParameter tries to guess the range of valid parameter values, as well as the type/unit of the parameter, when instantiated. This guess may not always be accurate. Raw control over the underlying parameter’s value can be had by accessing the raw_value attribute, which is always bounded on [0, 1] and is passed directly to the underlying plugin object.\nProperties label: Optional[str] – The units used by this parameter (Hz, dB, etc). May be None if the plugin does not expose units for this parameter or if automatic unit detection fails. units: Optional[str] – Alias for \u0026ldquo;label\u0026rdquo; – the units used by this parameter (Hz, dB, etc). May be None if the plugin does not expose units for this parameter or if automatic unit detection fails. pedalboard.ExternalPlugin A wrapper around a third-party effect plugin.\nDon’t use this directly; use one of pedalboard.VST3Plugin or pedalboard.AudioUnitPlugin instead.\nMethods __call__(*args, **kwargs) – Overloaded function. __call__(self: pedalboard.Plugin, input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio or MIDI buffer through this plugin, returning audio. Alias for process(). __call__(self: pedalboard.ExternalPlugin, midi_messages: object, duration: float, sample_rate: float, num_channels: int = 2, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio or MIDI buffer through this plugin, returning audio. Alias for process(). Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. Methods process(*args, **kwargs) – Overloaded function. process(self: pedalboard.ExternalPlugin, midi_messages: object, duration: float, sample_rate: float, num_channels: int = 2, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Pass a buffer of audio (as a 32- or 64-bit NumPy array) or a list of MIDI messages to this plugin, returning audio. process(self: pedalboard.Plugin, input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Pass a buffer of audio (as a 32- or 64-bit NumPy array) or a list of MIDI messages to this plugin, returning audio. Note: The process() method can also be used via __call__(); i.e.: just calling this object like a function (my_plugin(...)) will automatically invoke process() with the same arguments.\nreset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. pedalboard.Pedalboard(plugins: Optional[List[Plugin]] = None) A container for a series of Plugin objects, to use for processing audio, like a guitar pedalboard.\nPedalboard objects act like regular Python List objects, but come with an additional process() method (also aliased to __call__()), allowing audio to be passed through the entire Pedalboard object for processing:\nmy_pedalboard = Pedalboard() my_pedalboard.append(Reverb()) output_audio = my_pedalboard(input_audio) Warning: Pedalboard objects may only contain effects plugins (i.e.: those for which is_effect is True), and cannot contain instrument plugins (i.e.: those for which is_instrument is True).\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). append(plugin: Plugin) -\u0026gt; None – Append a plugin to the end of this container. insert(index: int, plugin: Plugin) -\u0026gt; None – Insert a plugin at the specified index. process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. remove(plugin: Plugin) -\u0026gt; None – Remove a plugin by its value. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.load_plugin(path_to_plugin_file: str, parameter_values: Dict[str, Union[str, int, float, bool]] = {}, plugin_name: Optional[str] = None, initialization_timeout: float = 10.0) -\u0026gt; ExternalPlugin Load an audio plugin.\nTwo plugin formats are supported:\nVST3® format is supported on macOS, Windows, and Linux Audio Units are supported on macOS Parameters path_to_plugin_file (str) – The path of a VST3® or Audio Unit plugin file or bundle. parameter_values (Dict[str, Union[str, int, float, bool]]) – An optional dictionary of initial values to provide to the plugin after loading. Keys in this dictionary are expected to match the parameter names reported by the plugin, but normalized to strings that can be used as Python identifiers. (These are the same identifiers that are used as keys in the .parameters dictionary of a loaded plugin.) plugin_name (Optional[str]) – An optional plugin name that can be used to load a specific plugin from a multi-plugin package. If a package is loaded but a plugin_name is not provided, an exception will be thrown. initialization_timeout (float) – The number of seconds that Pedalboard will spend trying to load this plugin. Some plugins load resources asynchronously in the background on startup; using larger values for this parameter can give these plugins time to load properly. Introduced in v0.7.6. Returns An instance of pedalboard.VST3Plugin or pedalboard.AudioUnitPlugin.\nThrows ImportError – if the plugin cannot be found or loaded RuntimeError – if the plugin file contains more than one plugin, but no plugin_name was provided pedalboard.Bitcrush A plugin that reduces the signal to a given bit depth, giving the audio a lo-fi, digitized sound. Floating-point bit depths are supported.\nBitcrushing changes the amount of \u0026ldquo;vertical\u0026rdquo; resolution used for an audio signal (i.e.: how many unique values could be used to represent each sample). For an effect that changes the \u0026ldquo;horizontal\u0026rdquo; resolution (i.e.: how many samples are available per second), see pedalboard.Resample.\nProperties bit_depth – The bit depth to quantize the signal to. Must be between 0 and 32 bits. May be an integer, decimal, or floating-point value. Each audio sample will be quantized onto 2 ** bit_depth values. Methods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Chain Run zero or more plugins as a plugin. Useful when used with the Mix plugin.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). append(plugin: Plugin) -\u0026gt; None – Append a plugin to the end of this container. insert(index: int, plugin: Plugin) -\u0026gt; None – Insert a plugin at the specified index. process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. remove(plugin: Plugin) -\u0026gt; None – Remove a plugin by its value. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Chorus A basic chorus effect.\nThis audio effect can be controlled via the speed and depth of the LFO controlling the frequency response, a mix control, a feedback control, and the centre delay of the modulation.\nNote: To get classic chorus sounds try to use a centre delay time around 7-8 ms with a low feedback volume and a low depth. This effect can also be used as a flanger with a lower centre delay time and a lot of feedback, and as a vibrato effect if the mix value is 1.\nProperties rate_hz – The speed of the chorus effect’s low-frequency oscillator (LFO), in Hertz. This value must be between 0 Hz and 100 Hz. Methods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Clipping A distortion plugin that adds hard distortion to the signal by clipping the signal at the provided threshold (in decibels).\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Compressor A dynamic range compressor, used to reduce the volume of loud sounds and \u0026ldquo;compress\u0026rdquo; the loudness of the signal.\nFor a lossy compression algorithm that introduces noise or artifacts, see pedalboard.MP3Compressor or pedalboard.GSMCompressor.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Convolution An audio convolution, suitable for things like speaker simulation or reverb modeling.\nThe convolution impulse response can be specified either by filename or as a 32-bit floating point NumPy array. If a NumPy array is provided, the sample_rate argument must also be provided to indicate the sample rate of the impulse response.\nSupport for passing NumPy arrays as impulse responses introduced in v0.9.10.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Delay A digital delay plugin with controllable delay time, feedback percentage, and dry/wet mix.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Distortion A distortion effect, which applies a non-linear (tanh, or hyperbolic tangent) waveshaping function to apply harmonically pleasing distortion to a signal.\nThis plugin produces a signal that is roughly equivalent to running:\ndef distortion(x): return tanh(x * db_to_gain(drive_db)) Methods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.ExternalPluginReloadType Indicates the behavior of an external plugin when reset() is called.\nMembers Unknown – The behavior of the plugin is unknown. This will force a full reinstantiation of the plugin every time reset is called. ClearsAudioOnReset – This plugin clears its internal buffers correctly when reset() is called. The plugin will not be reinstantiated when reset is called. PersistsAudioOnReset – This plugin does not clear its internal buffers as expected when reset() is called. This will force a full reinstantiation of the plugin every time reset is called. Properties name pedalboard.GSMFullRateCompressor An audio degradation/compression plugin that applies the GSM \u0026ldquo;Full Rate\u0026rdquo; compression algorithm to emulate the sound of a 2G cellular phone connection. This plugin internally resamples the input audio to a fixed sample rate of 8kHz (required by the GSM Full Rate codec), although the quality of the resampling algorithm can be specified.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Gain A gain plugin that increases or decreases the volume of a signal by amplifying or attenuating it by the provided value (in decibels). No distortion or other effects are applied.\nThink of this as a volume control.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.HighShelfFilter A high shelf filter plugin with variable Q and gain, as would be used in an equalizer. Frequencies above the cutoff frequency will be boosted (or cut) by the provided gain (in decibels).\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.HighpassFilter Apply a first-order high-pass filter with a roll-off of 6dB/octave. The cutoff frequency will be attenuated by -3dB (i.e.: 0.707x as loud, expressed as a gain factor) and lower frequencies will be attenuated by a further 6dB per octave.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.IIRFilter An abstract class that implements various kinds of infinite impulse response (IIR) filter designs. This should not be used directly; use HighShelfFilter, LowShelfFilter, or PeakFilter directly instead.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Invert Flip the polarity of the signal. This effect is not audible on its own and takes no parameters. This effect is mathematically identical to def invert(x): return -x.\nInverting a signal may be useful to cancel out signals in many cases; for instance, Invert can be used with the Mix plugin to remove the original signal from an effects chain that contains multiple signals.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.LadderFilter A multi-mode audio filter based on the classic Moog synthesizer ladder filter, invented by Dr. Bob Moog in 1968.\nDepending on the filter’s mode, frequencies above, below, or on both sides of the cutoff frequency will be attenuated. Higher values for the resonance parameter may cause peaks in the frequency response around the cutoff frequency.\nClass Mode The type of filter architecture to use.\nMembers LPF12 – A low-pass filter with 12 dB of attenuation per octave above the cutoff frequency. HPF12 – A high-pass filter with 12 dB of attenuation per octave below the cutoff frequency. BPF12 – A band-pass filter with 12 dB of attenuation per octave on both sides of the cutoff frequency. LPF24 – A low-pass filter with 24 dB of attenuation per octave above the cutoff frequency. HPF24 – A high-pass filter with 24 dB of attenuation per octave below the cutoff frequency. BPF24 – A band-pass filter with 24 dB of attenuation per octave on both sides of the cutoff frequency. Properties name Methods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Limiter A simple limiter with standard threshold and release time controls, featuring two compressors and a hard clipper at 0 dB.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.LowShelfFilter A low shelf filter with variable Q and gain, as would be used in an equalizer. Frequencies below the cutoff frequency will be boosted (or cut) by the provided gain value.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.LowpassFilter Apply a first-order low-pass filter with a roll-off of 6dB/octave. The cutoff frequency will be attenuated by -3dB (i.e.: 0.707x as loud).\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.MP3Compressor An MP3 compressor plugin that runs the LAME MP3 encoder in real-time to add compression artifacts to the audio stream.\nCurrently only supports variable bit-rate mode (VBR) and accepts a floating-point VBR quality value (between 0.0 and 10.0; lower is better).\nNote that the MP3 format only supports 8kHz, 11025Hz, 12kHz, 16kHz, 22050Hz, 24kHz, 32kHz, 44.1kHz, and 48kHz audio; if an unsupported sample rate is provided, an exception will be thrown at processing time.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Mix A utility plugin that allows running other plugins in parallel. All plugins provided will be mixed equally.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). append(plugin: Plugin) -\u0026gt; None – Append a plugin to the end of this container. insert(index: int, plugin: Plugin) -\u0026gt; None – Insert a plugin at the specified index. process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. remove(plugin: Plugin) -\u0026gt; None – Remove a plugin by its value. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.NoiseGate A simple noise gate with standard threshold, ratio, attack time and release time controls. Can be used as an expander if the ratio is low.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.PeakFilter A peak (or notch) filter with variable Q and gain, as would be used in an equalizer. Frequencies around the cutoff frequency will be boosted (or cut) by the provided gain value.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Phaser A 6 stage phaser that modulates first order all-pass filters to create sweeping notches in the magnitude frequency response. This audio effect can be controlled with standard phaser parameters: the speed and depth of the LFO controlling the frequency response, a mix control, a feedback control, and the centre frequency of the modulation.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.PitchShift A pitch shifting effect that can change the pitch of audio without affecting its duration.\nThis effect uses Chris Cannam’s wonderful Rubber Band library audio stretching library.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Plugin A generic audio processing plugin. Base class of all Pedalboard plugins.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.PluginContainer A generic audio processing plugin that contains zero or more other plugins. Not intended for direct use.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). append(plugin: Plugin) -\u0026gt; None – Append a plugin to the end of this container. insert(index: int, plugin: Plugin) -\u0026gt; None – Insert a plugin at the specified index. process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. remove(plugin: Plugin) -\u0026gt; None – Remove a plugin by its value. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.Resample A plugin that downsamples the input audio to the given sample rate, then upsamples it back to the original sample rate. Various quality settings will produce audible distortion and aliasing effects.\nClass Quality Indicates which specific resampling algorithm to use.\nResampling algorithms each provide a different tradeoff between speed and quality. Pedalboard provides two different types of resampling algorithms:\nAliasing algorithms, which cause high frequencies to appear as lower frequencies. Non-aliasing algorithms, which filter out high frequencies when downsampling and avoid introducing extra high-frequency content when upsampling. (These algorithms were introduced in Pedalboard v0.9.15.) Aliasing algorithms include:\nZeroOrderHold Linear CatmullRom Lagrange WindowedSinc Non-aliasing algorithms include:\nWindowedSinc256 WindowedSinc128 WindowedSinc64 WindowedSinc32 WindowedSinc16 WindowedSinc8 Choosing an algorithm to use depends on the signal being resampled, the relationship between the source and target sample rates, and the application of the resampled signal.\nIf downsampling by an integer factor (i.e.: from 44.1kHz to 22050Hz, or 48kHz to 24kHz), and if the source signal has no high-frequency content above half of the target sample rate the ZeroOrderHold algorithm will be the fastest by far and will produce no artifacts.\nIn all other cases, any of the numbered WindowedSinc algorithms (i.e.: WindowedSinc256, WindowedSinc64) will produce a clean signal with no artifacts. Higher numbers will produce a cleaner signal with less roll-off of high frequency content near the Nyquist frequency of the new sample rate.\nHowever, depending on your application, the artifacts introduced by each resampling method may be acceptable. Test each method to determine which is the best tradeoff between speed and accuracy for your use case.\nTo provide a good balance between speed and accuracy, WindowedSinc32 is the default from Pedalboard v0.9.15 onwards. (Previously, WindowedSinc was the default.)\nMembers ZeroOrderHold – The lowest quality and fastest resampling method, with lots of audible artifacts. Zero-order hold resampling chooses the next value to use based on the last value, without any interpolation. Think of it like nearest-neighbor resampling. Warning: This algorithm produces aliasing artifacts. Linear – A resampling method slightly less noisy than the simplest method. Linear resampling takes the average of the two nearest values to the desired sample, which is reasonably good for downsampling. Warning: This algorithm produces aliasing artifacts. CatmullRom – A moderately good-sounding resampling method which is fast to run. Slightly slower than Linear resampling, but slightly higher quality. Warning: This algorithm produces aliasing artifacts. Lagrange – A moderately good-sounding resampling method which is slow to run. Slower than CatmullRom resampling, but slightly higher quality. Warning: This algorithm produces aliasing artifacts. WindowedSinc – A very high quality (and the slowest) resampling method, with no audible artifacts when upsampling. This resampler applies a windowed sinc filter design with 100 zero-crossings of the sinc function to approximate an ideal brick-wall low-pass filter. Warning: This algorithm produces aliasing artifacts when downsampling, but not when upsampling. Note: This method was the default in versions of Pedalboard prior to v0.9.15. WindowedSinc256 – The highest possible quality resampling algorithm, with no audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 256 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter does not produce aliasing artifacts when upsampling or downsampling. Compare this in speed and quality to Resampy’s kaiser_best method. WindowedSinc128 – A very high quality resampling algorithm, with no audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 128 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter does not produce aliasing artifacts when upsampling or downsampling. This method is roughly as fast as Resampy’s kaiser_fast method, while producing results roughly equal in quality to Resampy’s kaiser_best method. WindowedSinc64 – A very high quality resampling algorithm, with few audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 64 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter does not produce aliasing artifacts when upsampling or downsampling. This method is (on average) faster than Resampy’s kaiser_fast method, and roughly equal in quality. WindowedSinc32 – A reasonably high quality resampling algorithm, with few audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 32 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter produces very few aliasing artifacts when upsampling or downsampling. This method is always faster than Resampy’s kaiser_fast method, while being reasonable in quality. Note: This method is the default in Pedalboard v0.9.15 and later. WindowedSinc16 – A medium quality resampling algorithm, with few audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 16 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter produces some aliasing artifacts when upsampling or downsampling. This method is faster than Resampy’s kaiser_fast method, while being acceptable in quality. WindowedSinc8 – A low quality resampling algorithm, with few audible artifacts when upsampling or downsampling. This resampler applies a windowed sinc filter with 16 zero-crossings to approximate an ideal brick-wall low-pass filter. This filter produces noticeable aliasing artifacts when upsampling or downsampling. This method can be more than 10x faster than Resampy’s kaiser_fast method, and is useful for applications that are tolerant of some resampling artifacts. Properties name Methods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. quality – The resampling algorithm used to resample the audio. target_sample_rate – The sample rate to resample the input audio to. This value may be a floating-point number, in which case a floating-point sampling rate will be used. Note that the output of this plugin will still be at the original sample rate; this is merely the sample rate used for quality reduction. pedalboard.Reverb A simple reverb effect. Uses a simple stereo reverb algorithm, based on the technique and tunings used in FreeVerb.\nMethods __call__(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio buffer through this plugin. Alias for process(). process(input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run a 32-bit or 64-bit floating point audio buffer through this plugin. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged. Properties is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin is not an audio effect and accepts only MIDI input, not audio. Introduced in v0.7.4. pedalboard.VST3Plugin A wrapper around third-party, audio effect or instrument plugins in Steinberg GmbH’s VST3® format.\nVST3® plugins are supported on macOS, Windows, and Linux. However, VST3® plugin files are not cross-compatible with different operating systems; a platform-specific build of each plugin is required to load that plugin on a given platform. (For example: a Windows VST3 plugin bundle will not load on Linux or macOS.)\nWarning: Some VST3® plugins may throw errors, hang, generate incorrect output, or outright crash if called from background threads. If you find that a VST3® plugin is not working as expected, try calling it from the main thread instead and open a GitHub Issue to track the incompatibility.\nSupport for instrument plugins introduced in v0.7.4.\nSupport for running VST3® plugins on background threads introduced in v0.8.8.\nMethods __call__(*args, **kwargs) – Overloaded function.\n__call__(self: pedalboard.Plugin, input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio or MIDI buffer through this plugin, returning audio. Alias for process(). __call__(self: pedalboard.VST3Plugin, midi_messages: object, duration: float, sample_rate: float, num_channels: int = 2, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Run an audio or MIDI buffer through this plugin, returning audio. Alias for process(). load_preset(preset_file_path: str) -\u0026gt; None – Load a VST3 preset file in .vstpreset format.\nprocess(*args, **kwargs) – Overloaded function.\nprocess(self: pedalboard.Plugin, input_array: numpy.ndarray, sample_rate: float, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Pass a buffer of audio (as a 32- or 64-bit NumPy array) or a list of MIDI messages to this plugin, returning audio. process(self: pedalboard.VST3Plugin, midi_messages: object, duration: float, sample_rate: float, num_channels: int = 2, buffer_size: int = 8192, reset: bool = True) -\u0026gt; numpy.ndarray[numpy.float32] – Pass a buffer of audio (as a 32- or 64-bit NumPy array) or a list of MIDI messages to this plugin, returning audio. reset() -\u0026gt; None – Clear any internal state stored by this plugin (e.g.: reverb tails, delay lines, LFO state, etc). The values of plugin parameters will remain unchanged.\nshow_editor(close_event: object = None) -\u0026gt; None – Show the UI of this plugin as a native window.\nThis method may only be called on the main thread, and will block the main thread until any of the following things happens:\nthe window is closed by clicking the close button the window is closed by pressing the appropriate (OS-specific) keyboard shortcut a KeyboardInterrupt (Ctrl-C) is sent to the program the threading.Event.set() method is called (by another thread) on a provided threading.Event object An example of how to programmatically close an editor window:\nimport pedalboard from threading import Event, Thread plugin = pedalboard.load_plugin(\u0026#34;../path-to-my-plugin-file\u0026#34;) close_window_event = Event() def other_thread(): # do something to determine when to close the window if should_close_window: close_window_event.set() thread = Thread(target=other_thread) thread.start() # This will block until the other thread calls .set(): plugin.show_editor(close_window_event) Properties category – A category that this plugin falls into, such as \u0026ldquo;Dynamics\u0026rdquo;, \u0026ldquo;Reverbs\u0026rdquo;, etc. Introduced in v0.9.4. descriptive_name – A more descriptive name for this plugin. This may be the same as the ‘name’ field, but some plugins may provide an alternative name. Introduced in v0.9.4. has_shared_container – True iff this plugin is part of a multi-plugin container. Introduced in v0.9.4. identifier – A string that can be saved and used to uniquely identify this plugin (and version) again. Introduced in v0.9.4. is_effect – True iff this plugin is an audio effect and accepts audio as input. Introduced in v0.7.4. is_instrument – True iff this plugin identifies itself as an instrument (generator, synthesizer, etc) plugin. Introduced in v0.9.4. manufacturer_name – The name of the manufacturer of this plugin, as reported by the plugin itself. Introduced in v0.9.4. name – The name of this plugin. preset_data – Get or set the current plugin state as bytes in .vstpreset format. Warning: This property can be set to change the plugin’s internal state, but providing invalid data may cause the plugin to crash, taking the entire Python process down with it.\nraw_state – A bytes object representing the plugin’s internal state. For the VST3 format, this is usually an XML-encoded string prefixed with an 8-byte header and suffixed with a single null byte. Warning: This property can be set to change the plugin’s internal state, but providing invalid data may cause the plugin to crash, taking the entire Python process down with it.\nreported_latency_samples – The number of samples of latency (delay) that this plugin reports to introduce into the audio signal due to internal buffering and processing. Pedalboard automatically compensates for this latency during processing, so this property is present for informational purposes. Note that not all plugins correctly report the latency that they introduce, so this value may be inaccurate (especially if the plugin reports 0). Introduced in v0.9.12. version – The version string for this plugin, as reported by the plugin itself. Introduced in v0.9.4. Functions time_stretch def time_stretch( input_audio: numpy.ndarray[numpy.float32], samplerate: float, stretch_factor: Union[float, numpy.ndarray[numpy.float64]] = 1.0, pitch_shift_in_semitones: Union[float, numpy.ndarray[numpy.float64]] = 0.0, high_quality: bool = True, transient_mode: str = \u0026#34;crisp\u0026#34;, transient_detector: str = \u0026#34;compound\u0026#34;, retain_phase_continuity: bool = True, use_long_fft_window: Optional[bool] = None, use_time_domain_smoothing: bool = False, preserve_formants: bool = True, ) -\u0026gt; numpy.ndarray[numpy.float32] Time-stretch (and optionally pitch-shift) a buffer of audio, changing its length.\nUsing a higher stretch_factor will shorten the audio – i.e., a stretch_factor of 2.0 will double the speed of the audio and halve the length of the audio, without changing the pitch of the audio.\nThis function allows for changing the pitch of the audio during the time stretching operation. The stretch_factor and pitch_shift_in_semitones arguments are independent and do not affect each other (i.e.: you can change one, the other, or both without worrying about how they interact).\nBoth stretch_factor and pitch_shift_in_semitones can be either floating-point numbers or NumPy arrays of double-precision floating point numbers. Providing a NumPy array allows the stretch factor and/or pitch shift to vary over the length of the output audio.\nNote: If a NumPy array is provided for stretch_factor or pitch_shift_in_semitones:\nThe length of each array must be the same as the length of the input audio. More frequent changes in the stretch factor or pitch shift will result in slower processing, as the audio will be processed in smaller chunks. Changes to the stretch_factor or pitch_shift_in_semitones more frequent than once every 1,024 samples (23 milliseconds at 44.1kHz) will not have any effect. The additional arguments provided to this function allow for more fine-grained control over the behavior of the time stretcher:\nhigh_quality (the default) enables a higher quality time stretching mode. Set this option to False to use less CPU power. transient_mode controls the behavior of the stretcher around transients (percussive parts of the audio). Valid options are \u0026quot;crisp\u0026quot; (the default), \u0026quot;mixed\u0026quot;, or \u0026quot;smooth\u0026quot;. transient_detector controls which method is used to detect transients in the audio signal. Valid options are \u0026quot;compound\u0026quot; (the default), \u0026quot;percussive\u0026quot;, or \u0026quot;soft\u0026quot;. retain_phase_continuity ensures that the phases of adjacent frequency bins in the audio stream are kept as similar as possible. Set this to False for a softer, phasier sound. use_long_fft_window controls the size of the fast-Fourier transform window used during stretching. The default (None) will result in a window size that varies based on other parameters and should produce better results in most situations. Set this option to True to result in a smoother sound (at the expense of clarity and timing), or False to result in a crisper sound. use_time_domain_smoothing can be enabled to produce a softer sound with audible artifacts around sharp transients. This option mixes well with use_long_fft_window=False. preserve_formants allows shifting the pitch of notes without substantially affecting the pitch profile (formants) of a voice or instrument. Warning: This is a function, not a Plugin instance, and cannot be used in Pedalboard objects, as it changes the duration of the audio stream.\nNote: The ability to pass a NumPy array for stretch_factor and pitch_shift_in_semitones was added in Pedalboard v0.9.8.\n","date":"4 October, 2025","id":12,"permalink":"/documents/pedalboard/","summary":"Github: https://github.com/spotify/pedalboard","tags":"","title":"Pedalboard 文档"},{"content":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.\nIf you\u0026rsquo;re interested, please give them a star and try them out! ❤️\nThe Origin of the Story I recently read papers on physical simulation and wanted to reproduce them. I started with Stable Neo-Hookean Flesh Simulation, though the choice isn\u0026rsquo;t critical. Many modern physical simulations are implicit, requiring Newton\u0026rsquo;s method to solve optimization problems.\nThis involves:\nComputing derivatives of the constitutive energy model (first-order gradient, second-order Hessian). Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs. From Dynamic Deformables, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:\nSymbolic differentiation with code generation. Automatic differentiation. Tools for the former include MATLAB or SymPy; for the latter, deep learning libraries like PyTorch or more suitable ones like TinyAD.\nWhy TinyAD? Deep learning libraries differentiate at the tensor level, but I needed scalar-level differentiation for physical simulations. Tensor-level differentiation could lead to unplayable frame rates.\nA problem arose: these tools are in the C++ toolchain, and I\u0026rsquo;m not proficient in C++ (I know some kindergarten-level C++, but CMake and libraries like Eigen defeated me after three days of trying). So, I switched to Rust, a language I\u0026rsquo;m more comfortable with. This was the start of all troubles…\nA Path That Seems Simple Rust lacks an automatic differentiation library for second-order Hessians (at least on crates.io). SymPy can generate Rust code, but it\u0026rsquo;s buggy. Given the implementation complexity, I started with symbolic code generation, creating Symars.\nSymPy\u0026rsquo;s symbolic expressions are tree-structured, with nodes as operators (Add, Mul, Div, Sin, etc.) or constants/symbols, and children as operands. Code generation involves depth-first traversal: compute child expressions, then the current node\u0026rsquo;s expression based on its type. Base cases are constants or symbols.\nI used the generated derivatives for a simple implicit spring-mass system, but debugging index errors in Hessian assembly was time-consuming.\nTrying the Untrodden Path Again To address this, I revisited automatic differentiation, aiming to adapt TinyAD for Rust.\nTwo Ways to Walk the Same Path Initially, I considered two approaches:\nWrite FFI bindings, as I don\u0026rsquo;t know C++ well. Replicate TinyAD\u0026rsquo;s logic. Cloning TinyAD, I couldn\u0026rsquo;t even pull dependencies or compile it. Examining the codebase, I found the core logic was ~1000 lines — manageable to replicate without running the project. Thus, Raddy was born.\nSymbolic diff \u0026amp; Codegen: Implementation Implementation details:\nEach scalar in the differentiation chain carries a gradient and Hessian, increasing memory overhead. I avoided implementing the Copy trait, requiring explicit cloning. Operator traits between (\u0026amp;)Type and (\u0026amp;)Type (four combinations) required repetitive code. I considered the following options: Macros. Python scripts for code generation. Macros breaks rust-analyzer (somebody refuse to agree on this, but for me this is true) and I am rather unfamiliar with Rust\u0026rsquo;s macro syntax, so I used Python scripts (in the meta/ directory) for simple string concatenation.\nTesting: I verified derivatives by generating symbolic grad and hessian code with Symars, cross-validating against Raddy\u0026rsquo;s results, ensuring test expressions covered all implemented methods. Symars performed reliably, without bugs. What about sparse matrices Dense matrices store adjacent values contiguously, but sparse matrices (with millions of elements) don\u0026rsquo;t. I implemented sparse Hessian assembly:\nDefine a problem via the Objective\u0026lt;N\u0026gt; trait: Specify problem size N (a compile-time constant for const generics). Implement computation logic, e.g., a spring-mass system (Hooke\u0026rsquo;s law, E=1/2 k x²): impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } Specify input components\u0026rsquo; indices (\u0026amp;[[usize; N]]). Automatically assemble sparse grad and hess (handling index mapping). Manually sum multiple grad and hess (simple matrix addition; triplet matrices are concatenated). Before tests, Raddy was 2.2k lines; after, it ballooned to 18k lines, showing LOC is a poor metric.\nFinally, I wrote a demo for fun and as an example.\nConclusion Gains:\nLearned how automatic differentiation works. First time using AI for documentation (it struggled with Rust syntax, producing test code with errors). Happiness! ","date":"2 October, 2025","id":13,"permalink":"/english-post/raddy/","summary":"TL;DR: I created Raddy, a forward autodiff library, and Symars, a symbolic codegen library.","tags":"rust graphics math","title":"Raddy devlog: forward autodiff system"},{"content":"From the perspective of a newbie user\nThe Documentation is a Disaster Recently, I had to optimize a custom operator and decided to use OpenAI\u0026rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.\nIf the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.\nHow Triton Fails at Clarity Take the tl.load documentation as an example. It mentions that block pointers support \u0026ldquo;boundary checks\u0026rdquo; and \u0026ldquo;padding options,\u0026rdquo; but:\nWhat does \u0026ldquo;boundary check\u0026rdquo; actually do? Does it skip out-of-bounds elements, returning a smaller tensor? Does it pad with a default value? Does it throw an error? The docs don\u0026rsquo;t say. What\u0026rsquo;s the \u0026ldquo;padding option\u0026rdquo;? After some trial and error, I realized it handles out-of-bounds elements — but this should be explicitly stated, not left for users to reverse-engineer.\nAnother issue: tl.make_block_ptr and tl.arange require block shapes and element counts to be powers of two. This restriction isn\u0026rsquo;t mentioned anywhere in the official docs. I only discovered it after hitting an error and finding a passing reference in an unofficial blog post.\nWhoever wrote this documentation did a huge disservice to the engineers who built Triton\u0026rsquo;s compiler. Triton\u0026rsquo;s compiler is awesome.\nKey API Clarifications tl.load For raw pointers (or tensors of pointers): Always set mask and other. mask=True: Load from HBM. mask=False: Use the value from other (a float). For block pointers (tl.make_block_ptr): Enable boundary checks on all dimensions and set padding=\u0026quot;zero\u0026quot;. The behavior of boundary_check is poorly defined, especially after reordering dimensions. Shape Constraints tl.arange element counts and tl.make_block_ptr block shapes must be powers of two. This might apply to all Triton tensor dimensions, but I haven\u0026rsquo;t verified it.\nMemory Access Pitfalls tl.load and tl.store silently corrupt data. Invalid memory access turns values into NaN—yes, even tl.store can corrupt valid data! Solution: Unless your dimensions are multiples of 64, always enable boundary checks for HBM reads/writes. Extra caution: Raw pointers require careful mask handling to avoid disasters. ","date":"2 October, 2025","id":14,"permalink":"/english-post/triton-pitfalls/","summary":"From the perspective of a newbie user","tags":"deep-learning triton","title":"Triton Common Pitfalls"},{"content":"Intro: A taste of the Rust programming language\nRecently, I tried to get started with Rust and wanted to write some code.\nMost people\u0026rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).\nHowever, I\u0026rsquo;ve never learned how to write backend services (I\u0026rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I\u0026rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.\nNote: This post only reproduces/discusses the IPC family of algorithms and does not address any performance optimizations, whether the algorithm is efficient, or why not to use some other algorithm.\nProject repo: Github\nImplicit Euler Physical simulation is essentially a numerical integration process.\nExplicit integration tends to explode, but implicit integration suffers from a \u0026ldquo;chicken-and-egg\u0026rdquo; problem (calculating the position at the next timestep requires knowing the velocity at the next timestep), making it impossible to solve explicitly. Instead, it requires solving a (possibly non-convex) optimization problem.\nWhat can be implicitly integrated? A mass-spring system can. But honestly, I\u0026rsquo;ve never written an optimization-based implicit integrator before, so I decided to start by implementing a mass-spring system.\nWhat Is It? Incremental Potential (IP) is a function of the degrees of freedom (DOF) of a scene at time t, IP(t).\nImplicit Euler constructs an then minimizes the IP (x(t+\\Delta t) = \\argmin_x E_{\\text{IP}}(x(t))) to obtain the position at t+\\Delta t.\nDeep learning typically uses gradient descent (and its variants), but in graphics, empirical evidence suggests gradient descent performs poorly. So, we opt for Newton\u0026rsquo;s method.\nImplementation Newton\u0026rsquo;s method is faster, but it introduces a problem: assembling the Hessian matrix. Fortunately, each component of the incremental potential is mostly a function of (k \\cdot n DOFs), where n is the dimensionality (I implemented 2D), and k is a small number (at most a few dozen). Thus, for each small IP contributing to the larger IP, the Hessian has only tens to hundreds of entries, which can be stored sparsely and assembled into the full Hessian. Following this tutorial, I implemented springs with vertices pinned to a wall.\nChoosing libraries: Used macroquad for GUI. Used nalgebra_glm for small-scale linear algebra. Initially planned to use nalgebra for large-scale linear algebra, but its sparse matrix functionality seemed incomplete, so I switched to faer. Initially used argmin for optimization. A Small Detour Before Contact IP Rust takes forever to compile, so configuring geometry shapes shouldn\u0026rsquo;t be hardcoded.\nAt first, I invented a weird file format and wrote a config based on my own logic:\n!k 1000.0 !node 0.0 0.0 0.2 0.0 0.4 0.0 0.6 0.0 0.1 0.2 0.3 0.2 Then I asked an AI to write a parser for me.\nLater, I realized that existing formats like JSON or TOML already have parsers, but by then, I was too lazy to change it.\nContact IP In short, Contact IP:\nRequires that point-edge pairs (aka primitive pairs) from two different bodies, which are close enough (within a threshold \\hat{d}), are assigned energy based on their distance. But to prevent interpenetration, there are additional requirements:\nOptimization courses teach that (damped) Newton\u0026rsquo;s method iteratively approaches the optimum. Each iteration involves a line search, and to prevent interpenetration, every intermediate step of the line search must ensure no primitive pairs penetrate, ultimately guaranteeing no interpenetration in the final result. Procedure At each line search step in Newton\u0026rsquo;s method:\nTraverse all primitive pairs (or use some acceleration structure — I didn\u0026rsquo;t implement this) and identify those with distances below the threshold. Compute the energy, gradient, and Hessian of the Contact IP for each primitive pair\u0026rsquo;s DOFs, then solve d = -A^{-1}g to get the search direction. Perform a CCD (Continuous Collision Detection) operation to ensure the line search doesn\u0026rsquo;t cause interpenetration (by setting a maximum step length). Use the Armijo condition for the line search. Repeat until sufficiently close to the minimum, at which point optimization is complete.\nImplementation Every step involved endless debugging…\nGradient \u0026amp; Hessian:\nIn 2D, each primitive pair\u0026rsquo;s DOFs are (2 DOFs per point) × (3 points) = 6 DOFs. The gradient of energy E w.r.t. DOFs can still be computed manually (a 6D vector). But the Hessian is a 6×6 matrix, and the paper\u0026rsquo;s notation is a mess—sometimes dyadic product, sometimes Kronecker product, with no clear labeling in the text. Manual computation failed. So, I used SymPy for symbolic computation and generated code from it. The differentiation code can be found in the symbolic/ folder. SymPy actually has Rust codegen, but it\u0026rsquo;s half-baked — often producing invalid Rust syntax, requiring string replacements, and only supporting single expressions (no vectors/matrices). Note: Later, I built my own SymPy→Rust code generator:\nSymars: Generate Rust code from SymPy expressions\nRemember: Point-to-segment distance requires case-by-case handling. CCD (ACCD) needs to be integrated into the optimization process, so argmin wasn\u0026rsquo;t suitable anymore. I discarded it and handwrote a damped Newton solver with ACCD and Armijo condition. After days of coding and debugging, the demo finally worked:\nThe constraints here are springs. ABD TL;DR, ABD Replaces traditional 6-DOF (translation + rotation) rigid bodies with 12-DOF bodies and heavily penalizes transformation matrices that deviate too far from rotation matrices, resulting in a (near-)rigid body simulation algorithm.\nIn 2D, an affine body (AB) has 6 DOFs: x = A x_0 + b, where the shape is defined by A (2×2) and b (2×1), assembled into a DOF vector: q = [flatten(A), b^T].\nWe know rotation matrices R satisfy R^T R = I. ABD uses an orthogonal potential energy \\kappa \\cdot \\text{frobnorm}(A^T A - I) to penalize A and keep it close to a rotation matrix.\nImplementation Any energy term requires second derivatives. Again, I used SymPy for differentiation. The project has thousands of lines of numerical computation code — don\u0026rsquo;t look at them. Affine bodies also need contact handling: Unlike mass-spring systems where each vertex is a DOF, an AB\u0026rsquo;s vertex position p is a function of DOFs, and the Contact IP is a function of p. A primitive pair involves two bodies, where one contributes an edge (two points p_1, p_2). Thus, derivatives must be taken w.r.t. both q s. The computational graph looks like this: After more endless debugging and parameter tuning (mainly \\kappa), the simulation finally ran:\nFinal Thoughts The resulting code is a bona fide spaghetti monster.\nEven though I spent a long time thinking about unifying interfaces before coding, the final design is neither OOP nor Rust-like, with inconsistent parameter passing everywhere.\nI can\u0026rsquo;t help but wonder: Is my ability just too low, or is code complexity truly not something design alone can solve?\nThe bright side:\nCargo is amazing — adding a dependency takes three seconds. Compared to Cmake, xmake or whatever-make, it\u0026rsquo;s night and day. No memory issues (since I didn\u0026rsquo;t and did not need to write unsafe code), so most effort went into logic. ","date":"2 October, 2025","id":15,"permalink":"/english-post/try-impl-ipc/","summary":"Intro: A taste of the Rust programming language","tags":"graphics graphics rust","title":"Try To Implement IPC"},{"content":"Intro: Troubleshooting Memory and Speed Performance\nDisclaimer: I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.\n1. Background and Motivation SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:\nFormat conversion (pickle → compressed JSON) triggered memory peaks around 30 GB. Data loading of the compressed JSON into Rust structures caused another ~30 GB spike. Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.\n2. Profile-Guided Optimization PGO requires empirical profiling to identify the true hotspots. I began with memory profiling using the memory-stats crate for lightweight inspection during early optimization stages. Then, I decomposed the data-loading pipeline into discrete steps:\nReading the compressed file (heavy disk I/O) Extracting the JSON string from the compressed stream Deserializing the JSON into native Rust data structures Populating an in-memory SQLite database for ad-hoc SQL queries Building the triangle mesh on CPU Initializing the rendering window (CPU-GPU transfer) Profiling revealed two major memory culprits: excessive cloning and multiple intermediate data structures. Below, I outline the optimizations.\nEliminating Redundant Clones During rapid prototyping, calls to .clone() are convenient. But they are expensive. Profiling showed that cloning large vectors contributed significantly to the memory peak and CPU time.\nFirst attempt: switch from cloned Vec\u0026lt;T\u0026gt; to borrowed \u0026amp;[T] slices. This failed due to lifetime constraints. Final solution: use Arc\u0026lt;[T]\u0026gt;. Although I\u0026rsquo;m not leveraging multithreading, Arc satisfies PyO3\u0026rsquo;s requirements, while no significant overhead is observed in this context. This change alone reduced memory usage and improved throughput noticeably.\nEarly Deallocation of Intermediate Structures Constructing the triangle mesh involved several temporary representations:\nRaw allocation buffers A list of triangles (vertices + face indices) A CPU-side mesh structure GPU upload buffers Each stage held onto its predecessor until the end of scope, inflating peak usage. To free these intermediates promptly, the following is implemented:\nScoped blocks to limit lifetimes Explicitly invoked drop() on unneeded buffers After these adjustments, peak memory dropped by roughly one-third.\n3. Sharding JSON Deserialization Deserializing the call-stack JSON with over 50 000 entries spiked memory usage dramatically. To mitigate this:\nShard the JSON data into chunks of at most 50 000 entries. Deserialize each chunk independently. Concatenate the resulting vectors. This streaming approach kept per-shard memory small, eliminating the previous giant allocation.\nIt is worth noting that serde_json::StreamDeserializer can be another option worth trying.\n4. Redesigning the Snapshot Format Even after the above optimizations, the call-stack data remained the largest in-memory component — duplicated once in Rust and again in the in-memory SQLite database.\nTo remove redundancy, I rethought what each representation serves:\nRust structures: display call stacks on screen upon user click. SQLite DB: serve ad-hoc SQL queries. Since SnapViewer is single-threaded and can tolerate occasional disk I/O, I split the snapshot into two files:\nallocations.json: lightweight JSON with allocation timestamps and sizes. elements.db: SQLite database holding call-stack text (indexed by allocation index). These two files are zipped together. At runtime:\nUnzip the snapshot. Load allocations.json into memory (small footprint). Open elements.db on disk. On click, query elements.db with WHERE idx = \u0026lt;allocation_index\u0026gt;. SQLite\u0026rsquo;s efficient on-disk indices make these lookups fast, with no perceptible impact on frame rate.\nRefactoring the Conversion Script I updated the snapshot-conversion script as follows:\nParse the original snapshot format. Bulk-insert call stacks into an in-memory SQLite database, then dump the DB as a byte stream. Serialize allocation metadata to JSON. Zip the JSON and DB byte stream. While conversion takes slightly longer, the resulting snapshot loads faster and uses a fraction of the memory.\n5. Results and Lessons After these optimizations, SnapViewer:\nNo longer spikes to 60+ GB of RAM on large snapshots, since we do not load the entire call stack information into memory at all. Starts up much faster. Maintains smooth rendering, even with on-demand call-stack queries. What I learned:\nDo not always load everything into memory. When you overflow your memory, the performance of virtual memory swapping system is probably worse than you think. When you need some mechanism to store most data on disk, but intelligentlly cache some of then in memory, SQLite should be a good start. It has its well-designed and industry-proven algorithm built into it. ","date":"2 October, 2025","id":16,"permalink":"/english-post/snapviewer-3-optim/","summary":"Intro: Troubleshooting Memory and Speed Performance","tags":"torch deep-learning rust","title":"SnapViewer Devlog #3: Optimizations"},{"content":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application\nBuilding a UI can often be the trickiest part of a development project, especially when you\u0026rsquo;re trying to integrate different languages and paradigms.\nFor SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.\nProject page: https://github.com/Da1sypetals/SnapViewer\nThe Initial Vision: An Integrated UI My core requirements for the UI were:\nInteractive Display: When an allocation is clicked in the viewer, its size, call stack, and other relevant information should be immediately displayed. SQL REPL: A command-line interface to execute SQL queries directly against the underlying database. Non-Blocking Operations: Both functionalities needed to operate without blocking each other. Early Attempts and Roadblocks Web: Rust to WASM My first thought was a web interface. Rust\u0026rsquo;s ability to compile to WASM and the three-d crate\u0026rsquo;s WebGPU support seemed promising. However, I quickly hit a wall with library versioning issues trying to compile even a simple Rust program to WASM. Rather than get bogged down, I decided to pivot.\nTUI: Terminal User Interface The natural next step was a Terminal User Interface (TUI). This approach avoids the complexities of cross-compilation and platform-specific GUI libraries.\nRatatui: A Promising Rust TUI Framework I started with Ratatui, a TUI framework for Rust. I got their demos running, but my plan to find an open-source example matching my \u0026ldquo;left-console, right-REPL\u0026rdquo; layout failed.\nDiving deep into the Ratatui documentation felt as complex as learning a new frontend framework like React, which defeated my goal of getting work done quickly. I abandoned this path.\nTextual \u0026amp; AI-Powered Development Given my goal of getting work done rather than becoming a TUI expert, I started thinking about AI. Rust isn\u0026rsquo;t particularly \u0026ldquo;AI-friendly\u0026rdquo; for code generation, but Python certainly is. This sparked an idea: What if I used AI to generate the TUI code in Python and then integrated my Rust application?\nI fed my requirements to several LLMs: Claude, Gemini, Deepseek, ChatGPT, and Grok. Claude\u0026rsquo;s initial results were impressive, while the others were largely unusable. After a few rounds of refinement with Claude, I had a working TUI demo:\nCombining Rust and Python: A Hybrid Approach Integrating Rust and Python is a standard process, but it has its quirks. I used PyO3 as a dependency to expose my Rust structures and bind Rust functions to Python.\nMy core Rust logic consists of:\nViewer: An infinite loop handling render draw calls and an event loop until the application shuts down. SQL REPL: Loads snapshot data into SQLite and executes SQL queries. Each of these operations is designed to be completed in milliseconds.\nDesigning App Structure My initial application structure idea was:\nMain Thread: Renders the TUI and accepts REPL inputs, calling SQL REPL Rust functions. Spawned Thread: Runs the infinite loop for the Snapshot Viewer. However, the three-d crate, which uses winit for window management, dictates that the window must run on the main thread. This immediately threw a wrench in my plans.\nAttempt 1: Multiprocessing My first revised design used multiprocessing:\nStart the application and load snapshot data. Spawn a new process to run the TUI application. Run the Viewer in the parent process. This setup allowed the child process to run the viewer window without blocking the TUI app. The challenge, however, was Inter-Process Communication (IPC). I needed a way for the viewer to send information (like selected allocation details) back to the TUI.\nI experimented with Python\u0026rsquo;s multiprocessing.Queue. My approach was to define a callback in Rust that put messages into the queue, and then have the parent process check the queue at a fixed interval (e.g., 0.1 seconds) to update the TUI\u0026rsquo;s logging panel.\nI encountered an implementation bug where the parent process wasn\u0026rsquo;t consuming all messages, causing the viewer and TUI to become out of sync. I then switched to a shared byte array with a lock for IPC. The child process would acquire the lock, write to the buffer, and release it. The parent process would try to acquire the lock at intervals to read the message and update the TUI.\nAttempt 2: Threading The multiprocessing solution had a couple of issues:\nThe TUI sometimes froze when typing in the REPL, likely due to lock contention. Balancing the log message update interval with the viewer\u0026rsquo;s framerate was tricky. Too frequent, and the UI lagged; too slow, and the viewer became unresponsive. I realized I could use multithreading instead! While winit requires the viewer window to run on the main thread, the TUI application does not. This led to a new, more elegant structure:\nSpawn a thread and start the TUI application on that thread. Start the viewer on the main thread. A naive implementation, however, caused the entire TUI to freeze. The culprit? The Global Interpreter Lock (GIL) in Python. The GIL ensures that only one thread can execute Python bytecode at a time.\nTime for some PyO3 details. By default, the extension function holds GIL during its execution; but when you don\u0026rsquo;t need to use Python objects during this call, a call to py::allow_thread can opt out this behavior, releasing the GIL.\nIn my case, the Rust extension holds GIL in the infinte render loop, preventing the TUI thread from updating the UI. By explicitly releasing the GIL during the viewer\u0026rsquo;s render loop, the TUI, running in its own sub-thread, was free to update, and the application could run as expected.\nAn Alternative: GUI with PyQt As an interesting side experiment, I wondered about a GUI instead of a TUI. I tasked Claude with translating my TUI code into a GUI application using PyQt. Claude did this in minutes, without errors.\nAfter a few minor styling tweaks (also done via chatting with Claude), here is what the app looks like:\n(I finally switched to Tkinter for compatibility issues with multithreading across platforms.)\nWrapping Up This journey highlights the flexibility and power of combining Rust\u0026rsquo;s performance with Python\u0026rsquo;s rapid development capabilities, especially when augmented by AI.\nUnderstanding the intricacies of thread management and inter-process communication helped a lot in this journey.\nHope you find this post is fun and informative to read! ❤️❤️❤️\n","date":"2 October, 2025","id":17,"permalink":"/english-post/snapviewer-2-ui/","summary":"Intro: Building the UI as a Hybrid Rust \u0026amp; Python Application","tags":"torch deep-learning rust","title":"Snapviewer Devlog #2: UI"},{"content":"Intro: PyTorch is a Deep Learning Operating System.\nCheck tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.\nAPI:\ntensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.\nContiguity check Modern LibTorch recommends using Packed tensor accessor (roughly the same memory cost as a pointer) to access elements in tensor.\nHowever, if you are to plug some others\u0026rsquo; implementation (likely using raw pointers like float*) into PyTorch, you are not likely to understand the code inside out and rewrite it.\nUsually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.\nAPI: tensor.is_contiguous()\nCheatsheet A quick utility that checks whether all tensors are on the same CUDA device:\nvoid CheckInputTensors(const std::vector\u0026lt;torch::Tensor\u0026gt; \u0026amp;tensors) { TORCH_CHECK(!tensors.empty(), \u0026#34;No tensors provided for device check\u0026#34;); auto first_device = tensors[0].device(); TORCH_CHECK(first_device.is_cuda(), \u0026#34;First tensor is not on CUDA\u0026#34;); int idx = 0; for (const auto \u0026amp;tensor: tensors) { TORCH_CHECK(tensor.device() == first_device, \u0026#34;All tensors must be on the same CUDA device, \u0026#34; \u0026#34;but found tensor at index [\u0026#34;, idx, \u0026#34;] on device \u0026#34;, tensor.device(), \u0026#34; while expecting \u0026#34;, first_device); TORCH_CHECK(tensor.is_contiguous(), \u0026#34;All tensors must be contiguous, but found tensor at index [\u0026#34;, idx, \u0026#34;] not contiguous\u0026#34;); idx += 1; } } CUDA stream Remember to always get the current CUDA stream via at::cuda::getCurrentCUDAStream() and pass it as the 4-th parameter in the \u0026lt;\u0026lt;\u0026lt;gridDim, blockDim, sharedMemorySizeBytes, stream\u0026gt;\u0026gt;\u0026gt; kernel call.\nThis is especially important when your operator is used in distributed training, where at::cuda::getCurrentCUDAStream() automatically selects the correct stream for you.\nCUDA toolkit version problem Most \u0026ldquo;symbol not found\u0026rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:\nPyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let\u0026rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas. If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system PATH, and libraries like CUBLAS or CUSPARSE in LD_LIBRARY_PATH. Let\u0026rsquo;s call this CUDA toolkit version VE, cuda Version of Extension. When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match). When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen. Memory Management in PyTorch Allocation When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be cudaMalloc and cudaFree. However, these force synchronization between CPU and GPU, which can starve the GPU.\nHere\u0026rsquo;s the key: PyTorch isn\u0026rsquo;t just an autograd tool. It\u0026rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.\nUsing the PyTorch allocator is straightforward. Follow these steps:\nSet dtype to torch::kInt8 and create a buffer tensor via torch::empty Get the pointer with buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;() This gives you a pointer to the buffer. Here\u0026rsquo;s a complete code snippet:\nauto buffer_options = torch::TensorOptions().device(your_device).dtype(torch::kInt8); auto buffer_tensor = torch::empty({buffer_size}, buffer_options); void *buffer_ptr = buffer_tensor.data_ptr\u0026lt;int8_t\u0026gt;(); Remember do not call cudaFree on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.\nPyTorch\u0026rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.\nI recommend reading this post (Chinese) for a deeper dive into how PyTorch manages memory.\nUsing CUBLAS, CUSPARSE, CUSolverDn, etc. We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.\nHandles When writing pure CUDA/C++ code, you manually call cusparseCreate to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.\nHowever this is not best practice in PyTorch CUDA extensions. There are good reasons: cusparseCreate introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.\nIf you use VizTracer to trace your program and visualize it in perfetto, you may notice cudaGetDeviceProperties call taking too much time on CPU side. This can be directly caused by cusparseCreate. LibTorch has API that automatically manages a pool of CUSPARSE handles:\nInclude the header that brings in CUDA context manager for LibTorch: #include \u0026lt;ATen/cuda/CUDAContext.h\u0026gt; Then, get handle via auto handle = at::cuda::getCurrentCUDASparseHandle(); automatically create a handle if there is not any, and caches it for subsequent uses. Use your handle as usual. I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch ATen. Searching in the repo with keyword getcurrentcuda can get you there quickly.\nBuffers Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since cudaMalloc and cudaFree are quite slow, which may cause your GPU to starve (verify this with VizTracer).\nA better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through torch.empty().\nBatched Matrix Multiplication Refer to this example to see how to perform batched matrix multiplication in CUSPARSE.\nTricks:\nTo broadcast, set stride to 0. It is possible to broadcast rowptr but not colind and values. Check documentation for details.\nTensor Options struct TensorOptions carries many information about the tensor:\nstruct C10_API TensorOptions { // ... omitted // members Device device_ = at::kCPU; // 16-bit caffe2::TypeMeta dtype_ = caffe2::TypeMeta::Make\u0026lt;float\u0026gt;(); // 16-bit Layout layout_ = at::kStrided; // 8-bit MemoryFormat memory_format_ = MemoryFormat::Contiguous; // 8-bit bool requires_grad_ : 1; bool pinned_memory_ : 1; // Existense of members bool has_device_ : 1; bool has_dtype_ : 1; bool has_layout_ : 1; bool has_requires_grad_ : 1; bool has_pinned_memory_ : 1; bool has_memory_format_ : 1; } The most important methods are:\n[[nodiscard]] TensorOptions device(Device device) const; [[nodiscard]] TensorOptions dtype(ScalarType dtype) const; [[nodiscard]] TensorOptions requires_grad(bool) const; Usage:\ntensor.options() returns an instance of TensorOptions that describes the tensor. opt.dtype(torch::kFloat64) has other properties remain the same as opt, only dtype changes to float64 or in C++, double. The .to(...) method of a tensor can take a TensorOptions instance as its only argument. For an exhaustive list of device and dtype, you may want to refer to:\nhttps://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h Debug layer by layer A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:\nCUDA kernel C++ wrapper data passed from Python (PyTorch) to C++ Python wrapper CUDA kernel Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.\nC++ wrapper The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a d_ prefix in variable names, or wrap it with thrust::device_ptr.\nprintf, std::cout or gdb will assist you in the journey.\ndata passed from Python (PyTorch) to C++ Refer to Pybind11 docs and try to answer these questions:\nHow various Python types are represented in Pybind11 API; How to properly configure the function prototype in Pybind11? Python Wrapper Ask LLMs. LLMs know python much better than I do.\nWhat to Reference To my knowledge, the PyTorch C++ documentation is very old. Many things in the source code are not documented there.\nIt is a better choice to just search in the PyTorch github repo, and read the comments and source code.\n","date":"2 October, 2025","id":18,"permalink":"/english-post/torch-cuda-ext/","summary":"Intro: PyTorch is a Deep Learning Operating System.","tags":"deep-learning cuda torch","title":"Notes on Writing PyTorch CUDA Extensions"},{"content":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。\n问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.\n此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。\nNotation n: 图节点数，规模为 1k~1M nnz: 图边数（稀疏矩阵非零元素数，Num NonZero） 规模为10n~1000n q, k, v: (n, d) A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。\n实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是$n^2$的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair取出来得到(nnz,d)，然后再做reduce和scatter, 和V相乘。 Reformulate 我们引入三个算子:\nSDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)): if M[i, j] != 0: out[i, j] = dot(A[i,:], B[:,j]) else: out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:\nout = SpMM(Softmax(SDDMM(Q, K_T, A)), V) 以此我们引出下面的实现\n实现：DGL Graph Transformer in a Nutshell — DGL 2.2.1 documentation\n对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：\nattn = dglsp.bsddmm(A, q, k.transpose(1, 0)) # (sparse) [N, N, nh] # Sparse softmax by default applies on the last sparse dimension. attn = attn.softmax() # (sparse) [N, N, nh] out = dglsp.bspmm(attn, v) # [N, dh, nh] 算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点\n进行的是最直观的并行，没有进行充分的优化 各个kernel分开执行，没有融合 没有利用tensor core 实现：FlashSparse https://github.com/ParCIS/FlashSparse/tree/main/eva\n主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core\n基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。\n矩阵格式：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。 矩阵格式转换时间开销 (CSR -\u0026gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。 FlashSparse的SpMM算法（C = A × B） 阶段1：转置访问与加载\n块形状：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。 稀疏块 A 加载：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。 稠密块 B 加载：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。 合并访问：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。 阶段2：交换与转置计算\n在张量核心上执行MMA指令：Bᵀ × Aᵀ。\nBᵀ 作为左操作数（m=16, k=8）。 Aᵀ 作为右操作数（k=8, n=8）。 结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。 阶段3：转置输出\n寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。\nFlashSparse的SDDMM算法（C = M ⊙ (AB)） 块形状：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。 转置计算的数据对齐：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。 转置计算：\n稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。 稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。 计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。 用M矩阵进行element-wise product，从C_dense 得到C_sparse 实测: 未测试\n实现：DF-GNN https://github.com/paoxiaode/DF-GNN\n主题：block/warp调度和算子融合\n由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。\n使用的矩阵格式是CSR，不需要做额外的格式转换\n算法流程 Launch Kernel on Grid: (n × h) ↓ Each Block → (rid, hid): one node, one head ↓ Load Q[rid, hid, :] → s_Q[f] (shm) ↓ For each tile of neighbors (size ≤ 32): - Load neighbor IDs from indices[] - Compute Q · K^T (dot product using s_Q and K[dst]) - Reduce in warp → store in neigh_nodes_weight[eid] - Find max(weight) in current tile → weightMax - Adjust partial_sum and acc with exp(old_max - new_max) - Compute exp(weight - weightMax) and accumulate acc += exp_w * V[] - Accumulate partial_sum += exp_w - Update weightMax_old ↓ Final normalization: out_feat = acc / partial_sum ↓ Write back to global memory 主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。\n实测: 代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等\n修复后：\n在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x\n精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计\nF3S https://github.com/HPCForge/Fused3S/tree/main/scripts 主题：算子融合+混合精度+利用tensor core\n其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。\n仅有forward的实现 F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。 优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点 和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点 空间开销：O(n+nnz)，但是常数会更大一些 矩阵格式转换时间开销 (CSR -\u0026gt; BSB)：一次性开销，暂时忽略。 算法流程： 划分行块：\n将 Q 按行划分为 $T_r = \\lceil N / r \\rceil$ 个块 $\\{Q_1, ..., Q_{T_r}\\}$，每个大小为 $r \\times d$。 将输出 O 同样划分为 $T_r$ 个块 $\\{O_1, ..., O_{T_r}\\}$，每个大小为 $r \\times d$。 对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：\n初始化\n$m_o \\leftarrow -\\infty \\in \\mathbb{R}^r$（行最大值） $l_o \\leftarrow 0 \\in \\mathbb{R}^r$（行 softmax 累加和） $O_i \\leftarrow 0 \\in \\mathbb{R}^{r \\times d}$（输出块，fp32） 加载数据：\n将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。 计算当前行窗口（RW）包含的 TCB 数量：$t = \\text{tro}[i+1] - \\text{tro}[i]$。 通过 sptd 获取当前 RW 对应的原始列索引向量 $c$。 从 $K$ 和 $V$ 中按索引 $c$ gather 出对应的行，得到 $\\hat{K}, \\hat{V} \\in \\mathbb{R}^{t \\cdot c \\times d}$。 划分 warp 块：\n将 $\\hat{K}$ 划分为 $T_c = \\lceil t / W \\rceil$ 个块 $\\{\\hat{K}_1, ..., \\hat{K}_{T_c}\\}$，每个大小为 $Wc \\times d$。 将 $\\hat{V}$ 同样划分为 $T_c$ 个块 $\\{\\hat{V}_1, ..., \\hat{V}_{T_c}\\}$，每个大小为 $Wc \\times d$。 对每个 warp 块索引 $j = 1$ 到 $T_c$：\nSDDMM：\n调用 $\\text{TBGemm}(Q_i, \\hat{K}_j^T, 0)$，计算中间得分块 $S_i \\in \\mathbb{R}^{r \\times c}$（fp32）。 用 BSB 中对应 TCB 的 bitmap 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。 Online Softmax：\n计算当前块行最大值：$m_i = \\max(m_o, \\text{rowmax}(S_i))$。 计算指数：$E_i = \\exp(S_i - m_i)$。 更新累加和：$l_o = \\text{diag}(\\exp(m_o - m_i)) \\cdot l_o + \\text{rowsum}(E_i)$。 将 $E_i$ 转为 fp16，存入 SMEM。 SpMM：\n对已有输出缩放：$O_i = \\text{diag}(\\exp(m_o - m_i)) \\cdot O_i$。 调用 $\\text{TBGemm}(E_i, \\hat{V}_j, O_i)$，将结果累加回 $O_i$。 更新行最大值：$m_o = m_i$。 最终归一化并写回：\n对输出块归一化：$O_i = \\text{diag}(l_o)^{-1} \\cdot O_i$。 将 $O_i$ 写回全局内存（HBM）。 Subroutine: TBGemm 输入:\n矩阵块 $A \\in \\mathbb{R}^{m \\times K}$ (位于 SMEM，共享内存) 矩阵块 $B \\in \\mathbb{R}^{K \\times P}$ (位于 HBM，全局内存) 累加项 $D \\in \\mathbb{R}^{m \\times P}$ (位于 SMEM，共享内存) 输出:\n结果矩阵 $C = A B + D \\in \\mathbb{R}^{m \\times P}$ 流程:\n切分块 (Tiling): 将输入矩阵 $A$, $B$, $D$ 按照 Tensor Core 的硬件 Tile 尺寸（例如 $16 \\times 8 \\times 16$）切分为对应的子块。\n并行迭代 (Output Tiles): 对结果矩阵 $C$ 的每个输出 Tile (通常由一个 Warp 或一个 Thread Block 计算):\n加载累加项 D: 从 SMEM 中加载 $D$ 对应的子块到线程的寄存器中，作为初始累加值 $C$. 内积迭代 (K-Tiles): 对 $K$ 维度的每个 $k$-tile 进行迭代累加:\n加载 A: 从 SMEM 中加载矩阵 $A$ 对应的 $A_{\\text{tile}}$ 子块。 加载 B: 从 HBM 中直接加载矩阵 $B$ 对应的 $B_{\\text{tile}}$ 子块。 执行 MMA 指令: 调用硬件支持的 PTX mma 指令（Matrix Multiply-Accumulate），执行计算并累加： $$C \\leftarrow A_{\\text{tile}} \\cdot B_{\\text{tile}} + C$$ 返回: 最终得到结果 $C$。\n实测: 代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。\n速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）\n限制：n % 16 == 0，因为需要分割成8x16的block\n精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。\n","date":"2 October, 2025","id":19,"permalink":"/posts/gnn-optim/","summary":"注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。","tags":"deep-learning cuda","title":"近期GNN Attention算子优化工作速览"},{"content":"Background When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.\nAt this point, you might come across this documentation, which teaches you how to record a memory snapshot and visualize it on this website.\nHowever, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).\nI looked into the website’s JavaScript code, and here’s what it primarily does:\nManually loads Python pickle files; Re-parses the raw data into graphical representations time the viewport changes, then renders it to the screen. This parsing logic is written in JavaScript. You can imagine the performance when it is executed each frame, operating on hundred-MB data.\nInspiration My current work includes optimizing a deep learning model whose optimization is under-explored compared to LLM. I encountered this issue while working with a snapshot of a model with several billion parameters.\nWhy not just use existing LLM infrastructure instead of optimizing manually? Long story short, this model was custom-designed by a researcher and contains many modules completely different from standard LLMs. It seems like nowadays, everyone assumes deep learning is all about LLMs — so much so that even some tech leads believe LLM infrastructure can be easily adapted to other models… but I digress. I originally wrote a simple script to parse the snapshot’s contents, hoping to identify memory allocation issues in the model. But after working with this model for a month, I finally had enough. That’s how this project — SnapViewer — came to be.\nTL;DR​​: The graphical data from the memory snapshot is parsed and represented as a massive triangle mesh, leveraging existing rendering libraries to handle mesh rendering efficiently.\nHere’s a snapshot of over 100 MB running smoothly on my integrated GPU:\nImplementation The reference implementation The snapshot format is partially documented in the record_memory_history function\u0026rsquo;s docstring. However, this documentation is incomplete — likely because later updates weren’t reflected in the docstring.\nThe actual parsing of the snapshot into a dictionary happens here.\nThis script converts the allocator trace into a memory timeline, which is then passed to the web viewer’s JS code. The JS code further transforms this into polygons (representing allocations) for visualization. Each polygon corresponds to an allocation, storing details like size and callstack. Implementation: Snapshot (De)serialize Initial implementation This part is impelmented in Python since I need to deal with Python-native data structures. I simply convert the dict to a json file.\nOptimizations Raw JSON is too large on disk → compress it in-memory (Python zipfile) before writing. During visualization, read the ZIP from disk (Rust zip crate) and decompress in-memory. Tradeoffs This approach causes a temporary memory spike during JSON parsing but avoids persistent high memory usage. Also leverages Rust’s serde-json (since Rust’s serde-pickle is incomplete and can’t handle recursive structures). Implementation: Rendering \u0026amp; Interaction​​ This part is implemented in Rust.\nRendering Since allocation data remains static during visualization, all allocations are combined into a single large mesh and sent to the GPU once.\n​Library Used​​: three-d\nProvides good mesh abstraction. Supports one-time GPU upload (no per-frame CPU→GPU transfers). Handles mouse/keyboard events. ​World-to-Window Coordinate Conversion​​ ​Step 1​​: Convert window coordinates to world coordinates (scale + window center offset). ​​Step 2​​: Convert world coordinates to memory positions (predefined scaling). UI \u0026amp; Interaction Features​ Memory Scale Markers​​ Dynamically adjust the number and precision of markers based on screen visibility. Keep markers at consistent screen positions while moving/zooming. Pan \u0026amp; Zoom​​ Track the original scale (1/zoom). Update to the new zoom level and compute the ratio between old and new scales. Adjust the screen center position based on the mouse’s invariant world position. Implementation: Query After using this tool at work for around a week, I find myself frequently needing to search in the memory snapshot, especially:\nFind all allocations which is alive at a specific timestamp Find all allocations whose call stack has a specific substring Preferablly the allocations should be sorted by allocation size in descending order My first thought was to build a simple REPL and a simple command parser, and map each command to a specific query function.\nHowever, after having listed out all the functionalities I want, I found it to be a subset of database query, especially SQL.\nSo I decided not to reinvent wheels: I just connect to a in-memory SQLite database. Interfacing user is simple: read user input, let SQLite execute it and format the output to human-readable format.\nIf you’ve struggled with PyTorch memory snapshots, check it out! Contributions \u0026amp; feedback welcome. ⭐\n","date":"1 October, 2025","id":20,"permalink":"/english-post/snapviewer/","summary":"When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.","tags":"torch deep-learning rust","title":"SnapViewer: Faster PyTorch Memory Allocation Viewer"},{"content":"内存与速度性能问题排查 免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。\n1. 背景与动机 SnapViewer 能够高效处理大型内存快照——例如，支持高达 500 MB 的压缩快照。然而，在处理 1.3 GB的snapshot的时，我发现了严重的内存和速度瓶颈：\n格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。 将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。 频繁的 page fault 和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。\n2. Profile-Guided Optimization（PGO） PGO 需要通过实证分析来识别真正的热点。我首先使用 memory-stats crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：\n读取压缩文件（重度磁盘 I/O） 从压缩流中提取 JSON 字符串 将 JSON 反序列化为原生 Rust 数据结构 填充内存中的 SQLite 数据库以支持即席 SQL 查询 在 CPU 上构建三角网格（triangle mesh） 初始化渲染窗口（CPU-GPU 数据传输） 性能分析揭示了两个主要的内存问题：过度使用 clone 和多个中间数据结构。以下是我实施的优化措施。\n消除冗余的 Clone 在快速原型开发阶段，调用 .clone() 非常方便，但代价高昂。性能分析显示，克隆大型 Vec 显著加剧了内存峰值和 CPU 时间。\n首次尝试：将对 Vec\u0026lt;T\u0026gt; 的 clone() 改为借用的 \u0026amp;[T] 切片。但由于生命周期约束无法做到. 最终方案：改用 Arc\u0026lt;[T]\u0026gt;。尽管我并未使用多线程，但 Arc 满足了 PyO3 的要求，且在此场景中未观察到明显开销。 仅此一项改动就显著降低了内存使用, 降低了启动耗时。\n提前释放中间结构 构建三角网格涉及多个临时表示形式：\n原始分配缓冲区 三角形列表（顶点 + 面索引） CPU 端的网格结构 GPU 上传缓冲区 每个阶段都会保留其前驱数据直至作用域结束，从而推高了峰值内存占用。为及时释放这些中间数据，我们采取了以下措施：\n使用作用域块（scoped blocks）限制生命周期 对不再需要的缓冲区显式调用 drop() 经过这些调整，峰值内存大约减少了三分之一。\n3. 分片处理 JSON 反序列化 对包含超过 50,000 个条目的调用栈 JSON 进行反序列化时，内存使用急剧飙升。为缓解此问题：\n将 JSON 数据分片，每片最多包含 50,000 个条目。 独立反序列化每个分片。 合并所得到的Vec。 这种流式处理方法使每个分片的内存占用保持在较低水平，避免了之前的大规模单次分配。\n值得注意的是，serde_json::StreamDeserializer 是另一个值得尝试的选项。\n4. 重新设计快照格式 即使经过上述优化，调用栈数据仍然是内存中最大的组件——在 Rust 中和内存 SQLite 数据库中各存一份，造成重复。\n为消除冗余，我重新思考了每种表示形式的用途：\nRust 结构：用户点击时在屏幕上显示调用栈。 SQLite 数据库：支持即席 SQL 查询。 由于 SnapViewer 是单线程的，且可容忍偶尔的磁盘 I/O，我将快照拆分为两个文件：\nallocations.json：轻量级 JSON，包含分配时间戳和大小。 elements.db：SQLite 数据库，存储调用栈文本（按分配索引建立索引）。 这两个文件被一起压缩打包。运行时：\n解压快照。 将 allocations.json 加载到内存（占用很小）。 打开磁盘上的 elements.db。 用户点击时，通过 WHERE idx = \u0026lt;allocation_index\u0026gt; 查询 elements.db。 SQLite 高效的磁盘索引使这些查询非常迅速，对帧率几乎没有可感知的影响。\n重构转换脚本 我对快照转换脚本进行了如下更新：\n解析原始快照格式。 将调用栈批量插入内存 SQLite 数据库，然后将数据库转储为字节流。 将分配元数据序列化为 JSON。 将 JSON 与数据库字节流一起压缩。 虽然转换过程略慢，但生成的快照加载更快，且内存占用大幅降低。\n5. 成果与经验总结 经过这些优化，SnapViewer 实现了以下改进：\n不再因加载大型快照而触发 60+ GB 的内存峰值，因为我们完全不再将整个调用栈信息加载到内存中。 启动速度显著提升。 即使进行按需调用栈查询，渲染依然流畅。 我学到的经验：\n不要总是把所有数据都加载到内存中。当你耗尽物理内存时，虚拟内存交换系统的性能可能比你想象的还要差。 当你需要将大部分数据存储在磁盘上，同时智能地缓存部分数据到内存时，SQLite 是一个好的选择。它内置了经过工业验证的高效算法。 ","date":"7 June, 2025","id":21,"permalink":"/posts/snapviewer-3-zh/","summary":"免责声明：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。","tags":"torch deep-learning rust","title":"Snapviewer Devlog #3: 性能优化"},{"content":"背景 在使用 PyTorch 训练模型时，内存不足（OOM）错误是很常见的，因此需要对 GPU 内存进行优化。当简单的方法（如减少batch size）不再有效时，就需要分析模型本身的内存占用情况。\n此时，你可能会看到这份文档，它教你如何记录内存快照并在网站上进行可视化。\n但是这里存在一个严重的问题：这个网站性能比较差。\n如果你的模型较小，快照只有几 MB，性能还可以接受。 但是如果你的模型很大，快照达到几十甚至上百 MB，网站就会变得极慢，帧率可能低至每分钟 2-3 帧（非笔误）。 我研究了网站的 JavaScript 代码，其主要功能是：\n手动加载 Python 的 pickle 文件； 每次视口发生变化时重新解析原始数据为图形表示，然后将其渲染到屏幕上。 这些解析逻辑用 JavaScript 编写，你可以想象一下每帧执行这些操作，处理上百 MB 数据要多久.\n灵感 我当前的工作包括优化一个非LLM的深度学习模型。在处理数十亿参数的模型所导出的显存snapshot时，我遇到了这个问题。\n为什么不用现有的 LLM 基础设施而选择手动优化？简单地说，这个模型是研究人员自定义设计的，其中包含许多与标准 LLM 完全不同的模块。现在似乎每个人都认为深度学习就只是关于 LLM——以至于一些技术负责人也认为 LLM 的基础设施可以很容易地适配到其他模型上……不过我有点跑题了。\n最初，我写了一个简单的脚本来解析快照内容，希望能发现模型中的内存分配问题。但是在一个月的工作中，我发现我还是需要一个带有GUI的可视化器, 于是我开发了 SnapViewer.\n简而言之：内存快照的图形数据被解析并呈现为一个巨大的三角形mesh，利用现有的渲染库来高效处理网格渲染。\n下面是一个 100 MB 以上的快照在我的集成显卡上流畅运行的截图：\n实现 参考实现 快照格式在 record_memory_history 函数的docstring中有部分记录。但这份文档并不完整, 可能是后续commit的人懒得更新docstring了.\n实际将快照解析为字典的过程发生在这里：\n该脚本将分配器跟踪转换为内存时间线，然后传递给网页查看器的 JS 代码。 JS 代码进一步将其转换为多边形（表示分配），用于可视化。每个多边形对应一个分配，存储大小和调用栈等细节。 实现：快照 (反)序列化 初始实现 我用 Python 实现这一部分，因为我需要处理 Python 原生数据结构。我只是简单地将字典转换为 JSON 文件。\n优化 原始 JSON 文件太大 → 在写入前进行内存压缩（Python zipfile）。 在可视化过程中，从磁盘读取 ZIP 文件（Rust zip crate），并在内存中解压缩。 权衡 这种方式在 JSON 解析过程中会导致短暂的内存峰值，但避免了持续的高内存使用。 利用 Rust 的 serde-json（因为 Rust 的 serde-pickle 功能不全，不能处理递归结构）。 实现：渲染与交互 这部分用 Rust 实现。\n渲染 由于分配数据在可视化过程中是静态的，所有分配被合并成一个大的三角形mesh，并一次性发送到 GPU。\n使用的库：three-d\n提供良好的网格抽象。 支持一次性上传到 GPU（无需每帧进行 CPU→GPU 传输）。 处理鼠标/键盘事件。 窗口到世界坐标转换 步骤 1：将窗口坐标转换为世界坐标（缩放 + 窗口中心偏移）。 步骤 2：将世界坐标转换为内存位置（预定义的缩放）。 UI \u0026amp; 交互功能 内存刻度标记 根据屏幕可见性动态调整标记的数量和精度。 保持标记在屏幕上的固定位置，即使移动或缩放。 移动 \u0026amp; 缩放 跟踪原始缩放比例（1/zoom）。 更新到新的缩放级别，并计算新旧比例之间的比值。 根据鼠标不变的世界位置，调整屏幕中心位置。 实现：查询 在工作中使用这个工具一周后，我发现自己经常需要搜索内存快照，尤其是：\n找到特定时间戳内所有存活的分配 找到调用栈中包含特定子字符串的所有分配 最好按照分配大小降序排列分配 我最初的想法是构建一个简单的 REPL 和一个简单的命令解析器，将每个命令映射到特定的查询函数。\n然而，在列出所有需要的功能后，我发现这其实是数据库查询的子集，尤其是 SQL。\n因此我决定不再造轮子：我只是连接到一个内存中的 SQLite 数据库。用户交互非常简单：读取用户输入，让 SQLite 执行，并将输出格式化为人可读的格式。\n如果你在使用 PyTorch 内存快照时遇到过困难，来看看吧！欢迎贡献和反馈。 ⭐\n","date":"6 June, 2025","id":22,"permalink":"/posts/snapviewer-zh/","summary":"在使用 PyTorch 训练模型时，内存不足（OOM）错误是很常见的，因此需要对 GPU 内存进行优化。当简单的方法（如减少batch size）不再有效时，就需要分析模型本身的内存占用情况。","tags":"torch deep-learning rust","title":"SnapViewer: 更快的PyTorch显存分配可视化"},{"content":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。\n这是我实现 Lsmkv 的时候记录的备注.\n组件 内存部分 磁盘部分 WAL 总体 初始化 需要 init flush thread。flush thread 的工作流程:\n等待 flush 信号量被 notify,获取一个 compact 信号量资源 启动一个 sstwriter,写入这个 memtable 一个 memtable 对一个 sst 等到写入 sst 写完之后,才进行: 从 frozen memtables、frozen memtable sizes 里面删除这个 memtable 从 wal 里面删除这个 memtable 对应的 wal update manifest Try Freeze 如果当前大小 \u0026gt; freeze size 那么就 freeze;进一步如果所有 frozen memtable 大小之和 \u0026gt; flush threshold,那么就 set flush signal。\n写操作 写 memtable 写 WAL try freeze 内存部分 Put 添加到 memtable; 更新 size。 size 不需要特别精确,只需要是一个大致的值即可。 Delete 添加一个 tomb 标记到 memtable Get 从 active memtable 中获取 从 new 到 old 遍历所有的 inactive memtable,获取。 磁盘部分 compact 信号量 二元信号量。\n需要 compact 的时候,添加资源 compact thread 开始 compact 的时候,消耗资源。 初始化 如果 auto compact 开启,初始化的时候需要 init compact thread:\nLevel 存储这个 level 所有文件对应的文件路径,装在 sst reader 里面\nGet (没有 delete, put) 从低到高,从新到旧,调用 sst 的 get 方法,获取 record。否则返回 none。\nInit Compact Thread Compact thread:\n等待 compact 信号量 依次查看每一层:如果这一层大小超过 threshold,就合并到下一层,否则就提前返回。 Compact 以 L0 -\u0026gt; L1 为例: 从前到后遍历所有的 kv-pair,同时维护:\nkeys_outdated 同一个 key,timetsamp 小于 oldest marker 的 kv pair 只需要保留一个。 keys_outdated 记录所有(出现过的,且 timestamp 小于 oldest marker)的 key L1 sst size 每达到一定值就关闭当前 sst,新开一个新的 sst。 更新 manifest。 SST writer 配置 max block size。\n每个 block 的开头一个 key 会添加到 index 中; 搜索这个 sst 的时候,会先对 index 进行二分查找; 在 block 之内采用线性搜索。 fpr,用于构建 bloom filter.\n写入 遍历所有的 kv pair: userkey(不含 timestamp)添加到 bloom filter; block 写入当前 kv; 如果当前 block 大小超过 max block size,就开启一个新的 block,然后写入对应的 index(内存) 将 index 和 bloom filter 写磁盘。 SST reader 查找: Get(key, timestamp) 查 bloom filter,如果不存在就返回。 将 index 整个载入内存中,进行二分查找,得到对应 key-timestamp 所在的区间。如果 out of bounds 就返回。 按照查找到的区间,读磁盘。 MVCC key 排布问题 struct Key bytes timestamp: u64 比较: key1 \u0026lt; key2:\nkey1.bytes \u0026lt; key2.bytes (字典序); 或者: key1.bytes == key2.bytes,而且 key1.timestamp \u0026gt; key2.timestamp 为什么这样比较? 在进行查询 Get(userkey, timestamp) 的时候,我们需要的是:\nuserkey 匹配 timestamp 小于查询的 timestamp,且尽可能大 因此,我们将\nuserkey 升序排序 timestamp 降序排序 在搜索 memtable(skiplist)的时候,或者对 index 进行二分查找的时候,就可以:\n直接使用 lower_bound,查找大于等于自己的第一个元素 如果 userkey 匹配,说明是 timestamp 小于当前 timestamp 的,timestamp 最大的记录,返回; 如果 userkey 不匹配,说明不存在 timestamp 小于当前 timestamp 的记录,返回(未找到)。 Transaction 数据结构 一个内存 tempmap,用来存储 transaction 已经写,但是未提交的内容。 创建的时候,从 tree 获取:\nstart timestamp,作为查询的 timestamp transaction id 然后写入 transaction start 到 WAL\nPut,Delete 写 tempmap,写 WAL\nGet 使用 start timestamp,先查 tempmap,再查 tree。\nCommit 从 tree 获取一个 commit timestamp; 写 WAL,记录 transaction id 和 commit timestamp。 在 replay 的时候,把 transaction id 和 commit timestamp 对应起来就可以知道 transaction 里面的 写操作 对应的 timestamp 调用 tree.active_memtable 的 API,将 transaction 的所有数据写入 tree 的 memtable。 WAL 看到 transaction start,先将 transaction 暂存到内存中:\n如果在 replay 结束之前看到了 transaction end,就将改动写入 tree 中(redo)。 否则放弃,视为没完成的事务(undo) 踩坑: Resource deadlock avoided (os error 35),可能是一个 thread 持有了自己的 joinhandle 并且 join 了自己;使用 maybe join 解决,即判断当前线程和 joinhandle 的线程是否一致,如果一致就不用 join。 死锁问题: wal 和 mem 都有锁,必须 按照同一顺序获取 才不会出现死锁。 Bloom filter 细节 本部分由 Deepseek 辅助写作\n该 Bloom filter 算法的主要步骤如下:\n参数计算:\n根据预期元素数量 n 和可接受误判率 p,通过公式计算最优位数 m 和哈希函数数量 k: $ m=\\lceil-n \\dfrac{\\ln(p)}{\\ln(2) ^ 2}\\rceil $ $ k=\\lceil\\dfrac{m}{n}\\ln(2)\\rceil $ 当直接指定参数时,使用给定的位数和哈希函数数量 哈希生成:\n使用 64 位指纹哈希(farmhash)生成初始哈希值 h 通过位运算构造增量值 delta = (h \u0026gt;\u0026gt; 33) | (h \u0026lt;\u0026lt; 31) 采用双重哈希技术,通过循环叠加 delta 生成 k 个不同的位位置: $ h_i \\equiv h + i \\cdot delta \\pmod m , 0 \\leq i \\lt k $ 数据插入:\n对输入 key 进行哈希计算得到初始 h 和 delta 循环 k 次生成位位置,将位数组中对应位置设为 1 采用位操作: byte_index = position/8,bit_mask = 1 \u0026laquo; (position%8) 存在性检测:\n重复插入时的哈希计算过程 检查所有 k 个对应位是否均为 1 任一位置为 0 则判定不存在,全部为 1 时判定可能存在 数据持久化:\n序列化时附加 CRC32 校验和 反序列化时验证校验和与数据完整性 ","date":"1 March, 2025","id":23,"permalink":"/posts/lsm/","summary":"Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。","tags":"database rust","title":"Lsm Tree 实现备注"},{"content":"Symars Rust代码生成库和 Raddy 自动求导库的来龙去脉\n故事的起因： 前段时间读了一些物理模拟的论文，想尝试复现一下。下手点先选了 stable neo hookean flesh simulation，但是选了什么并不重要。重要的是，所谓“现代”的物理模拟很多是隐式模拟，需要用牛顿法解一个优化问题。\n这之中就涉及到了：对能量的本构模型求导数（一阶梯度，二阶 hessian 矩阵）。这之中还涉及到从 小而稠密 的 hessian 子矩阵组装成 大而稀疏 的完整 hessian。这是一个精细活，一不小心就会出现极其难以排查的 bug。\n从 Dynamic Deformables 这篇文章中可以看出推导这个公式就要花不少功夫（就算是看懂论文里的 notation 也要好一会儿），于是我搜了搜更多东西，尝试寻找一些其他的解决方法：我不是很想在精细的 debug 上花很多时间。最终找到的解决方法有两种：\n求符号导数，然后进行代码生成； 自动求导。 找到的资料中，前者有 MATLAB 或者 SymPy，后者有 PyTorch 等深度学习库，和更适合的 TinyAD。\n为什么说更适合？因为深度学习库的求导是以tensor为单位的，但是我这里的求导需要以单个标量为单位，粒度不同，深度学习库可能会跑出完全没法看的帧率。\n但是一个致命的问题来了：上述工具都在 C++ 的工具链上，而我不会 C++（或者，我可能会一点点 C++，但是我不会 CMake，因此不会调包。）\n我曾经花了三天尝试在项目里用上 Eigen，然后失败告终，还是技术水平太菜了。\n我只好换一门我比较熟悉的语言：Rust。这是一切罪恶的开始\u0026hellip;\n一条看起来简单的路 目前 Rust 还没有一个可以求二阶 hessian 的自动求导库（至少我在 crates.io 没搜到）。\nSymPy 目前还不能生成 Rust 代码（可以，但是有 bug）。\n考虑实现难度我先选了后者：从 SymPy 表达式生成 Rust 代码。于是有了 Symars。\nSymPy 提供的访问符号表达式的数据结构是树的形式，节点类型是运算符类型（Add, Mul, Div, Sin, 等等）或者常数/符号，节点的孩子是 operand 操作数。实现代码生成的思路就是按深度优先遍历树，得到孩子的表达式，然后再根据节点类型得到当前节点的表达式。边界条件是当前节点是常数，或者符号。\n实现完了之后，我拿着生成的导数去先写一个简单的隐式弹簧质点系统；但是还是在 hessian 组装上消耗了很多时间在排查 index 打错这种 bug 上。\n再去走没走过的路 为了解决上述问题，我打算尝试原来放弃的那条路：自动求导。方案是在 Rust 里面使用 TinyAD。\n一条路的两种走法 一开始想了两个方法：毕竟我不是很懂 C++，可能相比于看懂整个 TinyAD 的 codebase，做一套 FFI 更现实一些。\n但是我发现，项目 clone 下来之后，我甚至不会拉依赖不会编译。（什么赛博残废）\n然后我重新观察了 TinyAD 的 codebase，发现核心逻辑大概在 ~1000 行代码，似乎不是不可能在完全不运行这个项目的前提下把代码复刻一遍。说干就干，于是有了Raddy：\n正确的走路姿势 找到了正确的走路姿势，开始着手实现。说一些实现细节：\n每个求导链路上的标量值都带一个相对变量的梯度和 hessian，所以肉眼可见的 memory overhead 比较严重；一个提醒用户的方法是不实现 Copy trait，在需要一个副本的时候 explicit clone。 有大量需要实现 (\u0026amp;)Type 和 (\u0026amp;)Type 之间的 operator trait，组合有 2 * 2 = 4 种，这意味着相同的代码要写 4 次。于是考虑引进某些元编程的方法： 用宏 macro 批量实现； 用 Python 脚本进行代码生成。 考虑到宏会让 rust-analyzer （局部）罢工，但是我离开 LSP 简直活不了，于是选择了后者。具体代码见 meta/ 目录，其实没啥技术含量，就是字符串拼接。\n测试：我要如何验证我求出来的导数是对的？第一个想法就是用我前面写过的 symars，对每个测试表达式生成其符号 grad 和 hessian 的代码，然后和求导结果交叉验证，然后让这些测试表达式尽可能覆盖所有实现过的方法。 symars 居然表现得很不错，稳定使用没有发现 bug。 稀疏之路 稠密的矩阵用一块连续的内存空间表示相邻的值；稀疏矩阵动辄上万的边长（上亿的总元素数 numel）不允许。于是针对稀疏矩阵单独实现了其 hessian 的组装过程：\n定义一个问题，即实现一个 Objective\u0026lt;N\u0026gt; trait，需要：\n确定 problem size N（这是编译器要求 const generics 必须是编译期常量） 实现计算逻辑 比如：弹簧质点系统的逻辑（其实就是高中学的胡克定律 $E =\\dfrac{1}{2}kx^2$ ）： 简单解释：在二维平面中模拟，每个点坐标 $(x,y)$ 有两个实数；每个弹簧涉及两个点，得到 $2 \\times 2 =4$ 这个自由度。 impl Objective\u0026lt;4\u0026gt; for SpringEnergy { type EvalArgs = f64; // restlength fn eval(\u0026amp;self, variables: \u0026amp;advec\u0026lt;4, 4\u0026gt;, restlen: \u0026amp;Self::EvalArgs) -\u0026gt; Ad\u0026lt;4\u0026gt; { // extract node positions from problem input: let p1 = advec::\u0026lt;4, 2\u0026gt;::new(variables[0].clone(), variables[1].clone()); let p2 = advec::\u0026lt;4, 2\u0026gt;::new(variables[2].clone(), variables[3].clone()); let len = (p2 - p1).norm(); let e = make::val(0.5 * self.k) * (len - make::val(*restlen)).powi(2); e } } 定义这个稀疏向量中的哪些分量，需要作为这个问题的输入（提供其 indices，\u0026amp;[[usize; N]]）。\nAD 自动组装 grad 和 hess（稀疏），涉及到 index map 的问题；\n最后用户手动将多个 grad 和 hess 加和。这一步就没有 index map 的问题了，就是简单的矩阵加法（triplet matrix 就更简单，直接把多个 triplet vector 接在一起就好了）。\n添加测试之前总共有2.2k行代码，添加测试之后项目总代码量膨胀到了18k行，再次证明数LOC是个没啥用的事情。\n最后，经过一大堆冗长的测试，写了一个 demo 来娱乐自己，顺便作为 example： 结语 收获：\n熟悉了自动求导 用 AI 写文档（他目前还读不懂我的代码，或者说还读不太懂 Rust，所以写的测试有许多语法问题） Happiness! ","date":"29 December, 2024","id":24,"permalink":"/posts/road-to-diff/","summary":"Symars  Rust代码生成库和 Raddy 自动求导库的来龙去脉","tags":"","title":"自动求导, 道阻且长"},{"content":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。\n1. 问题 对于实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和向量 $b \\in \\mathbb{R}^n$，求解\n$$Ax = b$$或者，等价的，\n$$\\text{argmin}_x f(x)$$其中\n$$f(x) = \\frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：\n$$a^{-2}x^2 + b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：\n这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。\n2.2. 最速下降法 首先，你应该知道梯度下降法：\n$$x_{i+1} = x_i - \\alpha\\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：\n$$\\alpha^* = \\text{argmin}_\\alpha f(x_i - \\alpha\\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量\n$r_i$：$b_i - Ax_i$，目标函数 $f(x)$ 在 $x_i$ 点的负梯度，或者线性方程组在 $x_i$ 点的残差。\n请记住：负梯度和残差是一个东西！ $d_i$：在 $x_i$ 点的搜索方向。最速下降算法里 $d_i = r_i$，共轭梯度里面需要一点修正。\n3.2. 最速下降 最速下降的新方向：$r_{i+1}$\n新方向与前一步下降方向 $r_i$ 垂直（画个等高线图直观理解，或者回想一下\u0026quot;等势面和电场线垂直\u0026quot;） 最速下降的 $\\alpha$ $$\\alpha_i = \\frac{r_i^T r_i}{d_i^T A d_i}$$3.3. 共轭梯度 我们直接逐项类比最速下降。\n新方向与前一步下降方向 $r_i$ 垂直 斜率之积为 $-a^{-2}b^2$ (Section 2.1)\n这个方向由最速下降的方向进行一些小改动得到，我们可以在后面的算法部分(Section 3.4)看到。把这个方向从和前一个搜索方向垂直改动到斜率之积为 $-a^{-2}b^2$ 就是 CG 和最速下降唯一不同的地方。 步长 $\\alpha$：由于是在一条直线上做优化，因此和最速下降的 $\\alpha$ 相同。 由于一次迭代只涉及到两个点、两个向量，只能构成一个平面，我们甚至不需要将二维向多维推广。\n若需推导，我们需要做的只是把点的 $n$ 维坐标映射到二维，然后对截面椭圆对应的二阶二次型进行 SVD 获得其长轴 $a$ 和短轴 $b$，进而根据其离及上述斜率积的二级结论计算两个方向的关系。这里不展开。 3.4. 算法 3.4.1. 初始化 算法输入：$A, b, x_0$\n$$\\vec{d}_{(0)} = \\vec{r}_{(0)} = \\vec{b}_{(0)} - \\mathbf{A}\\vec{x}_{(0)}$$3.4.2. 算法过程 $$\\alpha_{(i)} = \\frac{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}{\\vec{d}_{(i)}^T \\mathbf{A}\\vec{d}_{(i)}}$$$$\\vec{x}_{(i+1)} = \\vec{x}_{(i)} + \\alpha_{(i)}\\vec{d}_{(i)}$$$$\\vec{r}_{(i+1)} = \\vec{r}_{(i)} - \\alpha_{(i)}\\mathbf{A}\\vec{d}_{(i)}$$$$\\beta_{(i+1)} = \\frac{\\vec{r}_{(i+1)}^T \\vec{r}_{(i+1)}}{\\vec{r}_{(i)}^T \\vec{r}_{(i)}}$$$$\\vec{d}_{(i+1)} = \\vec{r}_{(i+1)} + \\beta_{(i+1)}\\vec{d}_{(i)}$$其中的最后一步就是通过 $\\beta$ 将 $r_{i+1}$ 修正成 $d_{i+1}$ 的。\n3.4.3. 起讫 起：如果你对解 $x$ 有粗略的估计，就使用那个值作为起始点 $x_0$；否则，直接使用 $x_0 = 0$。\n讫：通常的做法是在残差向量的 2-norm 小于某个给定阈值的时候就停下来。通常这个阈值为初始残差的一小部分\n$$\\|r_i\\| \u003c \\varepsilon \\|r_0\\|$$其中 $\\varepsilon$ 是一个输入的参数。\n3.5. 杂项 由于 $Ad_i$ 在每个循环中都要被计算，且 $$r_{i+1} = r_i - \\alpha_i A d_i$$故可以用上式计算 $r_{i+1}$，而不必用 $b - Ax_{i+1}$。\n上述方法有浮点误差累计的危险，因此我们应该每过几个循环就重新用 $r_i = b - Ax_i$ 重新计算残差。 ","date":"7 December, 2024","id":25,"permalink":"/posts/conj-grad/","summary":"本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。","tags":"math","title":"共轭梯度：一种高中解析几何的视角"},{"content":"个人信息 名称：黛西\nNickname: Da1sypetals\nEmail: da1sypetals.iota@gmail.com\n爱好 唱古风歌。\n比如, 我会唱这些：\n《人间不值得》《楚歌起》 黄诗扶\n《迟迟》《腐草为萤》 银临\n《故事外的人》 慕寒\n《惊鹊》《心上秋》 忘川风华录\n《泼墨漓江》 泠鸢yousa\n《敢归云间宿》 三无Marblue\n《忘川》《霁夜茶》 小曲儿\n《松烟入墨》《如是我闻》 Winky诗\n《悦神》 KBShinya\n《第三十八年夏至》《永定四十年》 河图\n《东风志》 Aki阿杰\n这里还有很多\u0026hellip;\n","date":"1 June, 2004","id":26,"permalink":"/about/","summary":"名称：黛西","tags":"","title":"关于我"}]