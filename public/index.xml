<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Da1sypetals</title>
    <link>https://da1sy-petals.vercel.app/</link>
    <description>Recent content on Da1sypetals</description>
    <generator>Hugo</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 13 Nov 2025 19:38:26 +0800</lastBuildDate>
    <atom:link href="https://da1sy-petals.vercel.app/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>cuTile 历险记，第0集：心智模型</title>
      <link>https://da1sy-petals.vercel.app/posts/cutile-0/</link>
      <pubDate>Thu, 13 Nov 2025 19:38:26 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/cutile-0/</guid>
      <description>&lt;p&gt;首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他&lt;del&gt;劫持&lt;/del&gt;捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。&lt;/p&gt;&#xA;&lt;p&gt;因此在使用cuTile的时候，要一直告诉自己 &amp;ldquo;This is not Python&amp;rdquo;.&lt;/p&gt;&#xA;&lt;h2 id=&#34;abstraction-level&#34;&gt;Abstraction Level&lt;/h2&gt;&#xA;&lt;p&gt;作为用户需要知道的：这门语言工作在哪个抽象层级。这里指的是这门语言提供的接口，&lt;strong&gt;并不&lt;/strong&gt;直接对应硬件。cuTile的compiler magic会把我们写的代码map到硬件上，但这并不是写cuTile的程序员需要关心的。&lt;/p&gt;&#xA;&lt;h3 id=&#34;内存&#34;&gt;内存&lt;/h3&gt;&#xA;&lt;p&gt;从逻辑上来说，cuTile暴露给用户的内存分为两种：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Global Memory (Gmem):&#xA;&lt;ul&gt;&#xA;&lt;li&gt;读写速度：慢&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;Cache:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;读写速度：较快&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;编程模型&#34;&gt;编程模型&lt;/h3&gt;&#xA;&lt;h4 id=&#34;global-array&#34;&gt;Global Array&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;存放在Gmem上&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;操作：只能进行Load(从Gmem读取到Cache)，以及Store(从Cache存入Gmem)。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;来源：PyTorch tensor 可以直接传入。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;tile-array&#34;&gt;Tile Array&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;存放在Cache上&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;操作：可以在上面进行数学操作如&lt;code&gt;sin&lt;/code&gt;, &lt;code&gt;mma&lt;/code&gt;等。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;来源：tile kernel内创建(例如&lt;code&gt;cuda.tile.zeros&lt;/code&gt;)，或者Global Array load得到&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;Immutable&lt;/strong&gt;：&lt;strong&gt;在逻辑上&lt;/strong&gt;，任何对Tile Array的计算操作都会返回新的Tile Array (Returns copies, not views)；你也不能直接对Tile Array里面的内容进行修改。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;cuTile的compiler magic肯定会在内部防止冗余内存的创建，毕竟速度&amp;gt;=SRAM的存储是如此昂贵；但是程序员是以immutable的形式编程的。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;metadata: dtype, shape&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;layout对用户是不可见的，交由编译器处理。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;示意图&#34;&gt;示意图：&lt;/h4&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/2025-11-14-10-38-36.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;编程问题&#34;&gt;编程问题&lt;/h2&gt;&#xA;&lt;h3 id=&#34;问题输入&#34;&gt;问题输入&lt;/h3&gt;&#xA;&lt;p&gt;Tensor 级别的计算过程，比如：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Matmul+activation&lt;/li&gt;&#xA;&lt;li&gt;Attention Mechanism&lt;/li&gt;&#xA;&lt;li&gt;其他可以用NumPy/PyTorch这一级别的抽象所描述的算法。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;cutile是用来解决什么问题的&#34;&gt;cuTile是用来解决什么问题的&lt;/h3&gt;&#xA;&lt;p&gt;对于同一个算法，如何明智地加载数据、进行计算，可以减少Load/Store的数据量、增大计算密度，并且减少需要在Gmem上materialize的中间数据。&lt;/p&gt;&#xA;&lt;h2 id=&#34;奇怪的想法&#34;&gt;奇怪的想法&lt;/h2&gt;&#xA;&lt;p&gt;据nv的编译器工程师在各种talk里面所说，cuTile将会屏蔽所有&lt;strong&gt;硬件特异性&lt;/strong&gt;的功能，交由编译器处理。如果是这样的话，其他硬件厂商是不是更方便在这一层级往下做？比如，直接开发一套&lt;code&gt;rocm.tile&lt;/code&gt;，在接口上对标&lt;code&gt;cuda.tile&lt;/code&gt;，然后实现自己的编译器，Lower到自己的具有硬件特异性的IR代码上进行优化。这样似乎就避免了triton目前推出越来越多nvidia定制的功能，导致其他硬件厂商无法跟进的问题。&lt;/p&gt;&#xA;&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=uZTtViomW6w&#34;&gt;cuTile talk @ scipy&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=UEdGJGz8Eyg&#34;&gt;cuTile talk @ torch&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>AI带读歌词：《眉间冬赴春》</title>
      <link>https://da1sy-petals.vercel.app/art/ai%E5%B8%A6%E8%AF%BB%E6%AD%8C%E8%AF%8D%E7%9C%89%E9%97%B4%E5%86%AC%E8%B5%B4%E6%98%A5/</link>
      <pubDate>Sun, 02 Nov 2025 14:53:28 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/art/ai%E5%B8%A6%E8%AF%BB%E6%AD%8C%E8%AF%8D%E7%9C%89%E9%97%B4%E5%86%AC%E8%B5%B4%E6%98%A5/</guid>
      <description>&lt;p&gt;&lt;strong&gt;注意：历史同人和历史毫无关系，请勿随便代入。&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;设定&#34;&gt;设定&lt;/h2&gt;&#xA;&lt;p&gt;谢玄，字幼度，陈郡阳夏人。谢安之侄，封康乐县公，淝水之战前锋都督，一战击溃苻坚百万兵，后世称“江左第一将”。&lt;/p&gt;&#xA;&lt;p&gt;谢道韫，谢奕之女、谢安侄女，王凝之之妻，中国历史上第一位被正史立传的女诗人，有“咏絮才”之誉，晚年寡居会稽，以文墨自遣。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;verse-1&#34;&gt;【Verse 1】&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;行迹迟 踏细雪上东山&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“东山”直指会稽东山，谢安早年隐居之地，也是谢氏子弟日常游憩的“自家后花园”。谢玄少年多病，史书说他“晚达”，故“行迹迟”；“细雪”先布下“雪”意象，为后文“咏絮”埋伏笔。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;畏冬寒 看天边星愈淡&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;谢玄幼年体弱，家人常恐其不寿，故“畏冬寒”是实写身体，也是隐喻：东晋末年的政治空气同样“寒冷”。“星愈淡”暗示晋室衰微，天命将改。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;回忆里小院温暖 捧灯相坐看&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“小院”是谢府乌衣巷旧宅，兄妹两小无猜，雪夜围灯读书。史书载谢安“内集儿女，讲论文义”，画面感极强。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;竟已隔几重山&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;一灯如豆的童年，转瞬被战火、婚姻、官爵隔开。“几重山”既是空间，也是身份：一个走向沙场，一个困于闺阁。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;pre-chorus-1&#34;&gt;【Pre-Chorus 1】&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;稚语童音 说儿时旧梦梦不完&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;谢安雪日“何所似”之问，谢朗答“撒盐空中”，道韫补“未若柳絮因风起”，全场“大笑乐”。歌词把这段千古对话，化成兄妹私下“说梦”。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;光华流转 心上写风月与民安&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“光华”承“灯”而来：灯影在壁，也照出二人各自抱负——&lt;/p&gt;&#xA;&lt;p&gt;谢玄：愿天下“民安”；&lt;/p&gt;&#xA;&lt;p&gt;道韫：愿文字“风月”长存。一句写尽“一文一武”两种人生取向。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;你披上荣光 走进青史路漫漫&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“荣光”指淝水战功；“青史路漫漫”点明谢玄已踏上“治国经略”的不归路。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;我捡点红妆 立笔冢玲珑辞畔&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“笔冢”典出王羲之“退笔成冢”，此处移用于道韫：她无法横刀立马，只能把“红妆”换“文冢”，以玲珑诗句自垒一冢青春。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;月已升 梦未返&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;重复句，像昆曲“水磨腔”，一唱三叹：时间往前走，童年永远留在那盏灯里。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;chorus-1&#34;&gt;【Chorus 1】&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;你走过治国经略 书写历史的轮廓&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;谢玄后半生几乎“马背上的办公桌”：北伐、筑堰、移民、练兵，把东晋的国境线硬往北推了三百里。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;王侯将相的叙事吞噬少年旧肝胆&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“王侯将相”是青史正典，而正典只会记录“功业”，不会记录“撒盐空中”的童心。“吞噬”二字，说历史对个体的残酷淘洗。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;我立于浮世茫茫 逐流惹一身尘满&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;道韫出嫁后，随王凝之守会稽，孙恩乱起，丈夫死于乱刀，她“手刃数贼”后仍被俘虏。歌词把这段经历，浓缩成“逐流惹尘”。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;朱门士族的圭臬规训才情与烂漫&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“朱门圭臬”指门第礼法：再高的“咏絮才”，也要先当好“王谢家媳妇”。才情与烂漫，被族规一层层缠成“礼”的茧。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 id=&#34;verse-2&#34;&gt;【Verse 2】&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;深深廊 世情比条框难&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;乌衣巷深廊回合，一眼望不到头；“条框”是族规、三纲五常，也是东晋门阀政治那张看不见的网。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;惊鸿影 隐入岁月的川&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;“惊鸿”出自《洛神赋》，比喻道韫才貌；然而才貌终被“岁月的川”带走，只剩史书里两行小字。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Triton Tensor Descriptor: 茴字的第三种写法</title>
      <link>https://da1sy-petals.vercel.app/posts/triton-td/</link>
      <pubDate>Fri, 31 Oct 2025 21:49:36 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/triton-td/</guid>
      <description>&lt;p&gt;今天我们来介绍&lt;del&gt;茴字的第3种写法&lt;/del&gt;&lt;/p&gt;&#xA;&lt;p&gt;今天我们来介绍 Triton 中的第三种进行 tensor 指针运算的 API：Tensor Descriptor。内容来自&lt;a href=&#34;https://triton-lang.org/main/python-api/generated/triton.language.make_tensor_descriptor.html&#34;&gt;triton 文档&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;关于-triton-的基本概念&#34;&gt;关于 triton 的基本概念&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;triton 只是和 python 共用语言前端（我们写的代码），triton 会接管 python 的 AST，然后后续步骤就交由 triton 编译器一步步 lower 到 GPU 代码了。&lt;/li&gt;&#xA;&lt;li&gt;在第一次执行一个 kernel 之前发生的事情称为编译期，之后的执行称为运行时。&lt;/li&gt;&#xA;&lt;li&gt;triton的 kernel launch 的grid 参数是一个 ndrange，在 kernel 里面获取到的 &lt;code&gt;program_id(i)&lt;/code&gt; 就是第 i 维度的 index。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;tensor-descriptor的用法&#34;&gt;Tensor Descriptor的用法&lt;/h2&gt;&#xA;&lt;h3 id=&#34;创建&#34;&gt;创建&lt;/h3&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;make_tensor_descriptor(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    pointer,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[M, N],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    strides&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[N, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    block_shape&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[M_BLOCK, N_BLOCK],&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;pointer&lt;/code&gt; 就是传入triton kernel的tensor&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;shape&lt;/code&gt; 是一个整数列表，&lt;strong&gt;可以编译期确定，也可以运行时动态传入，可以不是tilesize的倍数&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;传入 &lt;code&gt;[tensor.shape(i) for i in range(tensor.dim())]&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;strides&lt;/code&gt; 是一个整数列表，&lt;strong&gt;可以编译期确定，也可以运行时动态传入，可以不是tilesize的倍数&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;传入 &lt;code&gt;[tensor.stride(i) for i in range(tensor.dim())]&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;block_shape&lt;/code&gt; 是一个整数列表，&lt;strong&gt;必须是编译期常量&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;对应概念是CUDA的blockDim&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;上述三者的长度必须相同，等于输入tensor的 &lt;code&gt;.dim()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;读写&#34;&gt;读写&lt;/h3&gt;&#xA;&lt;h4 id=&#34;读&#34;&gt;读&lt;/h4&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; desc&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load([moffset, noffset])&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中唯一的参数offsets是一个整数列表：&lt;/p&gt;</description>
    </item>
    <item>
      <title>我的 Mac Mini Setup</title>
      <link>https://da1sy-petals.vercel.app/posts/macmini-setup/</link>
      <pubDate>Fri, 24 Oct 2025 09:55:27 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/macmini-setup/</guid>
      <description>&lt;p&gt;花点时间配置一下我的Mac Mini.&lt;/p&gt;&#xA;&lt;h2 id=&#34;配件&#34;&gt;配件&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Mac Mini M4 最丐版，3349&lt;/li&gt;&#xA;&lt;li&gt;Samsung 990 evo plus, 996&lt;/li&gt;&#xA;&lt;li&gt;海备思硬盘盒+扩展坞，471&lt;/li&gt;&#xA;&lt;li&gt;Redmi 显示器 4K 60Hz，1449&lt;/li&gt;&#xA;&lt;li&gt;鼠标，65&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;把系统装到外接硬盘里：&lt;a href=&#34;https://www.bilibili.com/video/BV1m2rUYcEQA&#34;&gt;教程&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;鼠标滚轮反向&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;软件&#34;&gt;软件&lt;/h2&gt;&#xA;&lt;h3 id=&#34;浏览器&#34;&gt;浏览器&lt;/h3&gt;&#xA;&lt;p&gt;我的需求是：&lt;strong&gt;Vertical tab, 以及熟悉&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;因为我之前在windows用的是edge，在mac上一搜居然也有，于是就直接用edge懒得换了。&lt;/p&gt;&#xA;&lt;h3 id=&#34;terminal&#34;&gt;Terminal&lt;/h3&gt;&#xA;&lt;h4 id=&#34;emulator-iterm2&#34;&gt;Emulator: iTerm2:&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;主题：&lt;a href=&#34;https://github.com/phureewat29/fairyfloss&#34;&gt;FairyFloss&lt;/a&gt;，一个很girly的主题，背景颜色改深了一点&lt;/li&gt;&#xA;&lt;li&gt;字体：Jetbrains Mono&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h4 id=&#34;shell-fish&#34;&gt;Shell: fish&lt;/h4&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;注意去github装 fish 4，软件源上很有可能还是fish 3.7，fish 3 已经不维护了&lt;/li&gt;&#xA;&lt;li&gt;设置默认shell：有两个步骤，两个都要做：&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;chsh&lt;/code&gt; 命令，具体格式请问AI&lt;/li&gt;&#xA;&lt;li&gt;按照下面指引&#xA;&lt;img src=&#34;../images/2025-10-24-10-12-50.png&#34; alt=&#34;&#34;&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;笔记&#34;&gt;笔记&lt;/h3&gt;&#xA;&lt;p&gt;ima.copilot，主要为了省心，我并没有折腾私有部署的兴趣，对信息安全也不甚关心。&lt;/p&gt;&#xA;&lt;h3 id=&#34;brew换源&#34;&gt;Brew换源&lt;/h3&gt;&#xA;&lt;p&gt;详见：&lt;a href=&#34;../brew-sources/&#34;&gt;这篇帖子&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h3 id=&#34;实用软件&#34;&gt;实用软件&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;typst&#xD;&#xA;fd (replacement of find)&#xD;&#xA;ripgrep&#xA;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    <item>
      <title>Brew 换源</title>
      <link>https://da1sy-petals.vercel.app/posts/brew-sources/</link>
      <pubDate>Thu, 23 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/brew-sources/</guid>
      <description>&lt;p&gt;原文：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mirrors.ustc.edu.cn/help/brew.git.html&#34;&gt;https://mirrors.ustc.edu.cn/help/brew.git.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mirrors.ustc.edu.cn/help/homebrew-bottles.html&#34;&gt;https://mirrors.ustc.edu.cn/help/homebrew-bottles.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mirrors.ustc.edu.cn/help/homebrew-core.git.html&#34;&gt;https://mirrors.ustc.edu.cn/help/homebrew-core.git.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://mirrors.ustc.edu.cn/help/homebrew-cask.git.html&#34;&gt;https://mirrors.ustc.edu.cn/help/homebrew-cask.git.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;-一设置环境变量永久生效针对-fish&#34;&gt;✅ 一、设置环境变量（永久生效，针对 fish）&lt;/h2&gt;&#xA;&lt;p&gt;在 fish 中，永久设置环境变量应使用   set -Ux （全局、导出、持久化）：&#xA;（set文档： https://fishshell.com/docs/current/cmds/set.html ）&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Homebrew 主程序仓库&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set -Ux HOMEBREW_BREW_GIT_REMOTE https://mirrors.ustc.edu.cn/brew.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Homebrew 核心公式仓库&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set -Ux HOMEBREW_CORE_GIT_REMOTE https://mirrors.ustc.edu.cn/homebrew-core.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 预编译二进制包（bottles）域名&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set -Ux HOMEBREW_BOTTLE_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 元数据 API 域名（Brew 4.0+ 必需）&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set -Ux HOMEBREW_API_DOMAIN https://mirrors.ustc.edu.cn/homebrew-bottles/api&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;-二配置-homebrew-cask-使用镜像git-仓库方式&#34;&gt;✅ 二、配置 Homebrew Cask 使用镜像（Git 仓库方式）&lt;/h2&gt;&#xA;&lt;p&gt;由于Brew 4.0+ 默认使用 JSON API，大多数情况下不需要手动设置 cask 的 Git 镜像。但如果你仍希望显式使用 USTC 的 cask Git 仓库（例如离线环境或调试），请运行：&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew tap --custom-remote homebrew/cask https://mirrors.ustc.edu.cn/homebrew-cask.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;⚠️ 注意：如果你以后想恢复官方源，可运行：&#xA;&lt;code&gt;brew tap --custom-remote homebrew/cask https://github.com/Homebrew/homebrew-cask&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyG Batching</title>
      <link>https://da1sy-petals.vercel.app/documents/pyg-batching/</link>
      <pubDate>Thu, 16 Oct 2025 14:57:00 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/documents/pyg-batching/</guid>
      <description>&lt;p&gt;内容来自&lt;a href=&#34;https://pytorch-geometric.readthedocs.io/en/latest/advanced/batching.html&#34;&gt;这里&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;高级 Mini-Batching（Mini-Batching）&lt;/p&gt;&#xA;&lt;p&gt;创建 mini-batching 对于让深度学习模型的训练扩展到海量数据至关重要。mini-batch 不会一个接一个地处理样本，而是将一组样本分组到一个统一的表示中，从而可以高效地并行处理。在图像或语言领域，这个过程通常是通过将每个样本重新缩放或 padding 到一组等大小的形状来实现的，然后将样本分组到一个额外的维度中。这个维度的长度等于分组在一个 mini-batch 中的样本数量，通常称为 batch_size。&lt;/p&gt;&#xA;&lt;p&gt;由于图是一种最通用的数据结构，可以包含任意数量的节点（nodes）或边（edges），因此上述两种方法要么不可行，要么可能导致大量不必要的内存消耗。在 PyG 中，我们采用另一种方法来实现对大量样本的并行化。在这里，adjacency matrices 以对角线方式堆叠（创建一个包含多个孤立子图的巨大图），并且节点和目标特征（features）简单地沿节点维度进行拼接，即：&lt;/p&gt;&#xA;$$&#xA;A = \begin{bmatrix} A_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; A_n \end{bmatrix}, \quad&#xA;X = \begin{bmatrix} X_1 \\ \vdots \\ X_n \end{bmatrix}, \quad&#xA;Y = \begin{bmatrix} Y_1 \\ \vdots \\ Y_n \end{bmatrix}.&#xA;$$&lt;p&gt;与其他 batching 过程相比，此过程具有一些关键优势：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;依赖于 message passing scheme 的 GNN operators 不需要修改，因为属于不同图的两个节点之间仍然不能交换消息。&lt;/li&gt;&#xA;&lt;li&gt;没有计算或内存开销。例如，此 batching 过程完全不需要对节点或边的特征进行任何 padding。请注意，adjacency matrices 没有额外的内存开销，因为它们以稀疏（sparse）方式保存，只包含非零项，即边。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;PyG 借助 &lt;code&gt;torch_geometric.loader.DataLoader&lt;/code&gt; 类自动将多个图 batch 成一个巨大的图。在内部，&lt;code&gt;DataLoader&lt;/code&gt; 只是一个常规的 PyTorch torch.utils.data.DataLoader，它重写了其 &lt;code&gt;collate()&lt;/code&gt; 功能，即定义如何将样本列表分组在一起。因此，所有可以传递给 PyTorch DataLoader 的参数也可以传递给 PyG &lt;code&gt;DataLoader&lt;/code&gt;，例如 worker 数量 &lt;code&gt;num_workers&lt;/code&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>我的萝卜头像</title>
      <link>https://da1sy-petals.vercel.app/documents/carrot/</link>
      <pubDate>Sun, 12 Oct 2025 15:29:58 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/documents/carrot/</guid>
      <description>&lt;h2 id=&#34;矢量图&#34;&gt;矢量图&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/carrot.svg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;位图hires&#34;&gt;位图，hires&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/carrot_padded.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;位图lowres&#34;&gt;位图，lowres&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/carrot_540.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>全民K歌如何下载歌曲</title>
      <link>https://da1sy-petals.vercel.app/documents/quanmin-download/</link>
      <pubDate>Sun, 05 Oct 2025 21:07:06 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/documents/quanmin-download/</guid>
      <description>&lt;h2 id=&#34;1-用默认浏览器打开&#34;&gt;1. 用默认浏览器打开&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/2025-10-05-21-08-03.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;2-按f12然后把focus放在html&#34;&gt;2. 按F12，然后把Focus放在html&lt;/h2&gt;&#xA;&lt;p&gt;先按F12，然后按下面的步骤操作&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/2025-10-05-21-10-32.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;3-搜索音频扩展名&#34;&gt;3. 搜索音频扩展名&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;先随便点一下里面的代码，然后 Ctrl-F&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;搜索&lt;code&gt;.m4a&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/2025-10-05-21-13-53.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;ol start=&#34;3&#34;&gt;&#xA;&lt;li&gt;找到一个含有&lt;code&gt;.m4a&lt;/code&gt; 的链接，&lt;strong&gt;然后双击，复制到新的标签页打开&lt;/strong&gt;。很长一串&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;4-下载&#34;&gt;4. 下载&lt;/h2&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../2025-10-05-21-15-16.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一个关于GNN在Tensor Core上的实现的数学问题</title>
      <link>https://da1sy-petals.vercel.app/posts/interesting-math-gnn-tensor-core/</link>
      <pubDate>Sat, 04 Oct 2025 16:07:12 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/interesting-math-gnn-tensor-core/</guid>
      <description>&lt;h2 id=&#34;完整的问题&#34;&gt;完整的问题&lt;/h2&gt;&#xA;&lt;p&gt;可以看&lt;a href=&#34;https://g.co/gemini/share/ad1bd7ed4a63&#34;&gt;我和Gemini的对话&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;抽象出的问题&#34;&gt;抽象出的问题&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;假设在mxn的矩阵中全为0，现在进行两个操作：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;以avgdeg/n的概率将矩阵中的值变为1；&lt;/li&gt;&#xA;&lt;li&gt;如果矩阵中有一列有一个1，那就把这一列都变成1。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;现在计算avgdeg&amp;rsquo;为矩阵中1的个数/(mn)，记avgdeg=d, avgdeg&amp;rsquo;=d&#39;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;h2 id=&#34;kimi的回答&#34;&gt;Kimi的回答&lt;/h2&gt;&#xA;&lt;p&gt;我们来逐步分析这个问题，并给出数学推导。&lt;/p&gt;&#xA;&lt;h3 id=&#34;问题设定&#34;&gt;问题设定&lt;/h3&gt;&#xA;&lt;p&gt;我们有一个 $ m \times n $ 的矩阵，初始时所有元素为 0。&lt;/p&gt;&#xA;&lt;p&gt;进行两个操作：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;随机置 1&lt;/strong&gt;：以概率 $ \frac{\text{avgdeg}}{n} $ 将每个元素独立地变为 1（否则保持 0）。&lt;br&gt;&#xA;记 $ d = \text{avgdeg} $，所以每个元素变为 1 的概率是 $ p = \frac{d}{n} $。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;strong&gt;列传播&lt;/strong&gt;：如果某一列中&lt;strong&gt;至少有一个 1&lt;/strong&gt;，则将该列&lt;strong&gt;所有元素都变为 1&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;最终，我们计算新的平均度：&lt;/p&gt;&#xA;\[&#xA;d&#39; = \frac{\text{矩阵中 1 的总数}}{mn}&#xA;\]&lt;p&gt;我们的目标是求 $ d&#39; $ 关于 $ d $ 的表达式，即 $ d&#39; = f(d) $。&lt;/p&gt;&#xA;&lt;h3 id=&#34;步骤-1计算某一列在操作-1-后至少有一个-1的概率&#34;&gt;步骤 1：计算某一列在操作 1 后&lt;strong&gt;至少有一个 1&lt;/strong&gt;的概率&lt;/h3&gt;&#xA;&lt;p&gt;在操作 1 后，每个元素独立地以概率 $ p = \frac{d}{n} $ 变为 1。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pedalboard 文档</title>
      <link>https://da1sy-petals.vercel.app/documents/pedalboard/</link>
      <pubDate>Sat, 04 Oct 2025 14:20:22 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/documents/pedalboard/</guid>
      <description>&lt;p&gt;Github: &lt;a href=&#34;https://github.com/spotify/pedalboard&#34;&gt;https://github.com/spotify/pedalboard&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Docs: &lt;a href=&#34;https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.LowShelfFilter&#34;&gt;https://spotify.github.io/pedalboard/reference/pedalboard.html#pedalboard.LowShelfFilter&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h1 id=&#34;pedalboard-api-documentation&#34;&gt;Pedalboard API Documentation&lt;/h1&gt;&#xA;&lt;p&gt;The &lt;code&gt;pedalboard&lt;/code&gt; module provides classes and functions for adding effects to audio. Most classes in this module are subclasses of &lt;code&gt;Plugin&lt;/code&gt;, each of which allows applying effects to an audio buffer or stream.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For audio I/O functionality (i.e.: reading and writing audio files), see the &lt;code&gt;pedalboard.io&lt;/code&gt; module.&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;The &lt;code&gt;pedalboard&lt;/code&gt; module is named after the concept of a guitar pedalboard, in which musicians will chain various effects pedals together to give them complete control over their sound. The &lt;code&gt;pedalboard&lt;/code&gt; module implements this concept with its main &lt;code&gt;Pedalboard&lt;/code&gt; class:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Raddy devlog: forward autodiff system</title>
      <link>https://da1sy-petals.vercel.app/english-post/raddy/</link>
      <pubDate>Thu, 02 Oct 2025 15:15:21 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/raddy/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; I created &lt;a href=&#34;https://github.com/Da1sypetals/Raddy&#34;&gt;Raddy&lt;/a&gt;, a forward autodiff library, and &lt;a href=&#34;https://github.com/Da1sypetals/Symars&#34;&gt;Symars&lt;/a&gt;, a symbolic codegen library.&lt;/p&gt;&#xA;&lt;p&gt;If you&amp;rsquo;re interested, please give them a star and try them out! ❤️&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-origin-of-the-story&#34;&gt;The Origin of the Story&lt;/h2&gt;&#xA;&lt;p&gt;I recently read papers on physical simulation and wanted to reproduce them. I started with &lt;a href=&#34;https://graphics.pixar.com/library/StableElasticity/paper.pdf&#34;&gt;Stable Neo-Hookean Flesh Simulation&lt;/a&gt;, though the choice isn&amp;rsquo;t critical. Many modern physical simulations are implicit, requiring Newton&amp;rsquo;s method to solve optimization problems.&lt;/p&gt;&#xA;&lt;p&gt;This involves:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Computing derivatives of the constitutive energy model (first-order gradient, second-order Hessian).&lt;/li&gt;&#xA;&lt;li&gt;Assembling a large, sparse Hessian from small, dense Hessian submatrices — a delicate task prone to hard-to-debug bugs.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;From &lt;a href=&#34;https://www.tkim.graphics/DYNAMIC_DEFORMABLES/&#34;&gt;Dynamic Deformables&lt;/a&gt;, I learned deriving these formulas is labor-intensive (even understanding the notation takes time). Searching for alternatives to avoid meticulous debugging, I found two solutions:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Triton Common Pitfalls</title>
      <link>https://da1sy-petals.vercel.app/english-post/triton-pitfalls/</link>
      <pubDate>Thu, 02 Oct 2025 15:12:24 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/triton-pitfalls/</guid>
      <description>&lt;p&gt;From the perspective of a newbie user&lt;/p&gt;&#xA;&lt;h2 id=&#34;the-documentation-is-a-disaster&#34;&gt;The Documentation is a Disaster&lt;/h2&gt;&#xA;&lt;p&gt;Recently, I had to optimize a custom operator and decided to use OpenAI&amp;rsquo;s Triton. After digging into the documentation, I was shocked at how poorly written it is — like an academic paper full of equations but lacking practical code examples.&lt;/p&gt;&#xA;&lt;p&gt;If the library operates on tensors, the docs should clearly specify input/output shapes and provide concrete examples (like PyTorch does). Instead, everything is vaguely described in plain text, leaving users to guess the details.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Try To Implement IPC</title>
      <link>https://da1sy-petals.vercel.app/english-post/try-impl-ipc/</link>
      <pubDate>Thu, 02 Oct 2025 15:03:07 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/try-impl-ipc/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Intro: A taste of the Rust programming language&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Recently, I tried to get started with Rust and wanted to write some code.&lt;/p&gt;&#xA;&lt;p&gt;Most people&amp;rsquo;s first application is probably some kind of backend service (converting HTTP requests from the frontend into CRUD operations on a database and returning the results to the frontend).&lt;/p&gt;&#xA;&lt;p&gt;However, I&amp;rsquo;ve never learned how to write backend services (I&amp;rsquo;ve been wanting to learn recently — if anyone has good zero-to-hero beginner resources, feel free to recommend them). So, I ended up picking up the two papers I&amp;rsquo;ve been studying lately (@Li2020IPC, @abd) to try reproducing them.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SnapViewer Devlog #3: Optimizations</title>
      <link>https://da1sy-petals.vercel.app/english-post/snapviewer-3-optim/</link>
      <pubDate>Thu, 02 Oct 2025 15:01:14 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/snapviewer-3-optim/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Intro: Troubleshooting Memory and Speed Performance&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; I develop and test primarily on Windows using the latest stable Rust toolchain and CPython 3.13.&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-background-and-motivation&#34;&gt;1. Background and Motivation&lt;/h2&gt;&#xA;&lt;p&gt;SnapViewer handles large memory snapshots effectively — for example, pickle files up to 1 GB and compressed snapshots up to 500 MB. However, when processing extremely large dumps (e.g., a 1.3 GB snapshot), we encountered serious memory and speed bottlenecks:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Format conversion (pickle → compressed JSON) triggered memory peaks around 30 GB.&lt;/li&gt;&#xA;&lt;li&gt;Data loading of the compressed JSON into Rust structures caused another ~30 GB spike.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Frequent page faults and intense disk I/O (observed in Task Manager) made the application sluggish and prone to stalls. To address this, we applied a Profile-Guided Optimization (PGO) approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snapviewer Devlog #2: UI</title>
      <link>https://da1sy-petals.vercel.app/english-post/snapviewer-2-ui/</link>
      <pubDate>Thu, 02 Oct 2025 14:56:13 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/snapviewer-2-ui/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Intro: Building the UI as a Hybrid Rust &amp;amp; Python Application&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Building a UI can often be the trickiest part of a development project, especially when you&amp;rsquo;re trying to integrate different languages and paradigms.&lt;/p&gt;&#xA;&lt;p&gt;For SnapViewer, my memory allocation viewer, I needed an integrated UI that could display allocation details on click and feature a REPL for SQL queries against a SQLite database. This post details my journey, the hurdles I faced, and the solutions I found, primarily focusing on a Rust backend and Python UI.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Notes on Writing PyTorch CUDA Extensions</title>
      <link>https://da1sy-petals.vercel.app/english-post/torch-cuda-ext/</link>
      <pubDate>Thu, 02 Oct 2025 14:48:02 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/torch-cuda-ext/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Intro: PyTorch is a Deep Learning Operating System.&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;check-tensor-storage&#34;&gt;Check tensor storage&lt;/h2&gt;&#xA;&lt;h3 id=&#34;device-check&#34;&gt;Device check&lt;/h3&gt;&#xA;&lt;p&gt;You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;API:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;tensor.is_cuda()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;tensor.device()&lt;/code&gt; (Use &lt;code&gt;operator==&lt;/code&gt; for equality comparison).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Sometimes the not on correct device problem causes strange error messages like &lt;code&gt;Cusparse context initialization failure&lt;/code&gt; or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.&lt;/p&gt;</description>
    </item>
    <item>
      <title>近期GNN Attention算子优化工作速览</title>
      <link>https://da1sy-petals.vercel.app/posts/gnn-optim/</link>
      <pubDate>Thu, 02 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/gnn-optim/</guid>
      <description>&lt;p&gt;注：本文用LLM辅助写作的地方主要在：&lt;em&gt;&lt;strong&gt;我认为LLM比我理解的更好的地方，会用LLM的表述代替。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;问题设定&#34;&gt;问题设定&lt;/h2&gt;&#xA;&lt;p&gt;需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.&lt;/p&gt;&#xA;&lt;p&gt;此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。&lt;/p&gt;&#xA;&lt;h3 id=&#34;notation&#34;&gt;Notation&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;n: 图节点数，规模为 1k~1M&#xD;&#xA;nnz: 图边数（稀疏矩阵非零元素数，Num NonZero）&#xD;&#xA;&#x9;  规模为10n~1000n&#xD;&#xA;q, k, v: (n, d)&#xD;&#xA;A: (n, n), binary, 高度稀疏&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;计算公式&#34;&gt;计算公式&lt;/h3&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;softmax((q @ k.transpose()) * A) @ V&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其中，&lt;code&gt;@&lt;/code&gt; 表示矩阵乘法，&lt;code&gt;*&lt;/code&gt;表示element-wise乘法。&lt;/p&gt;&#xA;&lt;h2 id=&#34;实现naive-version&#34;&gt;实现：naive version&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是$n^2$的，显存不够用。&lt;/li&gt;&#xA;&lt;li&gt;A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair取出来得到(nnz,d)，然后再做reduce和scatter, 和V相乘。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;reformulate&#34;&gt;Reformulate&lt;/h2&gt;&#xA;&lt;p&gt;我们引入三个算子:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;SDDMM (Sampled Dense-Dense MatMul)&lt;/strong&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;A (m, k), B (k, n), 稠密&lt;/li&gt;&#xA;&lt;li&gt;M (n, n)， 稀疏&#xA;SDDMM(A, B, M) 定义为：&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;for i, j in product(range(n), range(n)):&#xD;&#xA;&#x9;if M[i, j] != 0:&#xD;&#xA;&#x9;&#x9;out[i, j] = dot(A[i,:], B[:,j])&#xD;&#xA;&#x9;else:&#xD;&#xA;&#x9;&#x9;out[i, j] = 0&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;Sparse Softmax: 在稀疏矩阵上按行softmax&lt;/li&gt;&#xA;&lt;li&gt;SpMM：sparse A @ dense B&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;此时我们的计算公式就可以重新写成:&lt;/p&gt;</description>
    </item>
    <item>
      <title>SnapViewer: Faster PyTorch Memory Allocation Viewer</title>
      <link>https://da1sy-petals.vercel.app/english-post/snapviewer/</link>
      <pubDate>Wed, 01 Oct 2025 16:09:53 +0800</pubDate>
      <guid>https://da1sy-petals.vercel.app/english-post/snapviewer/</guid>
      <description>&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;&#xA;&lt;p&gt;When training models with PyTorch, out-of-memory (OOM) errors are common, necessitating GPU memory optimization. When simple methods (like reducing batch size) no longer work, analyzing the memory footprint of the model itself may be required.&lt;/p&gt;&#xA;&lt;p&gt;At this point, you might come across this &lt;a href=&#34;https://docs.pytorch.org/docs/stable/torch_cuda_memory.html&#34;&gt;documentation&lt;/a&gt;, which teaches you how to record a memory snapshot and visualize it on this website.&lt;/p&gt;&#xA;&lt;p&gt;However, there’s a major issue: the website is extremely laggy. If your model is small, with snapshots of just a few MB, the performance is somewhat tolerable. But if your model is large, with snapshots reaching tens or even hundreds of MB, the website becomes unbearably slow, with frame rates dropping as low as 2–3 frames per minute (this is not a typo).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snapviewer Devlog #3: 性能优化</title>
      <link>https://da1sy-petals.vercel.app/posts/snapviewer-3-zh/</link>
      <pubDate>Sat, 07 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/snapviewer-3-zh/</guid>
      <description>&lt;h1 id=&#34;内存与速度性能问题排查&#34;&gt;内存与速度性能问题排查&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;免责声明&lt;/strong&gt;：我主要在 Windows 上使用最新的稳定版 Rust 工具链和 CPython 3.13 进行开发和测试。&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-背景与动机&#34;&gt;1. 背景与动机&lt;/h2&gt;&#xA;&lt;p&gt;SnapViewer 能够高效处理大型内存快照——例如，支持高达 500 MB 的压缩快照。然而，在处理 1.3 GB的snapshot的时，我发现了严重的内存和速度瓶颈：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;格式转换（pickle → 压缩 JSON）引发了约 30 GB 的内存峰值。&lt;/li&gt;&#xA;&lt;li&gt;将压缩 JSON 加载到 Rust 数据结构中又引发了另一次约 30 GB 的内存激增。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;频繁的 page fault 和强烈的磁盘 I/O（在任务管理器中观察到）导致应用程序响应迟缓，甚至频繁卡顿。为了解决这一问题，我们采用了 Profile-Guided Optimization（PGO，基于性能分析的优化）方法。&lt;/p&gt;&#xA;&lt;h2 id=&#34;2-profile-guided-optimizationpgo&#34;&gt;2. Profile-Guided Optimization（PGO）&lt;/h2&gt;&#xA;&lt;p&gt;PGO 需要通过实证分析来识别真正的热点。我首先使用 &lt;a href=&#34;https://crates.io/crates/memory-stats&#34;&gt;memory-stats&lt;/a&gt; crate 进行内存分析，在早期优化阶段进行轻量级检查。随后，我将数据加载流水线拆解为若干离散步骤：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;读取压缩文件（重度磁盘 I/O）&lt;/li&gt;&#xA;&lt;li&gt;从压缩流中提取 JSON 字符串&lt;/li&gt;&#xA;&lt;li&gt;将 JSON 反序列化为原生 Rust 数据结构&lt;/li&gt;&#xA;&lt;li&gt;填充内存中的 SQLite 数据库以支持即席 SQL 查询&lt;/li&gt;&#xA;&lt;li&gt;在 CPU 上构建三角网格（triangle mesh）&lt;/li&gt;&#xA;&lt;li&gt;初始化渲染窗口（CPU-GPU 数据传输）&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;性能分析揭示了两个主要的内存问题：过度使用 &lt;code&gt;clone&lt;/code&gt; 和多个中间数据结构。以下是我实施的优化措施。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SnapViewer: 更快的PyTorch显存分配可视化</title>
      <link>https://da1sy-petals.vercel.app/posts/snapviewer-zh/</link>
      <pubDate>Fri, 06 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/snapviewer-zh/</guid>
      <description>&lt;h2 id=&#34;背景&#34;&gt;背景&lt;/h2&gt;&#xA;&lt;p&gt;在使用 PyTorch 训练模型时，内存不足（OOM）错误是很常见的，因此需要对 GPU 内存进行优化。当简单的方法（如减少batch size）不再有效时，就需要分析模型本身的内存占用情况。&lt;/p&gt;&#xA;&lt;p&gt;此时，你可能会看到这份&lt;a href=&#34;https://docs.pytorch.org/docs/stable/torch_cuda_memory.html&#34;&gt;文档&lt;/a&gt;，它教你如何记录内存快照并在网站上进行可视化。&lt;/p&gt;&#xA;&lt;p&gt;但是这里存在一个严重的问题：这个网站性能比较差。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;如果你的模型较小，快照只有几 MB，性能还可以接受。&lt;/li&gt;&#xA;&lt;li&gt;但是如果你的模型很大，快照达到几十甚至上百 MB，网站就会变得极慢，帧率可能低至每分钟 2-3 帧（非笔误）。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;我研究了网站的 JavaScript 代码，其主要功能是：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;手动加载 Python 的 pickle 文件；&lt;/li&gt;&#xA;&lt;li&gt;每次视口发生变化时重新解析原始数据为图形表示，然后将其渲染到屏幕上。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;这些解析逻辑用 JavaScript 编写，你可以想象一下每帧执行这些操作，处理上百 MB 数据要多久.&lt;/p&gt;&#xA;&lt;h2 id=&#34;灵感&#34;&gt;灵感&lt;/h2&gt;&#xA;&lt;p&gt;我当前的工作包括优化一个非LLM的深度学习模型。在处理数十亿参数的模型所导出的显存snapshot时，我遇到了这个问题。&lt;/p&gt;&#xA;&lt;p&gt;为什么不用现有的 LLM 基础设施而选择手动优化？简单地说，这个模型是研究人员自定义设计的，其中包含许多与标准 LLM 完全不同的模块。现在似乎每个人都认为深度学习就只是关于 LLM——以至于一些技术负责人也认为 LLM 的基础设施可以很容易地适配到其他模型上……不过我有点跑题了。&lt;/p&gt;&#xA;&lt;p&gt;最初，我写了一个简单的脚本来解析快照内容，希望能发现模型中的内存分配问题。但是在一个月的工作中，我发现我还是需要一个带有GUI的可视化器, 于是我开发了 SnapViewer.&lt;/p&gt;&#xA;&lt;p&gt;简而言之：内存快照的图形数据被解析并呈现为一个巨大的三角形mesh，利用现有的渲染库来高效处理网格渲染。&lt;/p&gt;&#xA;&lt;p&gt;下面是一个 100 MB 以上的快照在我的集成显卡上流畅运行的截图：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/snapviewer.gif&#34; alt=&#34;snapviewer&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;&#xA;&lt;h3 id=&#34;参考实现&#34;&gt;参考实现&lt;/h3&gt;&#xA;&lt;p&gt;快照格式在 &lt;code&gt;record_memory_history&lt;/code&gt; 函数的&lt;a href=&#34;https://github.com/pytorch/pytorch/blob/main/torch/cuda/memory.py&#34;&gt;docstring&lt;/a&gt;中有部分记录。但这份文档并不完整, 可能是后续commit的人懒得更新docstring了.&lt;/p&gt;&#xA;&lt;p&gt;实际将快照解析为字典的过程发生在&lt;a href=&#34;https://github.com/pytorch/pytorch/blob/main/torch/cuda/_memory_viz.py&#34;&gt;这里&lt;/a&gt;：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;该脚本将分配器跟踪转换为内存时间线，然后传递给网页查看器的 JS 代码。&lt;/li&gt;&#xA;&lt;li&gt;JS 代码进一步将其转换为多边形（表示分配），用于可视化。每个多边形对应一个分配，存储大小和调用栈等细节。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;实现快照-反序列化&#34;&gt;实现：快照 (反)序列化&lt;/h3&gt;&#xA;&lt;h4 id=&#34;初始实现&#34;&gt;初始实现&lt;/h4&gt;&#xA;&lt;p&gt;我用 Python 实现这一部分，因为我需要处理 Python 原生数据结构。我只是简单地将字典转换为 JSON 文件。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Search</title>
      <link>https://da1sy-petals.vercel.app/search/</link>
      <pubDate>Mon, 24 Mar 2025 23:00:00 -0300</pubDate>
      <guid>https://da1sy-petals.vercel.app/search/</guid>
      <description>&lt;h1 id=&#34;search&#34;&gt;Search&lt;/h1&gt;&#xA;&lt;p&gt;What would you like to read today?&lt;/p&gt;&#xA;&lt;p class=&#34;hidden&#34;&gt;It&amp;#39;s necessary to enable Javascript&lt;/p&gt;&#xD;&#xA;&lt;p class=&#34;search-loading hidden&#34;&gt;Loading...&lt;/p&gt;&#xD;&#xA;&#xD;&#xA;&lt;form id=&#34;search-form&#34; class=&#34;search-form&#34; action=&#34;#&#34; method=&#34;post&#34; accept-charset=&#34;UTF-8&#34; role=&#34;search&#34;&gt;&#xD;&#xA;  &lt;div class=&#34;search-bar&#34;&gt;&#xD;&#xA;    &lt;label for=&#34;query&#34; class=&#34;hidden&#34;&gt;&lt;/label&gt;&#xD;&#xA;    &lt;input id=&#34;query&#34; class=&#34;search-text&#34; type=&#34;text&#34; placeholder=&#34;Search...&#34;/&gt;&#xD;&#xA;  &lt;/div&gt;&#xD;&#xA;&lt;/form&gt;&#xD;&#xA;&#xD;&#xA;&lt;div class=&#34;search-results&#34;&gt;&lt;/div&gt;&#xD;&#xA;&#xD;&#xA;&lt;template&gt;&#xD;&#xA;  &lt;article class=&#34;search-result list-view&#34;&gt;&#xD;&#xA;    &lt;header&gt;&#xD;&#xA;      &lt;h2 class=&#34;title&#34;&gt;&lt;a href=&#34;#&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&#xD;&#xA;      &lt;div class=&#34;submitted&#34;&gt;&#xD;&#xA;        &lt;time class=&#34;created-date&#34;&gt;&lt;/time&gt;&#xD;&#xA;      &lt;/div&gt;&#xD;&#xA;    &lt;/header&gt;&#xD;&#xA;    &lt;p class=&#34;content&#34;&gt;&lt;/p&gt;&#xD;&#xA;  &lt;/article&gt;&#xD;&#xA;&lt;/template&gt;</description>
    </item>
    <item>
      <title>Lsm Tree 实现备注</title>
      <link>https://da1sy-petals.vercel.app/posts/lsm/</link>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/lsm/</guid>
      <description>&lt;p&gt;Lsm Tree 是一种内存-磁盘的层级式数据结构，常用于实现写多读少的存储引擎。&lt;/p&gt;&#xA;&lt;p&gt;这是我实现 &lt;a href=&#34;https://github.com/Da1sypetals/Lsmkv&#34;&gt;Lsmkv&lt;/a&gt; 的时候记录的备注.&lt;/p&gt;&#xA;&lt;h2 id=&#34;组件&#34;&gt;组件&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;内存部分&lt;/li&gt;&#xA;&lt;li&gt;磁盘部分&lt;/li&gt;&#xA;&lt;li&gt;WAL&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;总体&#34;&gt;总体&lt;/h2&gt;&#xA;&lt;h2 id=&#34;初始化&#34;&gt;初始化&lt;/h2&gt;&#xA;&lt;p&gt;需要 init flush thread。flush thread 的工作流程:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;等待 flush 信号量被 notify,获取一个 compact 信号量资源&lt;/li&gt;&#xA;&lt;li&gt;启动一个 sstwriter,写入这个 memtable&#xA;&lt;ul&gt;&#xA;&lt;li&gt;一个 memtable 对一个 sst&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;等到写入 sst 写完之后,才进行:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;从 frozen memtables、frozen memtable sizes 里面删除这个 memtable&lt;/li&gt;&#xA;&lt;li&gt;从 wal 里面删除这个 memtable 对应的 wal&lt;/li&gt;&#xA;&lt;li&gt;update manifest&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;try-freeze&#34;&gt;Try Freeze&lt;/h2&gt;&#xA;&lt;p&gt;如果当前大小 &amp;gt; freeze size 那么就 freeze;进一步如果所有 frozen memtable 大小之和 &amp;gt; flush threshold,那么就 set flush signal。&lt;/p&gt;&#xA;&lt;h2 id=&#34;写操作&#34;&gt;写操作&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;写 memtable&lt;/li&gt;&#xA;&lt;li&gt;写 WAL&lt;/li&gt;&#xA;&lt;li&gt;try freeze&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;内存部分&#34;&gt;内存部分&lt;/h2&gt;&#xA;&lt;h3 id=&#34;put&#34;&gt;Put&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;添加到 memtable;&lt;/li&gt;&#xA;&lt;li&gt;更新 size。&#xA;&lt;ul&gt;&#xA;&lt;li&gt;size 不需要特别精确,只需要是一个大致的值即可。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;delete&#34;&gt;Delete&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;添加一个 tomb 标记到 memtable&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h3 id=&#34;get&#34;&gt;Get&lt;/h3&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;从 active memtable 中获取&lt;/li&gt;&#xA;&lt;li&gt;从 new 到 old 遍历所有的 inactive memtable,获取。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;h2 id=&#34;磁盘部分&#34;&gt;磁盘部分&lt;/h2&gt;&#xA;&lt;h3 id=&#34;compact-信号量&#34;&gt;compact 信号量&lt;/h3&gt;&#xA;&lt;p&gt;二元信号量。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自动求导, 道阻且长</title>
      <link>https://da1sy-petals.vercel.app/posts/road-to-diff/</link>
      <pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/road-to-diff/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/Da1sypetals/Symars&#34;&gt;Symars &lt;/a&gt; Rust代码生成库和 &lt;a href=&#34;https://github.com/Da1sypetals/Raddy&#34;&gt;Raddy&lt;/a&gt; 自动求导库的来龙去脉&lt;/p&gt;&#xA;&lt;h2 id=&#34;故事的起因&#34;&gt;故事的起因：&lt;/h2&gt;&#xA;&lt;p&gt;前段时间读了一些物理模拟的论文，想尝试复现一下。下手点先选了 &lt;a href=&#34;https://graphics.pixar.com/library/StableElasticity/paper.pdf&#34;&gt;stable neo hookean flesh simulation&lt;/a&gt;，但是选了什么并不重要。重要的是，所谓“现代”的物理模拟很多是隐式模拟，需要用牛顿法解一个优化问题。&lt;/p&gt;&#xA;&lt;p&gt;这之中就涉及到了：对能量的本构模型求导数（一阶梯度，二阶 hessian 矩阵）。这之中还涉及到从 &lt;em&gt;小而稠密&lt;/em&gt;  的 hessian 子矩阵组装成 &lt;em&gt;大而稀疏&lt;/em&gt; 的完整 hessian。这是一个精细活，一不小心就会出现极其难以排查的 bug。&lt;/p&gt;&#xA;&lt;p&gt;从 &lt;a href=&#34;https://www.tkim.graphics/DYNAMIC_DEFORMABLES/&#34;&gt;&lt;em&gt;Dynamic Deformables&lt;/em&gt;&lt;/a&gt; 这篇文章中可以看出推导这个公式就要花不少功夫（就算是看懂论文里的 notation 也要好一会儿），于是我搜了搜更多东西，尝试寻找一些其他的解决方法：我不是很想在精细的 debug 上花很多时间。最终找到的解决方法有两种：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;求符号导数，然后进行代码生成；&lt;/li&gt;&#xA;&lt;li&gt;自动求导。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;找到的资料中，前者有 MATLAB 或者 SymPy，后者有 PyTorch 等深度学习库，和更适合的 &lt;a href=&#34;https://github.com/patr-schm/TinyAD&#34;&gt;TinyAD&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;为什么说更适合？因为深度学习库的求导是以tensor为单位的，但是我这里的求导需要以单个标量为单位，粒度不同，深度学习库可能会跑出完全没法看的帧率。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;但是一个致命的问题来了：上述工具都在 C++ 的工具链上，而我不会 C++（或者，我可能会一点点 C++，但是我不会 CMake，因此不会调包。）&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;我曾经花了三天尝试在项目里用上 Eigen，然后失败告终，还是技术水平太菜了。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;我只好换一门我比较熟悉的语言：Rust。这是一切罪恶的开始&amp;hellip;&lt;/p&gt;&#xA;&lt;h2 id=&#34;一条看起来简单的路&#34;&gt;一条看起来简单的路&lt;/h2&gt;&#xA;&lt;p&gt;目前 Rust 还没有一个可以求二阶 hessian 的自动求导库（至少我在 crates.io 没搜到）。&lt;br&gt;&#xA;SymPy 目前还不能生成 Rust 代码（可以，但是有 bug）。&lt;br&gt;&#xA;考虑实现难度我先选了后者：从 SymPy 表达式生成 Rust 代码。于是有了 &lt;a href=&#34;https://github.com/Da1sypetals/Symars&#34;&gt;Symars&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;SymPy 提供的访问符号表达式的数据结构是树的形式，节点类型是运算符类型（&lt;code&gt;Add&lt;/code&gt;, &lt;code&gt;Mul&lt;/code&gt;, &lt;code&gt;Div&lt;/code&gt;, &lt;code&gt;Sin&lt;/code&gt;, 等等）或者常数/符号，节点的孩子是 operand 操作数。实现代码生成的思路就是按深度优先遍历树，得到孩子的表达式，然后再根据节点类型得到当前节点的表达式。边界条件是当前节点是常数，或者符号。&lt;/p&gt;</description>
    </item>
    <item>
      <title>共轭梯度：一种高中解析几何的视角</title>
      <link>https://da1sy-petals.vercel.app/posts/conj-grad/</link>
      <pubDate>Sat, 07 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/posts/conj-grad/</guid>
      <description>&lt;p&gt;本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 &lt;a href=&#34;https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf&#34;&gt;CMU 的教案&lt;/a&gt;及其&lt;a href=&#34;https://flat2010.github.io/2018/10/26/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E9%80%9A%E4%BF%97%E8%AE%B2%E4%B9%89/#8-%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95&#34;&gt;翻译&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;h2 id=&#34;1-问题&#34;&gt;1. 问题&lt;/h2&gt;&#xA;&lt;p&gt;对于实对称矩阵 $A \in \mathbb{R}^{n \times n}$ 和向量 $b \in \mathbb{R}^n$，求解&lt;/p&gt;&#xA;$$Ax = b$$&lt;p&gt;或者，等价的，&lt;/p&gt;&#xA;$$\text{argmin}_x f(x)$$&lt;p&gt;其中&lt;/p&gt;&#xA;$$f(x) = \frac{1}{2}x^T A x - b^T x$$&lt;h2 id=&#34;2-预备知识&#34;&gt;2. 预备知识&lt;/h2&gt;&#xA;&lt;h3 id=&#34;21-从高中学的二级结论说起&#34;&gt;2.1. 从高中学的二级结论说起&lt;/h3&gt;&#xA;&lt;p&gt;高中的时候我们学过椭圆：&lt;/p&gt;&#xA;$$a^{-2}x^2 + b^{-2}y^2 = 1$$&lt;p&gt;如果你记性好的话，你应该记得这个二级结论：&lt;/p&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/2025-10-03-00-44-15.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。&lt;/p&gt;&#xA;&lt;h3 id=&#34;22-最速下降法&#34;&gt;2.2. 最速下降法&lt;/h3&gt;&#xA;&lt;p&gt;首先，你应该知道梯度下降法：&lt;/p&gt;&#xA;$$x_{i+1} = x_i - \alpha\nabla f(x_i)$$&lt;p&gt;最速下降法就是在梯度下降法的基础上，选择 $\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：&lt;/p&gt;&#xA;$$\alpha^* = \text{argmin}_\alpha f(x_i - \alpha\nabla f(x_i))$$&lt;h2 id=&#34;3-共轭梯度法&#34;&gt;3. 共轭梯度法&lt;/h2&gt;&#xA;&lt;h3 id=&#34;31-记号&#34;&gt;3.1. 记号&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;$x_i$：第 $i$ 次循环之后的 $x$ 向量&lt;/p&gt;</description>
    </item>
    <item>
      <title>关于我</title>
      <link>https://da1sy-petals.vercel.app/about/</link>
      <pubDate>Tue, 01 Jun 2004 00:00:00 +0000</pubDate>
      <guid>https://da1sy-petals.vercel.app/about/</guid>
      <description>&lt;h2 id=&#34;个人信息&#34;&gt;个人信息&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;名称：黛西&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Nickname: Da1sypetals&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Email: &lt;code&gt;da1sypetals.iota@gmail.com&lt;/code&gt;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;爱好&#34;&gt;爱好&lt;/h2&gt;&#xA;&lt;p&gt;唱&lt;strong&gt;古风歌&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;比如, 我会唱这些：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《人间不值得》《楚歌起》 黄诗扶&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《迟迟》《腐草为萤》 银临&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《故事外的人》 慕寒&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《惊鹊》《心上秋》 忘川风华录&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《泼墨漓江》 泠鸢yousa&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《敢归云间宿》 三无Marblue&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《忘川》《霁夜茶》 小曲儿&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《松烟入墨》《如是我闻》 Winky诗&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《悦神》 KBShinya&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《第三十八年夏至》《永定四十年》 河图&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;《东风志》 Aki阿杰&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://singings.netlify.app&#34;&gt;这里&lt;/a&gt;还有很多&amp;hellip;&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
