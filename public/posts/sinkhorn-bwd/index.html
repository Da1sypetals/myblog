<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>不通过反转正向传播的方式计算sinkhorn迭代的梯度 | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="https://da1sy-petals.vercel.app/posts/sinkhorn-bwd/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="不通过反转正向传播的方式计算sinkhorn迭代的梯度">
  <meta property="og:description" content="问题设定 问题 注：$\odot$ 代表逐元素乘法。
输入矩阵: $X \in \mathbb{R}^{n \times n}$。
$P = \exp(X)$（element-wise）。
通过对 $P$ 进行 Sinkhorn-knopp迭代，得到bistochastic matrix $R = \text{diag}(\alpha) P \text{diag}(\beta)$，其中 $\alpha, \beta \in \mathbb{R}^n_{&gt;0}$ 是缩放因子，满足：
行和约束：$R \mathbf{1} = \mathbf{1} \implies \alpha \odot (P\beta) = \mathbf{1}$ 列和约束：$R^T \mathbf{1} = \mathbf{1} \implies \beta \odot (P^T \alpha) = \mathbf{1}$ 损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。
目标 $L$ 对 $X$ 的梯度：$\frac{\partial L}{\partial X}$。
TLDR 通过使用CG方法求解下列方程：">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-05T22:07:08+08:00">
    <meta property="article:modified_time" content="2026-01-05T22:07:08+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="不通过反转正向传播的方式计算sinkhorn迭代的梯度">
  <meta name="twitter:description" content="问题设定 问题 注：$\odot$ 代表逐元素乘法。
输入矩阵: $X \in \mathbb{R}^{n \times n}$。
$P = \exp(X)$（element-wise）。
通过对 $P$ 进行 Sinkhorn-knopp迭代，得到bistochastic matrix $R = \text{diag}(\alpha) P \text{diag}(\beta)$，其中 $\alpha, \beta \in \mathbb{R}^n_{&gt;0}$ 是缩放因子，满足：
行和约束：$R \mathbf{1} = \mathbf{1} \implies \alpha \odot (P\beta) = \mathbf{1}$ 列和约束：$R^T \mathbf{1} = \mathbf{1} \implies \beta \odot (P^T \alpha) = \mathbf{1}$ 损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。
目标 $L$ 对 $X$ 的梯度：$\frac{\partial L}{\partial X}$。
TLDR 通过使用CG方法求解下列方程：">

      <link rel="stylesheet" href="/css/root.min.0e732b812b9751962e01a7c4798a1211cd5f8ac8abec7f99793fe306989e459f.css" integrity="sha256-DnMrgSuXUZYuAafEeYoSEc1fisir7H&#43;ZeT/jBpieRZ8=" crossorigin="anonymous">
      <link rel="stylesheet" href="/css/bundle.min.d95a325399a05b50fe47dcf35b8229b8a2a014fcee5435cfb28204c6ac335fc5.css" integrity="sha256-2VoyU5mgW1D&#43;R9zzW4IpuKKgFPzuVDXPsoIExqwzX8U=" crossorigin="anonymous">

      <script src="/js/bundle.cc8ae9952dbfb731affafabdf26e5c60a6910047ff59ccdeaf1daebaa26c8830.js" integrity="sha256-zIrplS2/tzGv&#43;vq98m5cYKaRAEf/Wczerx2uuqJsiDA=" crossorigin="anonymous"></script><script defer src="/js/search/flexsearch.compact.64594b125f7b78bdf4fa8316955922bbebb1cd6baef3f16654bfca20309f18f8.js" integrity="sha256-ZFlLEl97eL30&#43;oMWlVkiu&#43;uxzWuu8/FmVL/KIDCfGPg="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="https://da1sy-petals.vercel.app/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://daisy-songs.vercel.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://daisy-songs.vercel.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    <a id="theme-switcher" href="#">
<svg class="light-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 3V4M12 20V21M4 12H3M6.31412 6.31412L5.5 5.5M17.6859 6.31412L18.5 5.5M6.31412 17.69L5.5 18.5001M17.6859 17.69L18.5 18.5001M21 12H20M16 12C16 14.2091 14.2091 16 12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>

<svg class="dark-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
  </div>
</div></header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/posts/">
        <span itemprop="name">Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>不通过反转正向传播的方式计算sinkhorn迭代的梯度</h1><time class="dim" datetime="2026-01-05T22:07:08&#43;08:00">January 5, 2026</time></div>
  <section class="page-section"><h2 id="问题设定">问题设定</h2>
<h3 id="问题">问题</h3>
<blockquote>
<p>注：$\odot$ 代表逐元素乘法。</p>
</blockquote>
<ol>
<li>
<p>输入矩阵: $X \in \mathbb{R}^{n \times n}$。</p>
</li>
<li>
<p>$P = \exp(X)$（element-wise）。</p>
</li>
<li>
<p>通过对 $P$ 进行 Sinkhorn-knopp迭代，得到bistochastic matrix $R = \text{diag}(\alpha) P \text{diag}(\beta)$，其中 $\alpha, \beta \in \mathbb{R}^n_{>0}$ 是缩放因子，满足：</p>
<ul>
<li>行和约束：$R \mathbf{1} = \mathbf{1} \implies \alpha \odot (P\beta) = \mathbf{1}$</li>
<li>列和约束：$R^T \mathbf{1} = \mathbf{1} \implies \beta \odot (P^T \alpha) = \mathbf{1}$</li>
</ul>
</li>
<li>
<p>损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。</p>
</li>
</ol>
<h3 id="目标">目标</h3>
<p>$L$ 对 $X$ 的梯度：$\frac{\partial L}{\partial X}$。</p>
<h3 id="tldr">TLDR</h3>
<p>通过使用CG方法求解下列方程：</p>
$$\begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} (G \odot R) \mathbf{1} \\ (G \odot R)^T \mathbf{1} \end{bmatrix}$$<p>可以得到 $L$ 对 $X$ 的梯度：
</p>
$$\nabla_X L = (G - u \mathbf{1}^T - \mathbf{1} v^T) \odot R$$<p>在前向sinkhorn-knopp迭代充分收敛的条件下，该方法能够收敛。</p>
<h2 id="求解">求解</h2>
<p>我们的目标是求 $\frac{\partial L}{\partial X}$。根据链式法则：</p>
$$\frac{\partial L}{\partial X} = \frac{\partial L}{\partial R} \cdot \frac{\partial R}{\partial P} \cdot \frac{\partial P}{\partial X}$$<p>由于 $P_{ij} = e^{X_{ij}} \implies \frac{\partial P_{ij}}{\partial X_{ij}} = P_{ij}$, 若能求出 $\frac{\partial L}{\partial P}$，最终结果就是 $\nabla_X L = \nabla_P L \odot P$。</p>
<p>通过对 Sinkhorn 的平衡条件进行隐函数求导，可以证明得到 $\nabla_X L$ 的计算公式如下(过程略&hellip;&hellip;):</p>
$$\nabla_X L = (G - u \mathbf{1}^T - \mathbf{1} v^T) \odot R$$<p>其中 $u, v \in \mathbb{R}^n$ 是下列线性系统的解, 等号右边分别是$G \odot R$ 的<em>行和</em>和<em>列和</em>：</p>
$$\begin{cases} u + R v = (G \odot R) \mathbf{1} \\ R^T u + v = (G \odot R)^T \mathbf{1} \end{cases}$$<h2 id="具体实现步骤">具体实现步骤</h2>
<h3 id="1求解线性系统">1）求解线性系统</h3>
<p>将上述方程改写成矩阵形式：</p>
$$\begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} (G \odot R) \mathbf{1} \\ (G \odot R)^T \mathbf{1} \end{bmatrix} = b$$<p>求解上述线性系统，得到 $u$ 和 $v$。</p>
<h3 id="2组装梯度">2）组装梯度</h3>
<p>得到 $u$ 和 $v$ 后，代入：</p>
$$\frac{\partial L}{\partial X_{ij}} = (G_{ij} - u_i - v_j) R_{ij} = (G_{ij} - (u_i + v_j)) R_{ij}$$<p>对于每一个$i,j$，我们需要从上述方程中解得的就是$u_i + v_j$。</p>
<h2 id="问题-1">问题</h2>
<p>记$A=\begin{bmatrix} I & R \\ R^T & I \end{bmatrix}$。</p>
<h3 id="1-多解">1. 多解</h3>
<p>证明：</p>
<p>考虑非零向量 $w = \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix}$（其中 $\mathbf{1}$ 为全 1 的 $n$ 维列向量）。
计算 $Aw$：</p>
$$Aw = \begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \begin{bmatrix} I\mathbf{1} - R\mathbf{1} \\ R^T\mathbf{1} - I\mathbf{1} \end{bmatrix}$$<p>根据bistochastic matrix性质 $R\mathbf{1} = \mathbf{1}$ 和 $R^T\mathbf{1} = \mathbf{1}$：</p>
$$Aw = \begin{bmatrix} \mathbf{1} - \mathbf{1} \\ \mathbf{1} - \mathbf{1} \end{bmatrix} = \mathbf{0}$$<p>由于存在非零向量在 $A$ 的零空间（Null space）中，故 $\det(A) = 0$。</p>
<p>直观理解：bistochastic matrix的行和列和存在冗余（例如，行和为1，因此每行其实只需知道前n-1个元素）。</p>
<h3 id="2-不变量">2. 不变量</h3>
<h4 id="1-线性方程组--的解空间">1) 线性方程组 $Ax = b$ 的解空间</h4>
<p>由于 $A$ 是奇异的，对于给定的向量 $b$，如果方程有解，则必有无穷多解。其通解形式为：</p>
$$x = \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} u_0 \\ v_0 \end{bmatrix} + k \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \begin{bmatrix} u_0 + k\mathbf{1} \\ v_0 - k\mathbf{1} \end{bmatrix}$$<p>其中 $\begin{bmatrix} u_0 \\ v_0 \end{bmatrix}$ 是一个特解，$k$ 是任意实数标量。</p>
<h4 id="2-不变量-1">2) 不变量</h4>
<p>虽然解 $x$ 包含不确定的偏移量 $k$，但我们的计算目标是确定的。
我们的计算目标是矩阵 $M$，定义为：</p>
$$M = u\mathbf{1}^T + \mathbf{1}v^T \quad (\text{即 } M_{ij} = u_i + v_j)$$<p>
证明唯一性：
将含有自由变量 $k$ 的通解代入 $M$ 的表达式：</p>
$$M(k) = (u_0 + k\mathbf{1})\mathbf{1}^T + \mathbf{1}(v_0 - k\mathbf{1})^T$$<p>利用矩阵分配律展开：</p>
$$M(k) = u_0\mathbf{1}^T + k(\mathbf{1}\mathbf{1}^T) + \mathbf{1}v_0^T - k(\mathbf{1}\mathbf{1}^T)$$<p>消去 $k$ 相关项：</p>
$$M(k) = u_0\mathbf{1}^T + \mathbf{1}v_0^T = M_{\text{fixed}}$$<p>
结论：
对于 $Ax=b$ 的任何解 $x$，由它们计算得到的矩阵 $M$ 是确定的。即：</p>
$$M = f(R, b)$$<p>$M$ 是 $R$ 和 $b$ 的确定函数，不受具体解的影响；因此只要求解能收敛，就可以计算出正确的梯度。</p>
<h2 id="收敛性">收敛性</h2>
<p>我们采取共轭梯度法求解这个线性系统。</p>
<h3 id="1特征值">1）特征值</h3>
<p>若 $R$ 的奇异值为 $\sigma_1, \sigma_2, \dots, \sigma_n$，则 $A$ 的特征值为 $1 \pm \sigma_i$。</p>
<p>证明：设 $\mu$ 是 $A$ 的特征值，对应的特征向量为 $z = \begin{bmatrix} p \\ q \end{bmatrix}$，其中 $p, q \in \mathbb{R}^n$。</p>
<p>则有方程组：</p>
<ul>
<li>
<p>$p + Rq = \mu p \implies Rq = (\mu - 1)p$</p>
</li>
<li>
<p>$R^Tp + q = \mu q \implies R^Tp = (\mu - 1)q$</p>
</li>
</ul>
<p>将 (2) 代入 (1) 可得：</p>
$$\dfrac{RR^Tp}{\mu - 1} = (\mu - 1)p \implies RR^T p = (\mu - 1)^2 p$$<p>这表明 $(\mu - 1)^2$ 是矩阵 $RR^T$ 的特征值。根据奇异值分解的定义，$RR^T$ 的特征值正是 $R$ 的奇异值的平方 $\sigma_i^2$。</p>
<p>因此：</p>
$$(\mu - 1)^2 = \sigma_i^2 \implies \mu - 1 = \pm \sigma_i \implies \mu = 1 \pm \sigma_i$$<h3 id="2对称半正定性">2）对称半正定性</h3>
<p>$A$ 是对称半正定矩阵（Positive Semidefinite）。对称性是显然的。</p>
<p>$A$ 的特征值为 $1 \pm \sigma_i$，因为 $R$ 是bistochastic matrix，根据 Perron-Frobenius定理, 其最大奇异值 $\sigma_{\max}(R) = 1$。</p>
<p>由于所有奇异值 $\sigma_i$ 满足 $0 \le \sigma_i \le 1$，则：</p>
<ul>
<li>
<p>最大特征值 $\mu_{\max} = 1 + \sigma_1 = 1 + 1 = 2$。</p>
</li>
<li>
<p>最小特征值 $\mu_{\min} = 1 - \sigma_1 = 1 - 1 = 0$。</p>
</li>
</ul>
<p>因为所有特征值 $\mu_i \ge 0$，所以 $A$ 是半正定的。</p>
<h3 id="3相容性">3）相容性</h3>
<p>共轭梯度法在处理奇异的对称半正定矩阵时，<a href="https://arxiv.org/pdf/1809.00793">只有满足下列条件才会收敛到解</a>：</p>
$$b \in \mathcal{R}(A) \iff b \perp \mathcal{N}(A)$$<p>其中 $\mathcal{N}(A)$ 是 $A$ 的零空间（Null Space）；这个条件被称为相容性条件。</p>
<p>针对该矩阵的相容性条件：</p>
<p>由于 $R$ 是bistochastic matrix，我们已知 $A \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \mathbf{0}$。这意味着 $\begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix}$ 在零空间内。</p>
<p>设 $b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}$（$b_1, b_2 \in \mathbb{R}^n$），则根据相容性条件，</p>
$$\begin{bmatrix} b_1 \\ b_2 \end{bmatrix}^T \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = 0 \implies \sum_{i=1}^n (b_1)_i = \sum_{j=1}^n (b_2)_j$$<p>而 $b_1 = (G \odot R) \mathbf{1}$，$b_2 = (G \odot R)^T \mathbf{1}$，因此</p>
$$\sum_{i=1}^n (b_1)_i = \sum_{j=1}^n (b_2)_j = \sum_{i=1}^n \sum_{j=1}^n G_{ij} R_{ij}$$<p>满足相容性条件，算法应当收敛。</p>
<h2 id="torch实现">torch实现</h2>
<p>特别鸣谢Gemini的辅助编程。</p>
<p>cuTile的示例实现在<a href="https://gist.github.com/Da1sypetals/e9886cd679b32920100656d7a3dee79b">这里</a>.</p>
<blockquote>
<p>注：要确保正向sinkhorn充分收敛才能使用此方法；正向收敛不充分的情况下，和自动求导的结果对比会出现很大偏差。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> icecream <span style="color:#f92672">import</span> ic
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> einops <span style="color:#66d9ef">as</span> ein
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>float32
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>batch <span style="color:#f92672">=</span> <span style="color:#ae81ff">25001</span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>n <span style="color:#e6db74">= }</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>iters <span style="color:#e6db74">= }</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Fix torch seed</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># torch.manual_seed(0)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sinkhorn_forward</span>(M, iters<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>):
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(M)
</span></span><span style="display:flex;"><span>    R <span style="color:#f92672">=</span> P
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(iters):
</span></span><span style="display:flex;"><span>        R <span style="color:#f92672">=</span> R <span style="color:#f92672">/</span> R<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        R <span style="color:#f92672">=</span> R <span style="color:#f92672">/</span> R<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> R, P
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_cg_solve</span>(R, b):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Solve the system Ax = b using the Conjugate Gradient (CG) method.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The matrix A is structured as:
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    A = [[I,   R ],
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">         [R^T, I ]]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    batch_size, n, _ <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>    device <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>    dtype <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>dtype
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 1. Construct the complete 2n x 2n matrix A</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create identity matrix I</span>
</span></span><span style="display:flex;"><span>    eye <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(n, device<span style="color:#f92672">=</span>device, dtype<span style="color:#f92672">=</span>dtype)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>expand(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Concatenate blocks to form A</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># top: [I, R]</span>
</span></span><span style="display:flex;"><span>    top <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([eye, R], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># bottom: [R^T, I]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use einsum &#39;bij-&gt;bji&#39; for transpose</span>
</span></span><span style="display:flex;"><span>    R_T <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;bij-&gt;bji&#34;</span>, R)
</span></span><span style="display:flex;"><span>    bottom <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([R_T, eye], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># A shape: (batch, 2n, 2n)</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([top, bottom], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 2. CG Initialization</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initial guess x0 = 0, shape (batch, 2n)</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros_like(b)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initial residual r0 = b - A@x0 = b</span>
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> b<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Initial search direction p0 = r0</span>
</span></span><span style="display:flex;"><span>    p <span style="color:#f92672">=</span> r<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># rs_old = r^T * r (dot product per batch)</span>
</span></span><span style="display:flex;"><span>    rs_old <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;bi,bi-&gt;b&#34;</span>, r, r)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    max_iter <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 3. CG Iteration Loop</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(max_iter):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate Ap = A @ p</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># &#39;bij,bj-&gt;bi&#39; performs batch matrix-vector multiplication</span>
</span></span><span style="display:flex;"><span>        Ap <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;bij,bj-&gt;bi&#34;</span>, A, p)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate step size alpha = (r^T * r) / (p^T * A * p)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># pAp is the dot product of p and Ap per batch</span>
</span></span><span style="display:flex;"><span>        pAp <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;bi,bi-&gt;b&#34;</span>, p, Ap)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># alpha = rs_old / pAp</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Avoid division by zero here is very important</span>
</span></span><span style="display:flex;"><span>        alpha <span style="color:#f92672">=</span> rs_old <span style="color:#f92672">/</span> (pAp <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Update solution x = x + alpha * p</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># &#39;b,bi-&gt;bi&#39; scales each vector in the batch by its corresponding alpha</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">+=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;b,bi-&gt;bi&#34;</span>, alpha, p)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Update residual r = r - alpha * Ap</span>
</span></span><span style="display:flex;"><span>        r <span style="color:#f92672">-=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;b,bi-&gt;bi&#34;</span>, alpha, Ap)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate new residual inner product</span>
</span></span><span style="display:flex;"><span>        rs_new <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;bi,bi-&gt;b&#34;</span>, r, r)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate beta = (r_new^T * r_new) / (r_old^T * r_old)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Avoid division by zero here is not so important experimentally</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># but it&#39;s good to have it</span>
</span></span><span style="display:flex;"><span>        beta <span style="color:#f92672">=</span> rs_new <span style="color:#f92672">/</span> (rs_old <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Update search direction p = r + beta * p</span>
</span></span><span style="display:flex;"><span>        p <span style="color:#f92672">=</span> r <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#34;b,bi-&gt;bi&#34;</span>, beta, p)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        rs_old <span style="color:#f92672">=</span> rs_new
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sinkhorn_backward_implicit</span>(grad_R, R):
</span></span><span style="display:flex;"><span>    R <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> grad_R)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape (n,)</span>
</span></span><span style="display:flex;"><span>    c <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> grad_R)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">2</span>)  <span style="color:#75715e"># shape (n,)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Build 2n x 2n system</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((batch, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n), dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A[:, :n, :n] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(n, dtype<span style="color:#f92672">=</span>dtype)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    A[:, :n, n:] <span style="color:#f92672">=</span> R
</span></span><span style="display:flex;"><span>    A[:, n:, :n] <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    A[:, n:, n:] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(n, dtype<span style="color:#f92672">=</span>dtype)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ic(torch<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svdvals(A))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([r, c], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ic(A<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    ic(b<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># sol = torch.linalg.solve(A, b)</span>
</span></span><span style="display:flex;"><span>    sol <span style="color:#f92672">=</span> batch_cg_solve(R, b)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    alpha <span style="color:#f92672">=</span> sol[:, :n]
</span></span><span style="display:flex;"><span>    beta <span style="color:#f92672">=</span> sol[:, n:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Gproj <span style="color:#f92672">=</span> grad_R <span style="color:#f92672">-</span> alpha<span style="color:#f92672">.</span>unsqueeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">-</span> beta<span style="color:#f92672">.</span>unsqueeze(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Gproj <span style="color:#f92672">*</span> R
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Variable</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>dist <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>distributions<span style="color:#f92672">.</span>uniform<span style="color:#f92672">.</span>Uniform(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">4.0</span>)
</span></span><span style="display:flex;"><span>M <span style="color:#f92672">=</span> dist<span style="color:#f92672">.</span>sample((batch, n, n))
</span></span><span style="display:flex;"><span>M<span style="color:#f92672">.</span>requires_grad_()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Shared forward + one shared loss weight</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>R, P <span style="color:#f92672">=</span> sinkhorn_forward(M, iters)
</span></span><span style="display:flex;"><span>loss_weight <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(R)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Method A: Autograd</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>loss_a <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> loss_weight)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>loss_a<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>grad_M_autograd <span style="color:#f92672">=</span> M<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Method B: Implicit differentiation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>grad_R <span style="color:#f92672">=</span> loss_weight
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># KL pullback:</span>
</span></span><span style="display:flex;"><span>grad_M_implicit <span style="color:#f92672">=</span> sinkhorn_backward_implicit(grad_R, R)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compare</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>g1 <span style="color:#f92672">=</span> grad_M_autograd
</span></span><span style="display:flex;"><span>g2 <span style="color:#f92672">=</span> grad_M_implicit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>abs_diff <span style="color:#f92672">=</span> (g1 <span style="color:#f92672">-</span> g2)<span style="color:#f92672">.</span>abs()
</span></span><span style="display:flex;"><span>rel_diff <span style="color:#f92672">=</span> abs_diff <span style="color:#f92672">/</span> (g1<span style="color:#f92672">.</span>abs() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Comparison of gradients dL/dM&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;--------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">format_list</span>(ls):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>x<span style="color:#e6db74">:</span><span style="color:#e6db74">.2e</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span> <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> ls]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>MAE <span style="color:#f92672">=</span> abs_diff<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>max_abs_diff <span style="color:#f92672">=</span> abs_diff<span style="color:#f92672">.</span>reshape(batch, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>max(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>mean_rel_diff <span style="color:#f92672">=</span> rel_diff<span style="color:#f92672">.</span>mean(dim<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>))<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>max_rel_diff <span style="color:#f92672">=</span> rel_diff<span style="color:#f92672">.</span>reshape(batch, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>max(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;MAE: </span><span style="color:#e6db74">{</span>format_list(MAE)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;max_abs_diff: </span><span style="color:#e6db74">{</span>format_list(max_abs_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;mean_rel_diff: </span><span style="color:#e6db74">{</span>format_list(mean_rel_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;max_rel_diff: </span><span style="color:#e6db74">{</span>format_list(max_rel_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Max MAE = </span><span style="color:#e6db74">{</span>max(MAE)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Max max_abs_diff = </span><span style="color:#e6db74">{</span>max(max_abs_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Max mean_rel_diff = </span><span style="color:#e6db74">{</span>max(mean_rel_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Max max_rel_diff = </span><span style="color:#e6db74">{</span>max(max_rel_diff)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Grad (autograd) sample:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, g1[<span style="color:#ae81ff">0</span>, :<span style="color:#ae81ff">3</span>, :<span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Grad (implicit) sample:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, g2[<span style="color:#ae81ff">0</span>, :<span style="color:#ae81ff">3</span>, :<span style="color:#ae81ff">3</span>])
</span></span></code></pre></div></section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2026 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>1379 words</span>
    <span>11 - 15 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#问题设定">问题设定</a>
      <ul>
        <li><a href="#问题">问题</a></li>
        <li><a href="#目标">目标</a></li>
        <li><a href="#tldr">TLDR</a></li>
      </ul>
    </li>
    <li><a href="#求解">求解</a></li>
    <li><a href="#具体实现步骤">具体实现步骤</a>
      <ul>
        <li><a href="#1求解线性系统">1）求解线性系统</a></li>
        <li><a href="#2组装梯度">2）组装梯度</a></li>
      </ul>
    </li>
    <li><a href="#问题-1">问题</a>
      <ul>
        <li><a href="#1-多解">1. 多解</a></li>
        <li><a href="#2-不变量">2. 不变量</a></li>
      </ul>
    </li>
    <li><a href="#收敛性">收敛性</a>
      <ul>
        <li><a href="#1特征值">1）特征值</a></li>
        <li><a href="#2对称半正定性">2）对称半正定性</a></li>
        <li><a href="#3相容性">3）相容性</a></li>
      </ul>
    </li>
    <li><a href="#torch实现">torch实现</a></li>
  </ul>
</nav></aside></div>
  </div>
</body>

</html>