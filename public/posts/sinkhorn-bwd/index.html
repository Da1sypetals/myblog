<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>不通过反转正向传播的方式计算sinkhorn迭代的梯度 | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="https://da1sy-petals.vercel.app/posts/sinkhorn-bwd/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="不通过反转正向传播的方式计算sinkhorn迭代的梯度">
  <meta property="og:description" content="变量定义与前向过程 注：$\odot$ 代表逐元素乘法。
输入矩阵: $X \in \mathbb{R}^{n \times n}$。 指数化: $P = \exp(X)$（逐元素指数）。 Sinkhorn 结果: 得到双随机矩阵 $R = \text{diag}(u) P \text{diag}(v)$，其中 $u, v \in \mathbb{R}^n_{&gt;0}$ 是缩放因子，满足： 行和约束：$R \mathbf{1} = \mathbf{1} \implies u \odot (Pv) = \mathbf{1}$ 列和约束：$R^T \mathbf{1} = \mathbf{1} \implies v \odot (P^T u) = \mathbf{1}$ 损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。 隐函数微分推导 我们的目标是求 $\frac{\partial L}{\partial X}$。根据链式法则：
$$\frac{\partial L}{\partial X} = \frac{\partial L}{\partial R} \cdot \frac{\partial R}{\partial P} \cdot \frac{\partial P}{\partial X}$$由于 $P_{ij} = e^{X_{ij}}$，我们知道 $\frac{\partial P_{ij}}{\partial X_{ij}} = P_{ij}$。因此，若能求出 $\frac{\partial L}{\partial P}$，最终结果就是 $\nabla_X L = \nabla_P L \odot P$。">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2026-01-05T22:07:08+08:00">
    <meta property="article:modified_time" content="2026-01-05T22:07:08+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="不通过反转正向传播的方式计算sinkhorn迭代的梯度">
  <meta name="twitter:description" content="变量定义与前向过程 注：$\odot$ 代表逐元素乘法。
输入矩阵: $X \in \mathbb{R}^{n \times n}$。 指数化: $P = \exp(X)$（逐元素指数）。 Sinkhorn 结果: 得到双随机矩阵 $R = \text{diag}(u) P \text{diag}(v)$，其中 $u, v \in \mathbb{R}^n_{&gt;0}$ 是缩放因子，满足： 行和约束：$R \mathbf{1} = \mathbf{1} \implies u \odot (Pv) = \mathbf{1}$ 列和约束：$R^T \mathbf{1} = \mathbf{1} \implies v \odot (P^T u) = \mathbf{1}$ 损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。 隐函数微分推导 我们的目标是求 $\frac{\partial L}{\partial X}$。根据链式法则：
$$\frac{\partial L}{\partial X} = \frac{\partial L}{\partial R} \cdot \frac{\partial R}{\partial P} \cdot \frac{\partial P}{\partial X}$$由于 $P_{ij} = e^{X_{ij}}$，我们知道 $\frac{\partial P_{ij}}{\partial X_{ij}} = P_{ij}$。因此，若能求出 $\frac{\partial L}{\partial P}$，最终结果就是 $\nabla_X L = \nabla_P L \odot P$。">

      <link rel="stylesheet" href="/css/root.min.0e732b812b9751962e01a7c4798a1211cd5f8ac8abec7f99793fe306989e459f.css" integrity="sha256-DnMrgSuXUZYuAafEeYoSEc1fisir7H&#43;ZeT/jBpieRZ8=" crossorigin="anonymous">
      <link rel="stylesheet" href="/css/bundle.min.d95a325399a05b50fe47dcf35b8229b8a2a014fcee5435cfb28204c6ac335fc5.css" integrity="sha256-2VoyU5mgW1D&#43;R9zzW4IpuKKgFPzuVDXPsoIExqwzX8U=" crossorigin="anonymous">

      <script src="/js/bundle.cc8ae9952dbfb731affafabdf26e5c60a6910047ff59ccdeaf1daebaa26c8830.js" integrity="sha256-zIrplS2/tzGv&#43;vq98m5cYKaRAEf/Wczerx2uuqJsiDA=" crossorigin="anonymous"></script><script defer src="/js/search/flexsearch.compact.64594b125f7b78bdf4fa8316955922bbebb1cd6baef3f16654bfca20309f18f8.js" integrity="sha256-ZFlLEl97eL30&#43;oMWlVkiu&#43;uxzWuu8/FmVL/KIDCfGPg="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="https://da1sy-petals.vercel.app/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    <a id="theme-switcher" href="#">
<svg class="light-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 3V4M12 20V21M4 12H3M6.31412 6.31412L5.5 5.5M17.6859 6.31412L18.5 5.5M6.31412 17.69L5.5 18.5001M17.6859 17.69L18.5 18.5001M21 12H20M16 12C16 14.2091 14.2091 16 12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>

<svg class="dark-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
  </div>
</div></header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/posts/">
        <span itemprop="name">Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>不通过反转正向传播的方式计算sinkhorn迭代的梯度</h1><time class="dim" datetime="2026-01-05T22:07:08&#43;08:00">January 5, 2026</time></div>
  <section class="page-section"><h2 id="变量定义与前向过程">变量定义与前向过程</h2>
<blockquote>
<p>注：$\odot$ 代表逐元素乘法。</p>
</blockquote>
<ol>
<li>输入矩阵: $X \in \mathbb{R}^{n \times n}$。</li>
<li>指数化: $P = \exp(X)$（逐元素指数）。</li>
<li>Sinkhorn 结果: 得到双随机矩阵 $R = \text{diag}(u) P \text{diag}(v)$，其中 $u, v \in \mathbb{R}^n_{>0}$ 是缩放因子，满足：
<ul>
<li>行和约束：$R \mathbf{1} = \mathbf{1} \implies u \odot (Pv) = \mathbf{1}$</li>
<li>列和约束：$R^T \mathbf{1} = \mathbf{1} \implies v \odot (P^T u) = \mathbf{1}$</li>
</ul>
</li>
<li>损失函数: $L = f(R)$，令 $G = \nabla_R L = \frac{\partial L}{\partial R}$ 为已知梯度。</li>
</ol>
<h2 id="隐函数微分推导">隐函数微分推导</h2>
<p>我们的目标是求 $\frac{\partial L}{\partial X}$。根据链式法则：</p>
$$\frac{\partial L}{\partial X} = \frac{\partial L}{\partial R} \cdot \frac{\partial R}{\partial P} \cdot \frac{\partial P}{\partial X}$$<p>由于 $P_{ij} = e^{X_{ij}}$，我们知道 $\frac{\partial P_{ij}}{\partial X_{ij}} = P_{ij}$。因此，若能求出 $\frac{\partial L}{\partial P}$，最终结果就是 $\nabla_X L = \nabla_P L \odot P$。</p>
<p>通过对 Sinkhorn 的平衡条件进行隐函数求导，可以证明 $\nabla_X L$ 的计算公式如下：
最终梯度公式：</p>
$$\nabla_X L = (G - u \mathbf{1}^T - \mathbf{1} v^T) \odot R$$<p>其中 $u, v \in \mathbb{R}^n$ 是下列线性系统的解：</p>
$$\begin{cases} u + R v = (G \odot R) \mathbf{1} \\ R^T u + v = (G \odot R)^T \mathbf{1} \end{cases}$$<h2 id="具体实现步骤">具体实现步骤</h2>
<h3 id="1求解线性系统">1）求解线性系统</h3>
<p>将上述方程改写成矩阵形式：</p>
$$\begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} (G \odot R) \mathbf{1} \\ (G \odot R)^T \mathbf{1} \end{bmatrix}$$<p>求解上述线性系统，得到 $u$ 和 $v$。</p>
<h3 id="2组装梯度">2）组装梯度</h3>
<p>得到 $u$ 和 $v$ 后，代入：</p>
$$\frac{\partial L}{\partial X_{ij}} = (G_{ij} - u_i - v_j) R_{ij} = (G_{ij} - (u_i + v_j)) R_{ij}$$<p>对于每一个$i,j$，我们需要从上述方程中解得的就是$u_i + v_j$。</p>
<h2 id="问题">问题：</h2>
<p>记$A=\begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} u \\ v \end{bmatrix}$。</p>
<h3 id="1-a是奇异矩阵因此-x-有多解">1. A是奇异矩阵，因此 x 有多解</h3>
<p>证明：</p>
<p>考虑非零向量 $v = \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix}$（其中 $\mathbf{1}$ 为全 1 的 $n$ 维列向量）。
计算 $Av$：</p>
$$Av = \begin{bmatrix} I & R \\ R^T & I \end{bmatrix} \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \begin{bmatrix} I\mathbf{1} - R\mathbf{1} \\ R^T\mathbf{1} - I\mathbf{1} \end{bmatrix}$$<p>根据双随机矩阵性质 $R\mathbf{1} = \mathbf{1}$ 和 $R^T\mathbf{1} = \mathbf{1}$：</p>
$$Av = \begin{bmatrix} \mathbf{1} - \mathbf{1} \\ \mathbf{1} - \mathbf{1} \end{bmatrix} = \mathbf{0}$$<p>由于存在非零向量在 $A$ 的零空间（Null space）中，故 $\det(A) = 0$。</p>
<p>直观理解：双随机矩阵的行和列和存在冗余（例如，行和为1，因此每行其实只需知道前n-1个元素）。</p>
<h3 id="2-即使x有多解我们的计算目标也是确定的">2. 即使x有多解，我们的计算目标也是确定的</h3>
<h4 id="1-线性方程组--的解空间">1) 线性方程组 $Ax = b$ 的解空间</h4>
<p>由于 $A$ 是奇异的，对于给定的向量 $b$，如果方程有解，则必有无穷多解。其通解形式为：</p>
$$x = \begin{bmatrix} u \\ v \end{bmatrix} = \begin{bmatrix} u_0 \\ v_0 \end{bmatrix} + k \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \begin{bmatrix} u_0 + k\mathbf{1} \\ v_0 - k\mathbf{1} \end{bmatrix}$$<p>其中 $\begin{bmatrix} u_0 \\ v_0 \end{bmatrix}$ 是一个特解，$k$ 是任意实数标量。</p>
<h4 id="2-不变量">2) 不变量</h4>
<p>虽然解 $x$ 包含不确定的偏移量 $k$，但我们的计算目标是确定的。
我们的计算目标是矩阵 $M$，定义为：</p>
$$M = u\mathbf{1}^T + \mathbf{1}v^T \quad (\text{即 } M_{ij} = u_i + v_j)$$<p>
证明唯一性：
将含有自由变量 $k$ 的通解代入 $M$ 的表达式：</p>
$$M(k) = (u_0 + k\mathbf{1})\mathbf{1}^T + \mathbf{1}(v_0 - k\mathbf{1})^T$$<p>利用矩阵分配律展开：</p>
$$M(k) = u_0\mathbf{1}^T + k(\mathbf{1}\mathbf{1}^T) + \mathbf{1}v_0^T - k(\mathbf{1}\mathbf{1}^T)$$<p>消去 $k$ 相关项：</p>
$$M(k) = u_0\mathbf{1}^T + \mathbf{1}v_0^T = M_{fixed}$$<p>
结论：
对于 $Ax=b$ 的任何解 $x$，由它们计算得到的矩阵 $M$ 是确定的。即：</p>
$$M = f(R, b)$$<p>$M$ 是 $R$ 和 $b$ 的确定函数，不受具体解的影响；因此只要求解能收敛，就可以计算出正确的梯度。</p>
<h2 id="收敛性">收敛性</h2>
<p>我们采取共轭梯度法求解这个线性系统。</p>
<h3 id="1a的特征值">1）A的特征值</h3>
<p>若 $R$ 的奇异值为 $\sigma_1, \sigma_2, \dots, \sigma_n$，则 $A$ 的特征值为 $1 \pm \sigma_i$。</p>
<p>证明：设 $\mu$ 是 $A$ 的特征值，对应的特征向量为 $\mathbf{x} = \begin{bmatrix} \mathbf{u} \\ \mathbf{v} \end{bmatrix}$，其中 $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$。</p>
<p>则有方程组：</p>
<ul>
<li>
<p>$\mathbf{u} + R\mathbf{v} = \mu \mathbf{u} \implies R\mathbf{v} = (\mu - 1)\mathbf{u}$</p>
</li>
<li>
<p>$R^T\mathbf{u} + \mathbf{v} = \mu \mathbf{v} \implies R^T\mathbf{u} = (\mu - 1)\mathbf{v}$</p>
</li>
</ul>
<p>将 (2) 代入 (1) 可得：</p>
$$\dfrac{RR^T\mathbf{u}}{\mu - 1} = (\mu - 1)\mathbf{u} \implies RR^T \mathbf{u} = (\mu - 1)^2 \mathbf{u}$$<p>这表明 $(\mu - 1)^2$ 是矩阵 $RR^T$ 的特征值。根据奇异值分解的定义，$RR^T$ 的特征值正是 $R$ 的奇异值的平方 $\sigma_i^2$。</p>
<p>因此：</p>
$$(\mu - 1)^2 = \sigma_i^2 \implies \mu - 1 = \pm \sigma_i \implies \mu = 1 \pm \sigma_i$$<h3 id="2对称半正定性">2）对称半正定性</h3>
<p>$A$ 是对称半正定矩阵（Positive Semidefinite）。对称性是显然的。</p>
<p>$A$ 的特征值为 $1 \pm \sigma_i$，因为 $R$ 是双随机矩阵，根据 Perron-Frobenius 定理或谱范数的性质，双随机矩阵的最大奇异值 $\sigma_{\max}(R) = 1$。</p>
<p>由于所有奇异值 $\sigma_i$ 满足 $0 \le \sigma_i \le 1$，则：</p>
<p>最大特征值 $\mu_{\max} = 1 + \sigma_1 = 1 + 1 = 2$。</p>
<p>最小特征值 $\mu_{\min} = 1 - \sigma_1 = 1 - 1 = 0$。</p>
<p>因为所有特征值 $\mu_i \ge 0$，所以 $A$ 是半正定的。</p>
<h3 id="3相容性">3）相容性</h3>
<p>共轭梯度法在处理奇异的对称半正定矩阵时，只有在方程组相容的情况下才会收敛到解。所谓相容，是指向量 $b$ 必须落在矩阵 $A$ 的列空间（即值域）内，数学表达为：</p>
$$b \in \mathcal{R}(A) \iff b \perp \mathcal{N}(A)$$<p>其中 $\mathcal{N}(A)$ 是 $A$ 的零空间（Null Space）。</p>
<p>针对该矩阵的相容性条件：</p>
<p>由于 $R$ 是双随机矩阵，我们已知 $A \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = \mathbf{0}$。这意味着 $\begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix}$ 在零空间内。</p>
<p>如果 $b = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}$（$b_1, b_2 \in \mathbb{R}^n$），则相容性要求：</p>
$$\begin{bmatrix} b_1 \\ b_2 \end{bmatrix}^T \begin{bmatrix} \mathbf{1} \\ -\mathbf{1} \end{bmatrix} = 0 \implies \sum_{i=1}^n (b_1)_i = \sum_{j=1}^n (b_2)_j$$<p>结论： 只有当 $b$ 的上半部分元素之和等于下半部分元素之和时，CG 法才能收敛到一个精确解。</p>
<p>对于我们的问题，$b_1 = (G \odot R) \mathbf{1}$，$b_2 = (G \odot R)^T \mathbf{1}$，因此</p>
$$\sum_{i=1}^n (b_1)_i = \sum_{j=1}^n (b_2)_j = \sum_{i=1}^n \sum_{j=1}^n G_{ij} R_{ij}$$<p>满足相容性条件，算法应当收敛。</p>
<h2 id="实现">实现</h2>
<p>特别鸣谢Gemini的辅助编程。这里直接调用了<code>torch.linalg.solve</code>，没有自己实现共轭梯度法。</p>
<blockquote>
<p>注：要确保正向sinkhorn充分收敛才能使用此方法；正向收敛不充分的情况下，和自动求导的结果对比会出现很大偏差。</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> icecream <span style="color:#f92672">import</span> ic
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dtype <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>float32
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>iters <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>n <span style="color:#e6db74">= }</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>iters <span style="color:#e6db74">= }</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sinkhorn_forward</span>(A, iters<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>):
</span></span><span style="display:flex;"><span>    P <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(A)
</span></span><span style="display:flex;"><span>    u <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(n, dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>    v <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones(n, dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(iters):
</span></span><span style="display:flex;"><span>        u <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (P <span style="color:#f92672">@</span> v)
</span></span><span style="display:flex;"><span>        v <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (P<span style="color:#f92672">.</span>t() <span style="color:#f92672">@</span> u)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    R <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>diag(u) <span style="color:#f92672">@</span> P <span style="color:#f92672">@</span> torch<span style="color:#f92672">.</span>diag(v)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> R, P, u, v
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sinkhorn_backward_implicit</span>(grad_R, R):
</span></span><span style="display:flex;"><span>    R <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>detach()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    r <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> grad_R)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  <span style="color:#75715e"># shape (n,)</span>
</span></span><span style="display:flex;"><span>    c <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> grad_R)<span style="color:#f92672">.</span>sum(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  <span style="color:#75715e"># shape (n,)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Build 2n x 2n system</span>
</span></span><span style="display:flex;"><span>    A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros((<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n, <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> n), dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    A[:n, :n] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(n, dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>    A[:n, n:] <span style="color:#f92672">=</span> R
</span></span><span style="display:flex;"><span>    A[n:, :n] <span style="color:#f92672">=</span> R<span style="color:#f92672">.</span>t()
</span></span><span style="display:flex;"><span>    A[n:, n:] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(n, dtype<span style="color:#f92672">=</span>dtype)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    ic(torch<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svdvals(A))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([r, c])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sol <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>solve(A, b)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    alpha <span style="color:#f92672">=</span> sol[:n]
</span></span><span style="display:flex;"><span>    beta <span style="color:#f92672">=</span> sol[n:]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Gproj <span style="color:#f92672">=</span> grad_R <span style="color:#f92672">-</span> alpha[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">-</span> beta[<span style="color:#66d9ef">None</span>, :]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Gproj <span style="color:#f92672">*</span> R
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Variable</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0.0</span>, <span style="color:#ae81ff">0.01</span>, size<span style="color:#f92672">=</span>(n, n), dtype<span style="color:#f92672">=</span>dtype, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Shared forward + one shared loss weight</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>R, P, u, v <span style="color:#f92672">=</span> sinkhorn_forward(A, iters)
</span></span><span style="display:flex;"><span>loss_weight <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(R)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Aethod A: Autograd</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>loss_a <span style="color:#f92672">=</span> (R <span style="color:#f92672">*</span> loss_weight)<span style="color:#f92672">.</span>sum()
</span></span><span style="display:flex;"><span>loss_a<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>grad_A_autograd <span style="color:#f92672">=</span> A<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Aethod B: Implicit differentiation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>grad_R <span style="color:#f92672">=</span> loss_weight
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># KL pullback:</span>
</span></span><span style="display:flex;"><span>grad_A_implicit <span style="color:#f92672">=</span> sinkhorn_backward_implicit(grad_R, R)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compare</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">######################################################################</span>
</span></span><span style="display:flex;"><span>g1 <span style="color:#f92672">=</span> grad_A_autograd
</span></span><span style="display:flex;"><span>g2 <span style="color:#f92672">=</span> grad_A_implicit
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>abs_diff <span style="color:#f92672">=</span> (g1 <span style="color:#f92672">-</span> g2)<span style="color:#f92672">.</span>abs()
</span></span><span style="display:flex;"><span>rel_diff <span style="color:#f92672">=</span> abs_diff <span style="color:#f92672">/</span> (g1<span style="color:#f92672">.</span>abs() <span style="color:#f92672">+</span> <span style="color:#ae81ff">1e-12</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Comparison of gradients dL/dA&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;--------------------------------&#34;</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;AAE           :&#34;</span>, abs_diff<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Aax abs diff  :&#34;</span>, abs_diff<span style="color:#f92672">.</span>max()<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Aean rel diff :&#34;</span>, rel_diff<span style="color:#f92672">.</span>mean()<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Aax rel diff  :&#34;</span>, rel_diff<span style="color:#f92672">.</span>max()<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Grad (autograd) sample:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, g1[:<span style="color:#ae81ff">3</span>, :<span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Grad (implicit) sample:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, g2[:<span style="color:#ae81ff">3</span>, :<span style="color:#ae81ff">3</span>])
</span></span></code></pre></div></section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2026 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>925 words</span>
    <span>9 - 11 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#变量定义与前向过程">变量定义与前向过程</a></li>
    <li><a href="#隐函数微分推导">隐函数微分推导</a></li>
    <li><a href="#具体实现步骤">具体实现步骤</a>
      <ul>
        <li><a href="#1求解线性系统">1）求解线性系统</a></li>
        <li><a href="#2组装梯度">2）组装梯度</a></li>
      </ul>
    </li>
    <li><a href="#问题">问题：</a>
      <ul>
        <li><a href="#1-a是奇异矩阵因此-x-有多解">1. A是奇异矩阵，因此 x 有多解</a></li>
        <li><a href="#2-即使x有多解我们的计算目标也是确定的">2. 即使x有多解，我们的计算目标也是确定的</a></li>
      </ul>
    </li>
    <li><a href="#收敛性">收敛性</a>
      <ul>
        <li><a href="#1a的特征值">1）A的特征值</a></li>
        <li><a href="#2对称半正定性">2）对称半正定性</a></li>
        <li><a href="#3相容性">3）相容性</a></li>
      </ul>
    </li>
    <li><a href="#实现">实现</a></li>
  </ul>
</nav></aside></div>
  </div>
</body>

</html>