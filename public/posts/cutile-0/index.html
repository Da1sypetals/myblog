<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>cuTile 历险记，第0集：心智模型 | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="https://da1sy-petals.vercel.app/posts/cutile-0/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="cuTile 历险记，第0集：心智模型">
  <meta property="og:description" content="首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他劫持捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。
因此在使用cuTile的时候，要一直告诉自己 “This is not Python”.
Abstraction Level 作为用户需要知道的：这门语言工作在哪个抽象层级。这里指的是这门语言提供的接口，并不直接对应硬件。cuTile的compiler magic会把我们写的代码map到硬件上，但这并不是写cuTile的程序员需要关心的。
内存 从逻辑上来说，cuTile暴露给用户的内存分为两种：
Global Memory (Gmem): 读写速度：慢 Cache: 读写速度：较快 编程模型 Global Array 存放在Gmem上
操作：只能进行Load(从Gmem读取到Cache)，以及Store(从Cache存入Gmem)。
来源：PyTorch tensor 可以直接传入。
Tile Array 存放在Cache上
操作：可以在上面进行数学操作如sin, mma等。
来源：tile kernel内创建(例如cuda.tile.zeros)，或者Global Array load得到
Immutable：在逻辑上，任何对Tile Array的计算操作都会返回新的Tile Array (Returns copies, not views)；你也不能直接对Tile Array里面的内容进行修改。
cuTile的compiler magic肯定会在内部防止冗余内存的创建，毕竟速度&gt;=SRAM的存储是如此昂贵；但是程序员是以immutable的形式编程的。 metadata: dtype, shape
layout对用户是不可见的，交由编译器处理。 示意图： 编程问题 问题输入 Tensor 级别的计算过程，比如：
Matmul&#43;activation Attention Mechanism 其他可以用NumPy/PyTorch这一级别的抽象所描述的算法。 cuTile是用来解决什么问题的 对于同一个算法，如何明智地加载数据、进行计算，可以减少Load/Store的数据量、增大计算密度，并且减少需要在Gmem上materialize的中间数据。
奇怪的想法 据nv的编译器工程师在各种talk里面所说，cuTile将会屏蔽所有硬件特异性的功能，交由编译器处理。如果是这样的话，其他硬件厂商是不是更方便在这一层级往下做？比如，直接开发一套rocm.tile，在接口上对标cuda.tile，然后实现自己的编译器，Lower到自己的具有硬件特异性的IR代码上进行优化。这样似乎就避免了triton目前推出越来越多nvidia定制的功能，导致其他硬件厂商无法跟进的问题。
参考 cuTile talk @ scipy cuTile talk @ torch">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-13T19:38:26+08:00">
    <meta property="article:modified_time" content="2025-11-13T19:38:26+08:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="cuTile 历险记，第0集：心智模型">
  <meta name="twitter:description" content="首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他劫持捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。
因此在使用cuTile的时候，要一直告诉自己 “This is not Python”.
Abstraction Level 作为用户需要知道的：这门语言工作在哪个抽象层级。这里指的是这门语言提供的接口，并不直接对应硬件。cuTile的compiler magic会把我们写的代码map到硬件上，但这并不是写cuTile的程序员需要关心的。
内存 从逻辑上来说，cuTile暴露给用户的内存分为两种：
Global Memory (Gmem): 读写速度：慢 Cache: 读写速度：较快 编程模型 Global Array 存放在Gmem上
操作：只能进行Load(从Gmem读取到Cache)，以及Store(从Cache存入Gmem)。
来源：PyTorch tensor 可以直接传入。
Tile Array 存放在Cache上
操作：可以在上面进行数学操作如sin, mma等。
来源：tile kernel内创建(例如cuda.tile.zeros)，或者Global Array load得到
Immutable：在逻辑上，任何对Tile Array的计算操作都会返回新的Tile Array (Returns copies, not views)；你也不能直接对Tile Array里面的内容进行修改。
cuTile的compiler magic肯定会在内部防止冗余内存的创建，毕竟速度&gt;=SRAM的存储是如此昂贵；但是程序员是以immutable的形式编程的。 metadata: dtype, shape
layout对用户是不可见的，交由编译器处理。 示意图： 编程问题 问题输入 Tensor 级别的计算过程，比如：
Matmul&#43;activation Attention Mechanism 其他可以用NumPy/PyTorch这一级别的抽象所描述的算法。 cuTile是用来解决什么问题的 对于同一个算法，如何明智地加载数据、进行计算，可以减少Load/Store的数据量、增大计算密度，并且减少需要在Gmem上materialize的中间数据。
奇怪的想法 据nv的编译器工程师在各种talk里面所说，cuTile将会屏蔽所有硬件特异性的功能，交由编译器处理。如果是这样的话，其他硬件厂商是不是更方便在这一层级往下做？比如，直接开发一套rocm.tile，在接口上对标cuda.tile，然后实现自己的编译器，Lower到自己的具有硬件特异性的IR代码上进行优化。这样似乎就避免了triton目前推出越来越多nvidia定制的功能，导致其他硬件厂商无法跟进的问题。
参考 cuTile talk @ scipy cuTile talk @ torch">

      <link rel="stylesheet" href="/css/root.min.0e732b812b9751962e01a7c4798a1211cd5f8ac8abec7f99793fe306989e459f.css" integrity="sha256-DnMrgSuXUZYuAafEeYoSEc1fisir7H&#43;ZeT/jBpieRZ8=" crossorigin="anonymous">
      <link rel="stylesheet" href="/css/bundle.min.d95a325399a05b50fe47dcf35b8229b8a2a014fcee5435cfb28204c6ac335fc5.css" integrity="sha256-2VoyU5mgW1D&#43;R9zzW4IpuKKgFPzuVDXPsoIExqwzX8U=" crossorigin="anonymous">

      <script src="/js/bundle.cc8ae9952dbfb731affafabdf26e5c60a6910047ff59ccdeaf1daebaa26c8830.js" integrity="sha256-zIrplS2/tzGv&#43;vq98m5cYKaRAEf/Wczerx2uuqJsiDA=" crossorigin="anonymous"></script><script defer src="/js/search/flexsearch.compact.64594b125f7b78bdf4fa8316955922bbebb1cd6baef3f16654bfca20309f18f8.js" integrity="sha256-ZFlLEl97eL30&#43;oMWlVkiu&#43;uxzWuu8/FmVL/KIDCfGPg="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="https://da1sy-petals.vercel.app/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://daisy-songs.vercel.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/posts/"
      >知识</a>
    </div>
    <div class="nav-item">
      <a href="/art/"
      >文化</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="/documents/"
      >文档</a>
    </div>
    <div class="nav-item">
      <a href="https://daisy-songs.vercel.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    <a id="theme-switcher" href="#">
<svg class="light-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 3V4M12 20V21M4 12H3M6.31412 6.31412L5.5 5.5M17.6859 6.31412L18.5 5.5M6.31412 17.69L5.5 18.5001M17.6859 17.69L18.5 18.5001M21 12H20M16 12C16 14.2091 14.2091 16 12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>

<svg class="dark-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
  </div>
</div></header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sy-petals.vercel.app/posts/">
        <span itemprop="name">Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>cuTile 历险记，第0集：心智模型</h1><time class="dim" datetime="2025-11-13T19:38:26&#43;08:00">November 13, 2025</time></div>
  <section class="page-section"><p>首先，（在通常意义上来说）cuTile不是一个库，是一门语言，因为他<del>劫持</del>捕获了Python的源码并且使用了自己的编译器对这段代码进行编译、Lower、执行等操作。这一点在宏观上可以对比triton。</p>
<p>因此在使用cuTile的时候，要一直告诉自己 &ldquo;This is not Python&rdquo;.</p>
<h2 id="abstraction-level">Abstraction Level</h2>
<p>作为用户需要知道的：这门语言工作在哪个抽象层级。这里指的是这门语言提供的接口，<strong>并不</strong>直接对应硬件。cuTile的compiler magic会把我们写的代码map到硬件上，但这并不是写cuTile的程序员需要关心的。</p>
<h3 id="内存">内存</h3>
<p>从逻辑上来说，cuTile暴露给用户的内存分为两种：</p>
<ul>
<li>Global Memory (Gmem):
<ul>
<li>读写速度：慢</li>
</ul>
</li>
<li>Cache:
<ul>
<li>读写速度：较快</li>
</ul>
</li>
</ul>
<h3 id="编程模型">编程模型</h3>
<h4 id="global-array">Global Array</h4>
<ul>
<li>
<p>存放在Gmem上</p>
</li>
<li>
<p>操作：只能进行Load(从Gmem读取到Cache)，以及Store(从Cache存入Gmem)。</p>
</li>
<li>
<p>来源：PyTorch tensor 可以直接传入。</p>
</li>
</ul>
<h4 id="tile-array">Tile Array</h4>
<ul>
<li>
<p>存放在Cache上</p>
</li>
<li>
<p>操作：可以在上面进行数学操作如<code>sin</code>, <code>mma</code>等。</p>
</li>
<li>
<p>来源：tile kernel内创建(例如<code>cuda.tile.zeros</code>)，或者Global Array load得到</p>
</li>
<li>
<p><strong>Immutable</strong>：<strong>在逻辑上</strong>，任何对Tile Array的计算操作都会返回新的Tile Array (Returns copies, not views)；你也不能直接对Tile Array里面的内容进行修改。</p>
<ul>
<li>cuTile的compiler magic肯定会在内部防止冗余内存的创建，毕竟速度&gt;=SRAM的存储是如此昂贵；但是程序员是以immutable的形式编程的。</li>
</ul>
</li>
<li>
<p>metadata: dtype, shape</p>
<ul>
<li>layout对用户是不可见的，交由编译器处理。</li>
</ul>
</li>
</ul>
<h4 id="示意图">示意图：</h4>
<p><img src="../images/2025-11-14-10-38-36.png" alt=""></p>
<h2 id="编程问题">编程问题</h2>
<h3 id="问题输入">问题输入</h3>
<p>Tensor 级别的计算过程，比如：</p>
<ul>
<li>Matmul+activation</li>
<li>Attention Mechanism</li>
<li>其他可以用NumPy/PyTorch这一级别的抽象所描述的算法。</li>
</ul>
<h3 id="cutile是用来解决什么问题的">cuTile是用来解决什么问题的</h3>
<p>对于同一个算法，如何明智地加载数据、进行计算，可以减少Load/Store的数据量、增大计算密度，并且减少需要在Gmem上materialize的中间数据。</p>
<h2 id="奇怪的想法">奇怪的想法</h2>
<p>据nv的编译器工程师在各种talk里面所说，cuTile将会屏蔽所有<strong>硬件特异性</strong>的功能，交由编译器处理。如果是这样的话，其他硬件厂商是不是更方便在这一层级往下做？比如，直接开发一套<code>rocm.tile</code>，在接口上对标<code>cuda.tile</code>，然后实现自己的编译器，Lower到自己的具有硬件特异性的IR代码上进行优化。这样似乎就避免了triton目前推出越来越多nvidia定制的功能，导致其他硬件厂商无法跟进的问题。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=uZTtViomW6w">cuTile talk @ scipy</a></li>
<li><a href="https://www.youtube.com/watch?v=UEdGJGz8Eyg">cuTile talk @ torch</a></li>
</ul>
</section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2026 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>71 words</span>
    <span>2 - 3 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#abstraction-level">Abstraction Level</a>
      <ul>
        <li><a href="#内存">内存</a></li>
        <li><a href="#编程模型">编程模型</a></li>
      </ul>
    </li>
    <li><a href="#编程问题">编程问题</a>
      <ul>
        <li><a href="#问题输入">问题输入</a></li>
        <li><a href="#cutile是用来解决什么问题的">cuTile是用来解决什么问题的</a></li>
      </ul>
    </li>
    <li><a href="#奇怪的想法">奇怪的想法</a></li>
    <li><a href="#参考">参考</a></li>
  </ul>
</nav></aside></div>
  </div>
</body>

</html>