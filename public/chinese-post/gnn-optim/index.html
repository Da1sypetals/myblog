<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>近期GNN Attention算子优化工作速览 | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="https://da1sypetals.github.io/chinese-post/gnn-optim/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="近期GNN Attention算子优化工作速览">
  <meta property="og:description" content="注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。
问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.
此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。
Notation n: 图节点数，规模为 1k~1Mnnz: 图边数（稀疏矩阵非零元素数，Num NonZero）规模为10n~1000nq, k, v: (n, d)A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。
实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:
SDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)):if M[i, j] != 0:out[i, j] = dot(A[i,:], B[:,j])else:out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chinese-post">
    <meta property="article:published_time" content="2025-10-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-02T00:00:00+00:00">
    <meta property="article:tag" content="Deep-Learning">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="近期GNN Attention算子优化工作速览">
  <meta name="twitter:description" content="注：本文用LLM辅助写作的地方主要在：我认为LLM比我理解的更好的地方，会用LLM的表述代替。
问题设定 需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.
此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。
Notation n: 图节点数，规模为 1k~1Mnnz: 图边数（稀疏矩阵非零元素数，Num NonZero）规模为10n~1000nq, k, v: (n, d)A: (n, n), binary, 高度稀疏 计算公式 softmax((q @ k.transpose()) * A) @ V 其中，@ 表示矩阵乘法，*表示element-wise乘法。
实现：naive version 最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。 A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。 Reformulate 我们引入三个算子:
SDDMM (Sampled Dense-Dense MatMul) A (m, k), B (k, n), 稠密 M (n, n)， 稀疏 SDDMM(A, B, M) 定义为： for i, j in product(range(n), range(n)):if M[i, j] != 0:out[i, j] = dot(A[i,:], B[:,j])else:out[i, j] = 0 Sparse Softmax: 在稀疏矩阵上按行softmax SpMM：sparse A @ dense B 此时我们的计算公式就可以重新写成:">

      <link rel="stylesheet" href="/css/root.min.0e732b812b9751962e01a7c4798a1211cd5f8ac8abec7f99793fe306989e459f.css" integrity="sha256-DnMrgSuXUZYuAafEeYoSEc1fisir7H&#43;ZeT/jBpieRZ8=" crossorigin="anonymous">
      <link rel="stylesheet" href="/css/bundle.min.59eb1a059f8cd558e64375ede3e68d3e9120ddb0c6bdbab555c247689cef59e1.css" integrity="sha256-WesaBZ&#43;M1VjmQ3Xt4&#43;aNPpEg3bDGvbq1VcJHaJzvWeE=" crossorigin="anonymous">

      <script src="/js/bundle.cc8ae9952dbfb731affafabdf26e5c60a6910047ff59ccdeaf1daebaa26c8830.js" integrity="sha256-zIrplS2/tzGv&#43;vq98m5cYKaRAEf/Wczerx2uuqJsiDA=" crossorigin="anonymous"></script><script defer src="/js/search/flexsearch.compact.5e0de3b335e5c523c7cf45473dc43fccb6c75f64a9d59cc04a6eccbb7c25eb49.js" integrity="sha256-Xg3jszXlxSPHz0VHPcQ/zLbHX2Sp1ZzASm7Mu3wl60k="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="https://da1sypetals.github.io/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/chinese-post/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >Posts</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/chinese-post/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >Posts</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    <a id="theme-switcher" href="#">
<svg class="light-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 3V4M12 20V21M4 12H3M6.31412 6.31412L5.5 5.5M17.6859 6.31412L18.5 5.5M6.31412 17.69L5.5 18.5001M17.6859 17.69L18.5 18.5001M21 12H20M16 12C16 14.2091 14.2091 16 12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>

<svg class="dark-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
  </div>
</div>
</header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sypetals.github.io/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="https://da1sypetals.github.io/chinese-post/">
        <span itemprop="name">Chinese-Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>近期GNN Attention算子优化工作速览</h1><time class="dim" datetime="2025-10-02T00:00:00&#43;00:00">October 2, 2025</time><div class="term-container"><div class="tag">
        <a href="https://da1sypetals.github.io/tags/deep-learning/">#deep-learning</a>
      </div></ol></div>
  <section class="page-section"><p>注：本文用LLM辅助写作的地方主要在：<em><strong>我认为LLM比我理解的更好的地方，会用LLM的表述代替。</strong></em></p>
<h2 id="问题设定">问题设定</h2>
<p>需要计算Graph Transformer中的Attention。在此我们忽略multihead-attention，考虑基本的single-head attention.</p>
<p>此外，我们的attention mask(邻接矩阵A)是非结构化稀疏的。如果你的attention mask是结构化稀疏的，比如blockwise等可以被代码表示的稀疏pattern，你应该使用flash attention的varlen变体, 或者flex attention等attention编译器。</p>
<h3 id="notation">Notation</h3>
<pre tabindex="0"><code>n: 图节点数，规模为 1k~1M
nnz: 图边数（稀疏矩阵非零元素数，Num NonZero）
	  规模为10n~1000n
q, k, v: (n, d)
A: (n, n), binary, 高度稀疏
</code></pre><h3 id="计算公式">计算公式</h3>
<pre tabindex="0"><code>softmax((q @ k.transpose()) * A) @ V
</code></pre><p>其中，<code>@</code> 表示矩阵乘法，<code>*</code>表示element-wise乘法。</p>
<h2 id="实现naive-version">实现：naive version</h2>
<ol>
<li>最简单的就是把A给materialize出来，然后用作attention_mask。问题是A是n^2的，显存不够用。</li>
<li>A用COO方式存储，大小(2,nnz)，然后先把每条边的qk-pair算出来(nnz,d)，然后再做reduce和scatter和V相乘。</li>
</ol>
<h2 id="reformulate">Reformulate</h2>
<p>我们引入三个算子:</p>
<ul>
<li><strong>SDDMM (Sampled Dense-Dense MatMul)</strong>
<ul>
<li>A (m, k), B (k, n), 稠密</li>
<li>M (n, n)， 稀疏
SDDMM(A, B, M) 定义为：</li>
</ul>
</li>
</ul>
<pre tabindex="0"><code>for i, j in product(range(n), range(n)):
	if M[i, j] != 0:
		out[i, j] = dot(A[i,:], B[:,j])
	else:
		out[i, j] = 0
</code></pre><ul>
<li>Sparse Softmax: 在稀疏矩阵上按行softmax</li>
<li>SpMM：sparse A @ dense B</li>
</ul>
<p>此时我们的计算公式就可以重新写成:</p>
<pre tabindex="0"><code>out = SpMM(Softmax(SDDMM(Q, K_T, A)), V)
</code></pre><p>以此我们引出下面的实现</p>
<h2 id="实现dgl">实现：DGL</h2>
<p><a href="https://www.dgl.ai/dgl_docs/en/2.2.x/notebooks/sparse/graph_transformer.html">Graph Transformer in a Nutshell — DGL 2.2.1 documentation</a></p>
<p>对于稠密的q,k,v和CSR存储的A，通过如下代码计算attention：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>attn <span style="color:#f92672">=</span> dglsp<span style="color:#f92672">.</span>bsddmm(A, q, k<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>))  <span style="color:#75715e"># (sparse) [N, N, nh]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Sparse softmax by default applies on the last sparse dimension.</span>
</span></span><span style="display:flex;"><span>attn <span style="color:#f92672">=</span> attn<span style="color:#f92672">.</span>softmax()  <span style="color:#75715e"># (sparse) [N, N, nh]</span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> dglsp<span style="color:#f92672">.</span>bspmm(attn, v)  <span style="color:#75715e"># [N, dh, nh]</span>
</span></span></code></pre></div><p>算子在DGL库内部由CUDA实现。看DGL的代码可以发现，其实现利用了稀疏性，但是存在以下优化点</p>
<ul>
<li>进行的是最直观的并行，没有进行充分的优化</li>
<li>各个kernel分开执行，没有融合</li>
<li>没有利用tensor core</li>
</ul>
<h2 id="实现flashsparse">实现：FlashSparse</h2>
<p><a href="https://github.com/ParCIS/FlashSparse/tree/main/eva">https://github.com/ParCIS/FlashSparse/tree/main/eva</a></p>
<p>主题：对SDDMM,SpMM进行优化；尝试在稀疏输入中以最小粒度利用tensor core</p>
<p>基于一个基本观察：A × B = C ⟹ (Bᵀ × Aᵀ)ᵀ = C，发明了交换与转置MMA计算策略：目标是将稀疏矩阵划分所依赖的MMA指令维度，从较大的m维（值为16）切换到较小的n维（值为8）。标准张量核心MMA指令的形状为m16n8k8（FP16精度下，m=16, n=8, k=8）。这使得稀疏矩阵 A 可被划分为8×1的向量，相比之前工作中使用的16×1向量，计算冗余减少了约50%。</p>
<ul>
<li><strong>矩阵格式</strong>：本算法发明了ME-BCRS格式，基本想法是在一个8x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。
<ul>
<li>空间开销维持在O(n+nnz)，常数比较小，远没有达到head_dim的量级。</li>
<li>矩阵格式转换时间开销 (CSR -&gt; ME-BCRS)：由于是一次性开销，相对整个模型推理时间几乎可以忽略。</li>
</ul>
</li>
</ul>
<p><img src="../images/2025-10-02-19-50-28.png" alt=""></p>
<h3 id="flashsparse的spmm算法c--a--b">FlashSparse的SpMM算法（C = A × B）</h3>
<p><strong>阶段1：转置访问与加载</strong></p>
<ul>
<li><strong>块形状</strong>：算法将 A 划分为8×8的稀疏TC块（FP16精度下），将 B 划分为8×16的稠密TC块。</li>
<li><strong>稀疏块 A 加载</strong>：线程从全局内存（以行优先的ME-BCRS格式存储）加载8×8稀疏TC块 A，并在寄存器中将其转置为 Aᵀ，作为右操作数。</li>
<li><strong>稠密块 B 加载</strong>：线程从全局内存（行优先）加载8×16稠密TC块 B，并在寄存器中将其转置为 Bᵀ，作为左操作数。</li>
<li><strong>合并访问</strong>：通过重新排列线程访问的列，确保所需数据对齐形成2×2的FP16元素块，从而使内存事务匹配GPU最小32字节的事务粒度，实现合并访问，减少50%的访存开销。</li>
</ul>
<p><strong>阶段2：交换与转置计算</strong></p>
<p>在张量核心上执行MMA指令：Bᵀ × Aᵀ。</p>
<ul>
<li>Bᵀ 作为左操作数（m=16, k=8）。</li>
<li>Aᵀ 作为右操作数（k=8, n=8）。</li>
<li>结果为转置后的输出块 Cᵀ（尺寸为16×8），存储在寄存器中。</li>
</ul>
<p><strong>阶段3：转置输出</strong></p>
<p>寄存器中的 Cᵀ 必须在写回全局内存前转置回 C。由于寄存器中 Cᵀ 的数据布局与加载 B 时所需的 Bᵀ 布局完全相同，因此可复用为加载 B 设计的高效合并写回策略，将结果写入全局内存。</p>
<h3 id="flashsparse的sddmm算法c--m--ab">FlashSparse的SDDMM算法（C = M ⊙ (AB)）</h3>
<ul>
<li><strong>块形状</strong>：FlashSparse将稀疏输出矩阵 C 划分为8×16的稀疏TC块。两个稠密输入矩阵（按论文图8中的记号，记为 A_dense 和 B_dense，满足 C_sparse = A_dense × B_dense）分别以稠密TC块形式加载：A_dense 为8×8（行优先），B_dense 为8×16（列优先）。</li>
<li><strong>转置计算的数据对齐</strong>：SDDMM中稠密输入矩阵 A（行优先）和 B（列优先）的数据布局，恰好满足“交换与转置MMA计算”（Bᵀ × Aᵀ）的要求。</li>
</ul>
<p><strong>转置计算</strong>：</p>
<ul>
<li>稠密输入 B 被转置为 Bᵀ（尺寸16×8），作为左操作数。</li>
<li>稠密输入 A 被转置为 Aᵀ（尺寸8×8），作为右操作数。</li>
<li>计算 Bᵀ × Aᵀ 得到稠密结果 C_denseᵀ。</li>
<li>用M矩阵进行element-wise product，从C_dense 得到C_sparse</li>
</ul>
<h3 id="实测">实测:</h3>
<p>未测试</p>
<h2 id="实现df-gnn">实现：DF-GNN</h2>
<p><a href="https://github.com/paoxiaode/DF-GNN">https://github.com/paoxiaode/DF-GNN</a></p>
<p>主题：block/warp调度和算子融合</p>
<p>由于我主要看了tiling部分的算法（适用于大图和邻居数不确定的图，仅forward），所以主要介绍这部分。</p>
<p>使用的矩阵格式是CSR，不需要做额外的格式转换</p>
<h3 id="算法流程">算法流程</h3>
<pre tabindex="0"><code>Launch Kernel on Grid: (n × h)
       ↓
Each Block → (rid, hid): one node, one head
       ↓
Load Q[rid, hid, :] → s_Q[f] (shm)
       ↓
For each tile of neighbors (size ≤ 32):
    -  Load neighbor IDs from indices[]
    -  Compute Q · K^T (dot product using s_Q and K[dst])
    -  Reduce in warp → store in neigh_nodes_weight[eid]
    -  Find max(weight) in current tile → weightMax
    -  Adjust partial_sum and acc with exp(old_max - new_max)
    -  Compute exp(weight - weightMax) and accumulate acc += exp_w * V[]
    -  Accumulate partial_sum += exp_w
    -  Update weightMax_old
       ↓
Final normalization: out_feat = acc / partial_sum
       ↓
Write back to global memory
</code></pre><p>主要就是通过合理安排GPU资源（threadblock, thread）和计算任务的mapping，实现在一个kernel 内负载相对均衡的完成任务。</p>
<h3 id="实测-1">实测:</h3>
<p>代码方面：开源的代码有比较多的bug，包括了data race, 指针运算错误等等</p>
<p>修复后：</p>
<p>在常用工作范围内，forward速度达到DGL实现的2.5x ~ 3x</p>
<p><img src="../images/2025-10-02-19-50-59.png" alt=""></p>
<p>精度：和DGL实现对比，MAE在1e-8 ~ 1e-9量级，差距可以忽略不计</p>
<h2 id="f3s">F3S</h2>
<p><a href="https://github.com/HPCForge/Fused3S/tree/main/scripts">https://github.com/HPCForge/Fused3S/tree/main/scripts</a>
主题：算子融合+混合精度+利用tensor core</p>
<p>其主要思路还是类似FlashSparse，但是通过算子融合达到了更高的效率（访存开销，kernel launch开销更小）。混合精度算是一种tradeoff。</p>
<ol>
<li>仅有forward的实现</li>
<li>F3S也使用了自定义的矩阵格式BSB，基本想法是在一个16x1的block中，如果有一个元素是非零的，那么就把这整个block都当成非零的存下来。
<ul>
<li>优化的一点在于，block内是否为0被压缩到一个bit中，每个16x8block以uint128保存，充分利用了attention中adj只能为0/1的特点</li>
<li>和flashsparse相比不足的一点在16x1粒度更大，多余计算更多，也是本工作没考虑到的一点</li>
<li>空间开销：O(n+nnz)，但是常数会更大一些</li>
<li>矩阵格式转换时间开销 (CSR -&gt; BSB)：一次性开销，暂时忽略。</li>
</ul>
</li>
</ol>
<p><img src="../images/2025-10-02-19-52-13.png" alt=""></p>
<h3 id="算法流程-1"><strong>算法流程</strong>：</h3>
<ol>
<li>
<p>划分行块：</p>
<ul>
<li>将 Q 按行划分为 $T_r = \lceil N / r \rceil$ 个块 $\{Q_1, ..., Q_{T_r}\}$，每个大小为 $r \times d$。</li>
<li>将输出 O 同样划分为 $T_r$ 个块 $\{O_1, ..., O_{T_r}\}$，每个大小为 $r \times d$。</li>
</ul>
</li>
<li>
<p>对每个行块索引 $i = 1$ 到 $T_r$（并行处理）：</p>
<ul>
<li>
<p>初始化</p>
<ul>
<li>$m_o \leftarrow -\infty \in \mathbb{R}^r$（行最大值）</li>
<li>$l_o \leftarrow 0 \in \mathbb{R}^r$（行 softmax 累加和）</li>
<li>$O_i \leftarrow 0 \in \mathbb{R}^{r \times d}$（输出块，fp32）</li>
</ul>
</li>
<li>
<p>加载数据：</p>
<ul>
<li>将 $Q_i$ 从全局内存（HBM）加载到共享内存（SMEM）。</li>
<li>计算当前行窗口（RW）包含的 TCB 数量：$t = \text{tro}[i+1] - \text{tro}[i]$。</li>
<li>通过 <code>sptd</code> 获取当前 RW 对应的原始列索引向量 $c$。</li>
<li>从 $K$ 和 $V$ 中按索引 $c$ <strong>gather</strong> 出对应的行，得到 $\hat{K}, \hat{V} \in \mathbb{R}^{t \cdot c \times d}$。</li>
</ul>
</li>
<li>
<p>划分 warp 块：</p>
<ul>
<li>将 $\hat{K}$ 划分为 $T_c = \lceil t / W \rceil$ 个块 $\{\hat{K}_1, ..., \hat{K}_{T_c}\}$，每个大小为 $Wc \times d$。</li>
<li>将 $\hat{V}$ 同样划分为 $T_c$ 个块 $\{\hat{V}_1, ..., \hat{V}_{T_c}\}$，每个大小为 $Wc \times d$。</li>
</ul>
</li>
<li>
<p>对每个 warp 块索引 $j = 1$ 到 $T_c$：</p>
<ul>
<li>
<p>SDDMM：</p>
<ul>
<li>调用 $\text{TBGemm}(Q_i, \hat{K}_j^T, 0)$，计算中间得分块 $S_i \in \mathbb{R}^{r \times c}$（fp32）。</li>
<li>用 BSB 中对应 TCB 的 <strong>bitmap</strong> 对 $S_i$ 进行掩码（非零位置保留，其余置 0）。</li>
</ul>
</li>
<li>
<p>Online Softmax：</p>
<ul>
<li>计算当前块行最大值：$m_i = \max(m_o, \text{rowmax}(S_i))$。</li>
<li>计算指数：$E_i = \exp(S_i - m_i)$。</li>
<li>更新累加和：$l_o = \text{diag}(\exp(m_o - m_i)) \cdot l_o + \text{rowsum}(E_i)$。</li>
<li>将 $E_i$ 转为 fp16，存入 SMEM。</li>
</ul>
</li>
<li>
<p>SpMM：</p>
<ul>
<li>对已有输出缩放：$O_i = \text{diag}(\exp(m_o - m_i)) \cdot O_i$。</li>
<li>调用 $\text{TBGemm}(E_i, \hat{V}_j, O_i)$，将结果累加回 $O_i$。</li>
<li>更新行最大值：$m_o = m_i$。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>最终归一化并写回：</p>
<ul>
<li>对输出块归一化：$O_i = \text{diag}(l_o)^{-1} \cdot O_i$。</li>
<li>将 $O_i$ 写回全局内存（HBM）。</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="subroutine-tbgemm">Subroutine: TBGemm</h3>
<p><strong>输入:</strong></p>
<ul>
<li>矩阵块 $A \in \mathbb{R}^{m \times K}$ (位于 <strong>SMEM</strong>，共享内存)</li>
<li>矩阵块 $B \in \mathbb{R}^{K \times P}$ (位于 <strong>HBM</strong>，全局内存)</li>
<li>累加项 $D \in \mathbb{R}^{m \times P}$ (位于 <strong>SMEM</strong>，共享内存)</li>
</ul>
<p><strong>输出:</strong></p>
<ul>
<li>结果矩阵 $C = A B + D \in \mathbb{R}^{m \times P}$</li>
</ul>
<p><strong>流程:</strong></p>
<ol>
<li>
<p><strong>切分块 (Tiling):</strong> 将输入矩阵 $A$, $B$, $D$ 按照 <strong>Tensor Core</strong> 的硬件 Tile 尺寸（例如 $16 \times 8 \times 16$）切分为对应的子块。</p>
</li>
<li>
<p><strong>并行迭代 (Output Tiles):</strong> 对结果矩阵 $C$ 的<strong>每个输出 Tile</strong> (通常由一个 Warp 或一个 Thread Block 计算):</p>
<ul>
<li><strong>加载累加项 D:</strong> 从 <strong>SMEM</strong> 中加载 $D$ 对应的子块到线程的<strong>寄存器</strong>中，作为初始累加值 $C$.</li>
</ul>
</li>
<li>
<p><strong>内积迭代 (K-Tiles):</strong> 对 $K$ 维度的<strong>每个 $k$-tile</strong> 进行迭代累加:</p>
<ul>
<li><strong>加载 A:</strong> 从 <strong>SMEM</strong> 中加载矩阵 $A$ 对应的 $A_{\text{tile}}$ 子块。</li>
<li><strong>加载 B:</strong> 从 <strong>HBM</strong> 中<strong>直接</strong>加载矩阵 $B$ 对应的 $B_{\text{tile}}$ 子块。</li>
<li><strong>执行 MMA 指令:</strong> 调用硬件支持的 <strong>PTX <code>mma</code> 指令</strong>（Matrix Multiply-Accumulate），执行计算并累加：
$$C \leftarrow A_{\text{tile}} \cdot B_{\text{tile}} + C$$</li>
</ul>
</li>
<li>
<p><strong>返回:</strong> 最终得到结果 $C$。</p>
</li>
</ol>
<h3 id="实测-2">实测:</h3>
<p>代码方面：在矩阵格式转换部分有bug，已联系作者修复；开源代码没有multihead，需要自己实现。</p>
<p>速度达到DGL实现的3x(相对稀疏) 到5x (相对稠密）</p>
<p>限制：n % 16 == 0，因为需要分割成8x16的block</p>
<p>精度：和DGL实现对比，MAE在3e-5~1e-4 量级，很可能需要通过对模型进行end2end测试来确定是否适合使用。</p>
</section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2025 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>746 words</span>
    <span>11 - 14 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#问题设定">问题设定</a>
      <ul>
        <li><a href="#notation">Notation</a></li>
        <li><a href="#计算公式">计算公式</a></li>
      </ul>
    </li>
    <li><a href="#实现naive-version">实现：naive version</a></li>
    <li><a href="#reformulate">Reformulate</a></li>
    <li><a href="#实现dgl">实现：DGL</a></li>
    <li><a href="#实现flashsparse">实现：FlashSparse</a>
      <ul>
        <li><a href="#flashsparse的spmm算法c--a--b">FlashSparse的SpMM算法（C = A × B）</a></li>
        <li><a href="#flashsparse的sddmm算法c--m--ab">FlashSparse的SDDMM算法（C = M ⊙ (AB)）</a></li>
        <li><a href="#实测">实测:</a></li>
      </ul>
    </li>
    <li><a href="#实现df-gnn">实现：DF-GNN</a>
      <ul>
        <li><a href="#算法流程">算法流程</a></li>
        <li><a href="#实测-1">实测:</a></li>
      </ul>
    </li>
    <li><a href="#f3s">F3S</a>
      <ul>
        <li><a href="#算法流程-1"><strong>算法流程</strong>：</a></li>
        <li><a href="#subroutine-tbgemm">Subroutine: TBGemm</a></li>
        <li><a href="#实测-2">实测:</a></li>
      </ul>
    </li>
  </ul>
</nav><h3>Related</h3>
    <ul><li><a href="/english-post/snapviewer/">SnapViewer: Faster PyTorch Memory Allocation Viewer</a></li><li><a href="/chinese-post/snapviewer-3-zh/">Snapviewer Devlog #3: 性能优化</a></li></ul></aside></div>
  </div>
</body>

</html>