<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>共轭梯度：高中解析几何的拓展 | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="http://localhost:1313/chinese-post/conj-grad/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="共轭梯度：高中解析几何的拓展">
  <meta property="og:description" content="本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。
1. 问题 对于实对称矩阵 $A \in \mathbb{R}^{n \times n}$ 和向量 $b \in \mathbb{R}^n$，求解
$$Ax = b$$或者，等价的，
$$\text{argmin}_x f(x)$$其中
$$f(x) = \frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：
$$a^{-2}x^2 &#43; b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：
这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。
2.2. 最速下降法 首先，你应该知道梯度下降法：
$$x_{i&#43;1} = x_i - \alpha\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\alpha$ 使得 $x_{i&#43;1}$ 达到最小（在搜索方向上的最小值）：
$$\alpha^* = \text{argmin}_\alpha f(x_i - \alpha\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="chinese-post">
    <meta property="article:published_time" content="2024-12-07T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-07T00:00:00+00:00">
    <meta property="article:tag" content="Math">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="共轭梯度：高中解析几何的拓展">
  <meta name="twitter:description" content="本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 CMU 的教案及其翻译。
1. 问题 对于实对称矩阵 $A \in \mathbb{R}^{n \times n}$ 和向量 $b \in \mathbb{R}^n$，求解
$$Ax = b$$或者，等价的，
$$\text{argmin}_x f(x)$$其中
$$f(x) = \frac{1}{2}x^T A x - b^T x$$2. 预备知识 2.1. 从高中学的二级结论说起 高中的时候我们学过椭圆：
$$a^{-2}x^2 &#43; b^{-2}y^2 = 1$$如果你记性好的话，你应该记得这个二级结论：
这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。
2.2. 最速下降法 首先，你应该知道梯度下降法：
$$x_{i&#43;1} = x_i - \alpha\nabla f(x_i)$$最速下降法就是在梯度下降法的基础上，选择 $\alpha$ 使得 $x_{i&#43;1}$ 达到最小（在搜索方向上的最小值）：
$$\alpha^* = \text{argmin}_\alpha f(x_i - \alpha\nabla f(x_i))$$3. 共轭梯度法 3.1. 记号 $x_i$：第 $i$ 次循环之后的 $x$ 向量">

    <link rel="stylesheet" href="/css/root.css">
    <link rel="stylesheet" href="/css/bundle.css">

      <script src="/js/bundle.js"></script><script defer src="/js/search/flexsearch.compact.5e0de3b335e5c523c7cf45473dc43fccb6c75f64a9d59cc04a6eccbb7c25eb49.js" integrity="sha256-Xg3jszXlxSPHz0VHPcQ/zLbHX2Sp1ZzASm7Mu3wl60k="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="http://localhost:1313/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/chinese-post/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/chinese-post/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    <a id="theme-switcher" href="#">
<svg class="light-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M12 3V4M12 20V21M4 12H3M6.31412 6.31412L5.5 5.5M17.6859 6.31412L18.5 5.5M6.31412 17.69L5.5 18.5001M17.6859 17.69L18.5 18.5001M21 12H20M16 12C16 14.2091 14.2091 16 12 16C9.79086 16 8 14.2091 8 12C8 9.79086 9.79086 8 12 8C14.2091 8 16 9.79086 16 12Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>

<svg class="dark-icon" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M3.32031 11.6835C3.32031 16.6541 7.34975 20.6835 12.3203 20.6835C16.1075 20.6835 19.3483 18.3443 20.6768 15.032C19.6402 15.4486 18.5059 15.6834 17.3203 15.6834C12.3497 15.6834 8.32031 11.654 8.32031 6.68342C8.32031 5.50338 8.55165 4.36259 8.96453 3.32996C5.65605 4.66028 3.32031 7.89912 3.32031 11.6835Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
  </div>
</div>
</header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="http://localhost:1313/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="http://localhost:1313/chinese-post/">
        <span itemprop="name">Chinese-Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>共轭梯度：高中解析几何的拓展</h1><time class="dim" datetime="2024-12-07T00:00:00&#43;00:00">December 7, 2024</time><div class="term-container"><div class="tag">
        <a href="http://localhost:1313/tags/math/">#math</a>
      </div></ol></div>
  <section class="page-section"><p>本文没有任何数学推导。我们从直观上理解这个算法，然后直接介绍算法的流程。希望了解数学推导的读者可以查看 <a href="https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf">CMU 的教案</a>及其<a href="https://flat2010.github.io/2018/10/26/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E9%80%9A%E4%BF%97%E8%AE%B2%E4%B9%89/#8-%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95">翻译</a>。</p>
<h2 id="1-问题">1. 问题</h2>
<p>对于实对称矩阵 $A \in \mathbb{R}^{n \times n}$ 和向量 $b \in \mathbb{R}^n$，求解</p>
$$Ax = b$$<p>或者，等价的，</p>
$$\text{argmin}_x f(x)$$<p>其中</p>
$$f(x) = \frac{1}{2}x^T A x - b^T x$$<h2 id="2-预备知识">2. 预备知识</h2>
<h3 id="21-从高中学的二级结论说起">2.1. 从高中学的二级结论说起</h3>
<p>高中的时候我们学过椭圆：</p>
$$a^{-2}x^2 + b^{-2}y^2 = 1$$<p>如果你记性好的话，你应该记得这个二级结论：</p>
<p><img src="../images/2025-10-03-00-44-15.png" alt=""></p>
<p>这是一个从圆里面推广而来的结论：如果 $a = b$，椭圆退化为圆，$k_{OM}k_l = -1$，即 $OM, l$ 两条直线垂直。</p>
<h3 id="22-最速下降法">2.2. 最速下降法</h3>
<p>首先，你应该知道梯度下降法：</p>
$$x_{i+1} = x_i - \alpha\nabla f(x_i)$$<p>最速下降法就是在梯度下降法的基础上，选择 $\alpha$ 使得 $x_{i+1}$ 达到最小（在搜索方向上的最小值）：</p>
$$\alpha^* = \text{argmin}_\alpha f(x_i - \alpha\nabla f(x_i))$$<h2 id="3-共轭梯度法">3. 共轭梯度法</h2>
<h3 id="31-记号">3.1. 记号</h3>
<ul>
<li>
<p>$x_i$：第 $i$ 次循环之后的 $x$ 向量</p>
</li>
<li>
<p>$r_i$：$b_i - Ax_i$，目标函数 $f(x)$ 在 $x_i$ 点的负梯度，或者线性方程组在 $x_i$ 点的残差。</p>
<ul>
<li>请记住：负梯度和残差是一个东西！</li>
</ul>
</li>
<li>
<p>$d_i$：在 $x_i$ 点的搜索方向。最速下降算法里 $d_i = r_i$，共轭梯度里面需要一点修正。</p>
</li>
</ul>
<h3 id="32-最速下降">3.2. 最速下降</h3>
<ol>
<li>
<p>最速下降的新方向：$r_{i+1}$</p>
<ul>
<li>新方向与前一步下降方向 $r_i$ 垂直（画个等高线图直观理解，或者回想一下&quot;等势面和电场线垂直&quot;）</li>
</ul>
</li>
</ol>
<p><img src="../images/2025-10-03-00-38-10.png" alt=""></p>
<ol start="2">
<li>最速下降的 $\alpha$</li>
</ol>
$$\alpha_i = \frac{r_i^T r_i}{d_i^T A d_i}$$<h3 id="33-共轭梯度">3.3. 共轭梯度</h3>
<p>我们直接逐项类比最速下降。</p>
<ul>
<li>
<p>新方向与前一步下降方向 $r_i$ 垂直 斜率之积为 $-a^{-2}b^2$ (Section 2.1)</p>
<ul>
<li>这个方向由最速下降的方向进行一些小改动得到，我们可以在后面的算法部分(Section 3.4)看到。把这个方向从和前一个搜索方向垂直改动到斜率之积为 $-a^{-2}b^2$ 就是 CG 和最速下降唯一不同的地方。</li>
</ul>
<p><img src="../images/2025-10-03-00-39-26.png" alt=""></p>
<ul>
<li>步长 $\alpha$：由于是在一条直线上做优化，因此和最速下降的 $\alpha$ 相同。</li>
</ul>
</li>
</ul>
<p>由于一次迭代只涉及到两个点、两个向量，只能构成一个平面，我们甚至不需要将二维向多维推广。</p>
<ul>
<li>若需推导，我们需要做的只是把点的 $n$ 维坐标映射到二维，然后对截面椭圆对应的二阶二次型进行 SVD 获得其长轴 $a$ 和短轴 $b$，进而根据其离及上述斜率积的二级结论计算两个方向的关系。这里不展开。</li>
</ul>
<h3 id="34-算法">3.4. 算法</h3>
<h4 id="341-初始化">3.4.1. 初始化</h4>
<p>算法输入：$A, b, x_0$</p>
$$\vec{d}_{(0)} = \vec{r}_{(0)} = \vec{b}_{(0)} - \mathbf{A}\vec{x}_{(0)}$$<h4 id="342-算法过程">3.4.2. 算法过程</h4>
$$\alpha_{(i)} = \frac{\vec{r}_{(i)}^T \vec{r}_{(i)}}{\vec{d}_{(i)}^T \mathbf{A}\vec{d}_{(i)}}$$$$\vec{x}_{(i+1)} = \vec{x}_{(i)} + \alpha_{(i)}\vec{d}_{(i)}$$$$\vec{r}_{(i+1)} = \vec{r}_{(i)} - \alpha_{(i)}\mathbf{A}\vec{d}_{(i)}$$$$\beta_{(i+1)} = \frac{\vec{r}_{(i+1)}^T \vec{r}_{(i+1)}}{\vec{r}_{(i)}^T \vec{r}_{(i)}}$$$$\vec{d}_{(i+1)} = \vec{r}_{(i+1)} + \beta_{(i+1)}\vec{d}_{(i)}$$<p>其中的最后一步就是通过 $\beta$ 将 $r_{i+1}$ 修正成 $d_{i+1}$ 的。</p>
<h4 id="343-起讫">3.4.3. 起讫</h4>
<ol>
<li>
<p>起：如果你对解 $x$ 有粗略的估计，就使用那个值作为起始点 $x_0$；否则，直接使用 $x_0 = 0$。</p>
</li>
<li>
<p>讫：通常的做法是在残差向量的 2-norm 小于某个给定阈值的时候就停下来。通常这个阈值为初始残差的一小部分</p>
</li>
</ol>
$$\|r_i\| < \varepsilon \|r_0\|$$<p>其中 $\varepsilon$ 是一个输入的参数。</p>
<h3 id="35-杂项">3.5. 杂项</h3>
<ol>
<li>由于 $Ad_i$ 在每个循环中都要被计算，且</li>
</ol>
$$r_{i+1} = r_i - \alpha_i A d_i$$<p>故可以用上式计算 $r_{i+1}$，而不必用 $b - Ax_{i+1}$。</p>
<ol start="2">
<li>上述方法有浮点误差累计的危险，因此我们应该每过几个循环就重新用 $r_i = b - Ax_i$ 重新计算残差。</li>
</ol>
</section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2025 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>215 words</span>
    <span>4 - 5 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#1-问题">1. 问题</a></li>
    <li><a href="#2-预备知识">2. 预备知识</a>
      <ul>
        <li><a href="#21-从高中学的二级结论说起">2.1. 从高中学的二级结论说起</a></li>
        <li><a href="#22-最速下降法">2.2. 最速下降法</a></li>
      </ul>
    </li>
    <li><a href="#3-共轭梯度法">3. 共轭梯度法</a>
      <ul>
        <li><a href="#31-记号">3.1. 记号</a></li>
        <li><a href="#32-最速下降">3.2. 最速下降</a></li>
        <li><a href="#33-共轭梯度">3.3. 共轭梯度</a></li>
        <li><a href="#34-算法">3.4. 算法</a></li>
        <li><a href="#35-杂项">3.5. 杂项</a></li>
      </ul>
    </li>
  </ul>
</nav></aside></div>
  </div>
</body>

</html>