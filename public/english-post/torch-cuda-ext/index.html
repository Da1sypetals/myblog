<!DOCTYPE html>
<html lang="zh-CN" dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script>
    MathJax = {
        tex: {
            displayMath: [['\\[', '\\]'], ['$$', '$$']],  
            inlineMath: [['$', '$']]                  
        },
        loader: {
            load: ['ui/safe']
        },
    };
</script>
  
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Notes on Writing PyTorch CUDA Extensions | Da1sypetals</title>
<link rel="icon" href="/favicon.svg" sizes="any" type="image/svg+xml" /><meta property="og:url" content="http://localhost:1313/english-post/torch-cuda-ext/">
  <meta property="og:site_name" content="Da1sypetals">
  <meta property="og:title" content="Notes on Writing PyTorch CUDA Extensions">
  <meta property="og:description" content="Intro: PyTorch is a Deep Learning Operating System.
Check tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.
API:
tensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="english-post">
    <meta property="article:published_time" content="2025-10-02T14:48:02+08:00">
    <meta property="article:modified_time" content="2025-10-02T14:48:02+08:00">
    <meta property="article:tag" content="Deep-Learning">
    <meta property="article:tag" content="Cuda">
    <meta property="article:tag" content="Torch">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Notes on Writing PyTorch CUDA Extensions">
  <meta name="twitter:description" content="Intro: PyTorch is a Deep Learning Operating System.
Check tensor storage Device check You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.
API:
tensor.is_cuda() tensor.device() (Use operator== for equality comparison). Sometimes the not on correct device problem causes strange error messages like Cusparse context initialization failure or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.">

    <link rel="stylesheet" href="/css/root.css">
    <link rel="stylesheet" href="/css/bundle.css">

      <script src="/js/bundle.js"></script><script defer src="/js/search/flexsearch.compact.5e0de3b335e5c523c7cf45473dc43fccb6c75f64a9d59cc04a6eccbb7c25eb49.js" integrity="sha256-Xg3jszXlxSPHz0VHPcQ/zLbHX2Sp1ZzASm7Mu3wl60k="></script>
<script defer src="/js/search/search.1d980f84df11f3eb7c8c5f17f541d49a0611608df179dd74fa7f06225eb56ace.js" integrity="sha256-HZgPhN8R8&#43;t8jF8X9UHUmgYRYI3xed10&#43;n8GIl61as4="></script>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Bricolage+Grotesque:opsz,wght@12..96,200..800&family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&display=swap" rel="stylesheet">

</head>

<body class="notransition">
  <div id="container">
    <header id="main-header"><div role="navigation" aria-label="Main">
  <div class="nav-left">
    <a href="http://localhost:1313/" style="color: inherit;">Da1sypetals</a>
  </div>
  <div class="nav-right">
    <div style="position:absolute;width:0px;height:0px;">
      <div id="nav-dropdown-menu" class="hidden" href="#">
    <div class="nav-item">
      <a href="/posts/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    </div>
    <a id="nav-dropdown-button" href="#"><svg width="20px" height="20px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M4 6H20M4 12H20M4 18H20" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
</svg>
</a>
    <div id="nav-menu">
    <div class="nav-item">
      <a href="/posts/"
      >文章</a>
    </div>
    <div class="nav-item">
      <a aria-current="true" class="ancestor" href="/english-post/"
      >en-Posts</a>
    </div>
    <div class="nav-item">
      <a href="https://singings.netlify.app"
      >歌单</a>
    </div>
    <div class="nav-item">
      <a href="/about/"
      >关于我</a>
    </div>
</div>
    
  </div>
</div></header>
    <div class="flex grow">
      <div id="main-pane">
        <main id="main-content"><div class="single-header">
<ol class="breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="http://localhost:1313/">
        <span itemprop="name">Home</span>
      </a>
      <meta itemprop="position" content='1' />
    </li>
    <span>&nbsp»&nbsp</span>
    <li itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
      <a itemprop="item" href="http://localhost:1313/english-post/">
        <span itemprop="name">English-Posts</span>
      </a>
      <meta itemprop="position" content='2' />
    </li>
    <span>&nbsp»&nbsp</span>
</ol>
<h1>Notes on Writing PyTorch CUDA Extensions</h1><time class="dim" datetime="2025-10-02T14:48:02&#43;08:00">October 2, 2025</time><div class="term-container"><div class="tag">
        <a href="http://localhost:1313/tags/deep-learning/">#deep-learning</a>
      </div><div class="tag">
        <a href="http://localhost:1313/tags/cuda/">#cuda</a>
      </div><div class="tag">
        <a href="http://localhost:1313/tags/torch/">#torch</a>
      </div></ol></div>
  <section class="page-section"><p><strong>Intro: PyTorch is a Deep Learning Operating System.</strong></p>
<h2 id="check-tensor-storage">Check tensor storage</h2>
<h3 id="device-check">Device check</h3>
<p>You should ALWAYS check EXPLICITLY whether input tensors are on desired devices. In most cases you want them on the same GPU, or in rare cases you want some tensors on CPU to perform some operations that are not efficient on GPU.</p>
<p><strong>API:</strong></p>
<ul>
<li><code>tensor.is_cuda()</code></li>
<li><code>tensor.device()</code> (Use <code>operator==</code> for equality comparison).</li>
</ul>
<p>Sometimes the not on correct device problem causes strange error messages like <code>Cusparse context initialization failure</code> or things even more weird, which first seem unrelated to a device problem. This is why I suggest you always start your debug journey here.</p>
<h3 id="contiguity-check">Contiguity check</h3>
<p>Modern LibTorch recommends using <a href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/core/TensorAccessor.h">Packed tensor accessor</a> (roughly the same memory cost as a pointer) to access elements in tensor.</p>
<p>However, if you are to plug some others&rsquo; implementation (likely using raw pointers like <code>float*</code>) into PyTorch, you are not likely to understand the code inside out and rewrite it.</p>
<p>Usually, in the context of deep learning, most implementations assumes a row-major contiguous storage. You should explicitly check whether the input tensors are contiguous in the C++ code that wraps the CUDA kernel.</p>
<p><strong>API:</strong> <code>tensor.is_contiguous()</code></p>
<h3 id="cheatsheet">Cheatsheet</h3>
<p>A quick utility that checks whether all tensors are on the same CUDA device:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">CheckInputTensors</span>(<span style="color:#66d9ef">const</span> std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>torch<span style="color:#f92672">::</span>Tensor<span style="color:#f92672">&gt;</span> <span style="color:#f92672">&amp;</span>tensors) {
</span></span><span style="display:flex;"><span>    TORCH_CHECK(<span style="color:#f92672">!</span>tensors.empty(), <span style="color:#e6db74">&#34;No tensors provided for device check&#34;</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> first_device <span style="color:#f92672">=</span> tensors[<span style="color:#ae81ff">0</span>].device();
</span></span><span style="display:flex;"><span>    TORCH_CHECK(first_device.is_cuda(), <span style="color:#e6db74">&#34;First tensor is not on CUDA&#34;</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> idx <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">const</span> <span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span>tensor: tensors) {
</span></span><span style="display:flex;"><span>        TORCH_CHECK(tensor.device() <span style="color:#f92672">==</span> first_device,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;All tensors must be on the same CUDA device, &#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;but found tensor at index [&#34;</span>, idx,
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;] on device &#34;</span>, tensor.device(),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34; while expecting &#34;</span>, first_device);
</span></span><span style="display:flex;"><span>        TORCH_CHECK(tensor.is_contiguous(),
</span></span><span style="display:flex;"><span>            <span style="color:#e6db74">&#34;All tensors must be contiguous, but found tensor at index [&#34;</span>,
</span></span><span style="display:flex;"><span>            idx, <span style="color:#e6db74">&#34;] not contiguous&#34;</span>);
</span></span><span style="display:flex;"><span>        idx <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="cuda-stream">CUDA stream</h2>
<p>Remember to always get the current CUDA stream via <code>at::cuda::getCurrentCUDAStream()</code> and pass it as the 4-th parameter in the <code>&lt;&lt;&lt;gridDim, blockDim, sharedMemorySizeBytes, stream&gt;&gt;&gt;</code> kernel call.</p>
<p>This is especially important when your operator is used in distributed training, where <code>at::cuda::getCurrentCUDAStream()</code> automatically selects the correct stream for you.</p>
<h2 id="cuda-toolkit-version-problem">CUDA toolkit version problem</h2>
<p>Most &ldquo;symbol not found&rdquo; problem are caused by compiler / assembler / library version mismatch. Let me elaborate on this a bit:</p>
<ul>
<li>PyTorch has an important version information attached to it: The version of CUDA that torch is compiled on (let&rsquo;s call it VT, cuda Version of Torch, for the sake of simplicity). The torch installation comes with its own CUDA toolkit (that matches VT) with no nvcc, ptxas.</li>
<li>If you are to write custom CUDA extension to PyTorch, it will use the nvcc and ptxas in your system <code>PATH</code>, and libraries like CUBLAS or CUSPARSE in <code>LD_LIBRARY_PATH</code>. Let&rsquo;s call this CUDA toolkit version VE, cuda Version of Extension.</li>
<li>When you try to compile a CUDA extension, Make sure that your VT and VE perfectly match (NOT major version match).</li>
<li>When you compile your extension, PyTorch hints you that a minor version mismatch should not be a problem. Remember, everything that should not happen will eventually happen.</li>
</ul>
<h2 id="memory-management-in-pytorch">Memory Management in PyTorch</h2>
<h3 id="allocation">Allocation</h3>
<p>When you need a buffer on HBM (e.g., for CUSPARSE or CUBLAS), your first instinct might be <code>cudaMalloc</code> and <code>cudaFree</code>. However, these force synchronization between CPU and GPU, which can starve the GPU.</p>
<p>Here&rsquo;s the key: PyTorch isn&rsquo;t just an autograd tool. It&rsquo;s a deep learning operating system that manages VRAM internally with a pooling and caching mechanism.</p>
<p>Using the PyTorch allocator is straightforward. Follow these steps:</p>
<ul>
<li>Set <code>dtype</code> to <code>torch::kInt8</code> and create a buffer tensor via <code>torch::empty</code></li>
<li>Get the pointer with <code>buffer_tensor.data_ptr&lt;int8_t&gt;()</code></li>
</ul>
<p>This gives you a pointer to the buffer. Here&rsquo;s a complete code snippet:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">auto</span> buffer_options <span style="color:#f92672">=</span> torch<span style="color:#f92672">::</span>TensorOptions().device(your_device).dtype(torch<span style="color:#f92672">::</span>kInt8);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">auto</span> buffer_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">::</span>empty({buffer_size}, buffer_options);
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#f92672">*</span>buffer_ptr <span style="color:#f92672">=</span> buffer_tensor.data_ptr<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">int8_t</span><span style="color:#f92672">&gt;</span>();
</span></span></code></pre></div><p>Remember do not call <code>cudaFree</code> on the pointer. RAII semantics will give the memory back to the allocator when destructor is called.</p>
<p>PyTorch&rsquo;s memory management is pretty much like a combination of OS memory management (buddy system, SLAB) and JVM or .net runtime (garbage collection, memory pool, caching and reusing memory blocks), but manages VRAM instead of a RAM.</p>
<p>I recommend reading <a href="https://zhuanlan.zhihu.com/p/680769942">this post (Chinese)</a> for a deeper dive into how PyTorch manages memory.</p>
<h2 id="using-cublas-cusparse-cusolverdn-etc">Using CUBLAS, CUSPARSE, CUSolverDn, etc.</h2>
<p>We use CUSPARSE as an example. The same rule apply to other libraries like CUBLAS or CUSolverDn.</p>
<h3 id="handles">Handles</h3>
<p>When writing pure CUDA/C++ code, you manually call <code>cusparseCreate</code> to initialize the CUSPARSE context and prepare for subsequent CUSPARSE API calls.</p>
<p>However this is not best practice in PyTorch CUDA extensions. There are good reasons: <code>cusparseCreate</code> introduces a milliseconds-level delay on CPU side. This may not be noticeable at first, but remember that operators are written to be run millions of times, which turns this into a significant overhead. This can cause GPU to starve when waiting CPU for synchronization.</p>
<ul>
<li>If you use <code>VizTracer</code> to trace your program and visualize it in <a href="https://ui.perfetto.dev">perfetto</a>, you may notice <code>cudaGetDeviceProperties</code> call taking too much time on CPU side. This can be directly caused by <code>cusparseCreate</code>.</li>
</ul>
<p>LibTorch has API that automatically manages a pool of CUSPARSE handles:</p>
<ul>
<li>Include the header that brings in CUDA context manager for LibTorch: <code>#include &lt;ATen/cuda/CUDAContext.h&gt;</code></li>
<li>Then, get handle via <code>auto handle = at::cuda::getCurrentCUDASparseHandle();</code> automatically create a handle if there is not any, and caches it for subsequent uses.</li>
<li>Use your handle as usual.</li>
</ul>
<p>I could not find documentation for these APIs, so if you want to know more, you may need to read the source code of PyTorch <code>ATen</code>. Searching in the repo with keyword <code>getcurrentcuda</code> can get you there quickly.</p>
<p><img src="../images/2025-10-02-14-53-44.png" alt=""></p>
<h3 id="buffers">Buffers</h3>
<p>Many CUSPARSE operations need buffers. If you need to make multiple CUSPARSE API calls with similar buffer size, it is bad practice to allocate right before the CUSPARE API call and deallocate right after since <code>cudaMalloc</code> and <code>cudaFree</code> are quite slow, which may cause your GPU to starve (verify this with VizTracer).</p>
<p>A better practice should be pre-allocating the buffer and pass its pointer into where the CUSPARSE API is called through <code>torch.empty()</code>.</p>
<h3 id="batched-matrix-multiplication">Batched Matrix Multiplication</h3>
<p>Refer to <a href="https://github.com/NVIDIA/CUDALibrarySamples/blob/master/cuSPARSE/spmm_csr_batched/spmm_csr_batched_example.c">this example</a> to see how to perform batched matrix multiplication in CUSPARSE.</p>
<p>Tricks:</p>
<ul>
<li>To broadcast, set stride to 0.</li>
<li>It is possible to broadcast <code>rowptr</code> but not <code>colind</code> and <code>values</code>.</li>
</ul>
<p>Check documentation for details.</p>
<h2 id="tensor-options">Tensor Options</h2>
<p><code>struct TensorOptions</code> carries many information about the tensor:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">C10_API</span> TensorOptions {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// ... omitted
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// members
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    Device device_ <span style="color:#f92672">=</span> at<span style="color:#f92672">::</span>kCPU; <span style="color:#75715e">// 16-bit
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    caffe2<span style="color:#f92672">::</span>TypeMeta dtype_ <span style="color:#f92672">=</span> caffe2<span style="color:#f92672">::</span>TypeMeta<span style="color:#f92672">::</span>Make<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">float</span><span style="color:#f92672">&gt;</span>(); <span style="color:#75715e">// 16-bit
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    Layout layout_ <span style="color:#f92672">=</span> at<span style="color:#f92672">::</span>kStrided; <span style="color:#75715e">// 8-bit
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    MemoryFormat memory_format_ <span style="color:#f92672">=</span> MemoryFormat<span style="color:#f92672">::</span>Contiguous; <span style="color:#75715e">// 8-bit
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">bool</span> requires_grad_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> pinned_memory_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// Existense of members
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">bool</span> has_device_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> has_dtype_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> has_layout_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> has_requires_grad_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> has_pinned_memory_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">bool</span> has_memory_format_ : <span style="color:#ae81ff">1</span>;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The most important methods are:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#a6e22e">[[nodiscard]]</span> TensorOptions device(Device device) <span style="color:#66d9ef">const</span>;
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">[[nodiscard]]</span> TensorOptions dtype(ScalarType dtype) <span style="color:#66d9ef">const</span>;
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">[[nodiscard]]</span> TensorOptions requires_grad(<span style="color:#66d9ef">bool</span>) <span style="color:#66d9ef">const</span>;
</span></span></code></pre></div><p><strong>Usage:</strong></p>
<ul>
<li><code>tensor.options()</code> returns an instance of <code>TensorOptions</code> that describes the <code>tensor</code>.</li>
<li><code>opt.dtype(torch::kFloat64)</code> has other properties remain the same as <code>opt</code>, only <code>dtype</code> changes to <code>float64</code> or in C++, <code>double</code>.</li>
<li>The <code>.to(...)</code> method of a tensor can take a <code>TensorOptions</code> instance as its only argument.</li>
</ul>
<p>For an exhaustive list of device and dtype, you may want to refer to:</p>
<ul>
<li><a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/types.h</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h">https://github.com/pytorch/pytorch/blob/main/c10/core/DeviceType.h</a></li>
</ul>
<h2 id="debug-layer-by-layer">Debug layer by layer</h2>
<p>A CUDA extension is roughly split into 4 parts, from the bottom to the top namely:</p>
<ol>
<li>CUDA kernel</li>
<li>C++ wrapper</li>
<li>data passed from Python (PyTorch) to C++</li>
<li>Python wrapper</li>
</ol>
<h3 id="cuda-kernel">CUDA kernel</h3>
<p>Debugging CUDA kernel is a very very difficult problem and we shall not discuss it here.</p>
<h3 id="c-wrapper">C++ wrapper</h3>
<p>The first thing I want to hint you is that do not dereference a pointer pointing to device in host functions. You should always mark device pointers with a <code>d_</code> prefix in variable names, or wrap it with <code>thrust::device_ptr</code>.</p>
<p><code>printf</code>, <code>std::cout</code> or <code>gdb</code> will assist you in the journey.</p>
<h3 id="data-passed-from-python-pytorch-to-c">data passed from Python (PyTorch) to C++</h3>
<p>Refer to Pybind11 docs and try to answer these questions:</p>
<ul>
<li>How various Python types are represented in Pybind11 API;</li>
<li>How to properly configure the function prototype in Pybind11?</li>
</ul>
<h3 id="python-wrapper">Python Wrapper</h3>
<p>Ask LLMs. LLMs know python much better than I do.</p>
<h2 id="what-to-reference">What to Reference</h2>
<p>To my knowledge, the PyTorch C++ <a href="https://pytorch.org/cppdocs/api/library_root.html">documentation</a> is very old. Many things in the source code are not documented there.</p>
<p>It is a better choice to just search in the PyTorch <a href="https://github.com/pytorch/pytorch">github repo</a>, and read the comments and source code.</p>
</section></main>
        <footer id="main-footer"><div class="footer">
  <a href="#">Scroll to Top</a>
  <div class="footer-copyright">
    <div class="dim">© 2025 Da1sypetals</div>
    <div>Made with ❤️ and powered by <a href="https://github.com/math-queiroz/rusty-typewriter" target="_blank">Rusty Typewriter</a> theme for <a href="https://gohugo.io/" target="_blank">Hugo</a></div>
  </div>
</div>
</footer>
      </div><aside id="side-pane" class="side-sticky"><div class="side-details">
    <span>1376 words</span>
    <span>9 - 11 minutes read</span></div><h3>Table Of Contents</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#check-tensor-storage">Check tensor storage</a>
      <ul>
        <li><a href="#device-check">Device check</a></li>
        <li><a href="#contiguity-check">Contiguity check</a></li>
        <li><a href="#cheatsheet">Cheatsheet</a></li>
      </ul>
    </li>
    <li><a href="#cuda-stream">CUDA stream</a></li>
    <li><a href="#cuda-toolkit-version-problem">CUDA toolkit version problem</a></li>
    <li><a href="#memory-management-in-pytorch">Memory Management in PyTorch</a>
      <ul>
        <li><a href="#allocation">Allocation</a></li>
      </ul>
    </li>
    <li><a href="#using-cublas-cusparse-cusolverdn-etc">Using CUBLAS, CUSPARSE, CUSolverDn, etc.</a>
      <ul>
        <li><a href="#handles">Handles</a></li>
        <li><a href="#buffers">Buffers</a></li>
        <li><a href="#batched-matrix-multiplication">Batched Matrix Multiplication</a></li>
      </ul>
    </li>
    <li><a href="#tensor-options">Tensor Options</a></li>
    <li><a href="#debug-layer-by-layer">Debug layer by layer</a>
      <ul>
        <li><a href="#cuda-kernel">CUDA kernel</a></li>
        <li><a href="#c-wrapper">C++ wrapper</a></li>
        <li><a href="#data-passed-from-python-pytorch-to-c">data passed from Python (PyTorch) to C++</a></li>
        <li><a href="#python-wrapper">Python Wrapper</a></li>
      </ul>
    </li>
    <li><a href="#what-to-reference">What to Reference</a></li>
  </ul>
</nav><h3>Related</h3>
    <ul><li><a href="/posts/gnn-optim/">近期GNN Attention算子优化工作速览</a></li><li><a href="/english-post/snapviewer/">SnapViewer: Faster PyTorch Memory Allocation Viewer</a></li><li><a href="/posts/snapviewer-3-zh/">Snapviewer Devlog #3: 性能优化</a></li></ul></aside></div>
  </div>
</body>

</html>